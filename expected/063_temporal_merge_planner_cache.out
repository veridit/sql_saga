\i sql/include/test_setup.sql
--
-- test_setup.sql
--
-- Common setup for regression tests that need to be self-contained.
-- This script creates the extension, a user role, and grants permissions.
--
SET datestyle = 'ISO, YMD';
CREATE EXTENSION IF NOT EXISTS btree_gist;
CREATE EXTENSION IF NOT EXISTS sql_saga CASCADE;
DO $$
BEGIN
    CREATE ROLE sql_saga_unprivileged_user;
EXCEPTION WHEN duplicate_object THEN
END
$$;
GRANT USAGE ON SCHEMA sql_saga TO sql_saga_unprivileged_user;
GRANT SELECT ON ALL TABLES IN SCHEMA sql_saga TO sql_saga_unprivileged_user;
GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA sql_saga TO sql_saga_unprivileged_user;
/*
 * Allow the unprivileged user to create tables in the public schema.
 * This is required for tests that create their own tables.
 * PG 15+ restricts this by default.
 */
GRANT CREATE ON SCHEMA public TO PUBLIC;
BEGIN;
\echo '----------------------------------------------------------------------------'
----------------------------------------------------------------------------
\echo 'Test: `temporal_merge_plan` EXPLAIN output for performance monitoring'
Test: `temporal_merge_plan` EXPLAIN output for performance monitoring
\echo 'This test verifies that the planner function produces a stable and efficient'
This test verifies that the planner function produces a stable and efficient
\echo 'EXPLAIN plan. Its output is captured to monitor for performance regressions.'
EXPLAIN plan. Its output is captured to monitor for performance regressions.
\echo '----------------------------------------------------------------------------'
----------------------------------------------------------------------------
-- Instruct temporal_merge to RAISE NOTICE the sql used for analysis.
SET client_min_messages TO NOTICE;
SET sql_saga.temporal_merge.log_sql = true;
CREATE SCHEMA tmpc;
--------------------------------------------------------------------------------
-- SCENARIO 1: Simple Surrogate Primary Key (id int NOT NULL)
--------------------------------------------------------------------------------
SAVEPOINT s1;
\echo '\n--- Scenario 1: Simple Surrogate Primary Key (id int NOT NULL) ---'

--- Scenario 1: Simple Surrogate Primary Key (id int NOT NULL) ---
CREATE TABLE tmpc.target (id int NOT NULL, valid_range daterange, valid_from date, valid_until date, value text);
SELECT sql_saga.add_era('tmpc.target', 'valid_range',
    valid_from_column_name => 'valid_from',
    valid_until_column_name => 'valid_until');
NOTICE:  sql_saga: Created trigger "target_synchronize_temporal_columns_trigger" on table tmpc.target to synchronize columns: valid_from, valid_until
 add_era 
---------
 t
(1 row)

CREATE TABLE tmpc.source1 (row_id int, id int NOT NULL, valid_from date, valid_until date, value text);
\echo '--- Setting up tables with indexes and data for a realistic plan ---'
--- Setting up tables with indexes and data for a realistic plan ---
SELECT sql_saga.add_unique_key(table_oid => 'tmpc.target'::regclass, column_names => ARRAY['id'], key_type => 'primary');
NOTICE:  sql_saga: altering table tmpc.target to add constraints: ALTER COLUMN id SET NOT NULL, ADD PRIMARY KEY (id, valid_range WITHOUT OVERLAPS)
 add_unique_key  
-----------------
 target_id_valid
(1 row)

CREATE INDEX ON tmpc.source1 (id);
CREATE INDEX ON tmpc.source1 USING gist (daterange(valid_from, valid_until));
\echo '\d tmpc.target'
d tmpc.target
\d tmpc.target
                   Table "tmpc.target"
   Column    |   Type    | Collation | Nullable | Default 
-------------+-----------+-----------+----------+---------
 id          | integer   |           | not null | 
 valid_range | daterange |           | not null | 
 valid_from  | date      |           | not null | 
 valid_until | date      |           |          | 
 value       | text      |           |          | 
Indexes:
    "target_pkey" PRIMARY KEY (id, valid_range WITHOUT OVERLAPS)
    "target_id_idx" btree (id)
    "target_id_valid_range_idx" btree (id, valid_range)
Check constraints:
    "target_check" CHECK (valid_from < valid_until AND valid_from > '-infinity'::date)
    "target_valid_check" CHECK (NOT isempty(valid_range))
Triggers:
    target_valid_sync_temporal_trg BEFORE INSERT OR UPDATE OF valid_range, valid_from, valid_until ON tmpc.target FOR EACH ROW EXECUTE FUNCTION sql_saga.tmpc_target_valid_template_sync()

\echo '\d tmpc.source1'
d tmpc.source1
\d tmpc.source1
                  Table "tmpc.source1"
   Column    |  Type   | Collation | Nullable | Default 
-------------+---------+-----------+----------+---------
 row_id      | integer |           |          | 
 id          | integer |           | not null | 
 valid_from  | date    |           |          | 
 valid_until | date    |           |          | 
 value       | text    |           |          | 
Indexes:
    "source1_daterange_idx" gist (daterange(valid_from, valid_until))
    "source1_id_idx" btree (id)

-- Insert enough rows to make an index scan more attractive to the planner.
INSERT INTO tmpc.target (id, valid_from, valid_until, value)
SELECT i, '2023-01-01', '2024-01-01', 'A' FROM generate_series(1, 1000) as i;
INSERT INTO tmpc.source1 VALUES (1, 500, '2023-06-01', '2023-07-01', 'B');
ANALYZE tmpc.target;
ANALYZE tmpc.source1;
\echo '\n--- Performance Monitoring: EXPLAIN the cached planner query ---'

--- Performance Monitoring: EXPLAIN the cached planner query ---
DO $$
DECLARE
    plan_sqls JSONB;
    sql_step JSONB;
    sql_stmt TEXT;
    rec RECORD;
    has_seq_scan BOOLEAN;
BEGIN
    -- Run once to populate cache. Output is discarded.
    PERFORM * FROM sql_saga.temporal_merge_plan(
        target_table => 'tmpc.target'::regclass,
        source_table => 'tmpc.source1'::regclass,
        identity_columns => '{id}'::text[],
        mode => 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode,
        era_name => 'valid'
    );

    -- Find the plan SQLs for the source table of this scenario.
    SELECT c.plan_sqls INTO plan_sqls
    FROM pg_temp.temporal_merge_plan_cache c
    WHERE c.cache_key LIKE '%' || 'tmpc.source1'::regclass::text;

    -- Clean up temp tables created by the first run so we can re-execute them.
    CALL sql_saga.temporal_merge_drop_temp_tables();

    RAISE NOTICE '--- Explaining % steps from cache', jsonb_array_length(plan_sqls);
    FOR sql_step IN SELECT * FROM jsonb_array_elements(plan_sqls)
    LOOP
        RAISE NOTICE '---';
        sql_stmt := sql_step->>'sql';

        IF sql_step->>'type' = 'setup' THEN
            RAISE NOTICE 'Executing setup: %', sql_stmt;
            EXECUTE sql_stmt;
        ELSE
            RAISE NOTICE 'Explaining: %', sql_stmt;
            has_seq_scan := false;
            FOR rec IN EXECUTE 'EXPLAIN (ANALYZE, COSTS OFF, TIMING OFF, SUMMARY OFF, BUFFERS) ' || sql_stmt
            LOOP
                RAISE NOTICE '%', rec."QUERY PLAN";
                -- The schema is hardcoded here, which is fine for this specific test.
                -- We add a space to not match "target_rows".
                IF rec."QUERY PLAN" LIKE '%Seq Scan on target %' THEN
                    has_seq_scan := true;
                END IF;
            END LOOP;

            IF has_seq_scan THEN
                RAISE EXCEPTION 'Performance regression detected: EXPLAIN plan contains a "Seq Scan on target".';
            END IF;
        END IF;
    END LOOP;
END;
$$;
NOTICE:  --- Explaining 43 steps from cache
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_initial ON COMMIT DROP AS
                WITH source_data AS (
                    SELECT
                        source_table.row_id /* row_id_column */ as source_row_id,
                        source_table.row_id /* v_causal_select_expr */ as causal_id,
                        source_table.id,  -- v_non_temporal_lookup_cols_select_list_prefix
                        source_table.valid_from as valid_from, source_table.valid_until as valid_until, /* v_source_temporal_cols_expr */
                        jsonb_strip_nulls(jsonb_build_object('value', source_table.value)) /* v_source_data_payload_expr */ AS data_payload,
                        jsonb_strip_nulls(jsonb_build_object()) /* v_source_ephemeral_payload_expr */ AS ephemeral_payload,
                        jsonb_build_object('id', source_table.id) /* v_stable_pk_cols_jsonb_build_source */ as stable_pk_payload,
                        source_table.id IS NULL /* v_entity_id_check_is_null_expr */ as stable_identity_columns_are_null,
                        (true) /* v_lookup_cols_are_null_expr */ as natural_identity_column_values_are_null,
                        true /* v_is_identifiable_expr */ as is_identifiable,
                        true /* v_consistency_check_expr */ as temporal_columns_are_consistent
                    FROM tmpc.source1 /* v_source_table_ident */ source_table
                )
                SELECT 
                    *,
                    -- OPTIMIZATION: Pre-compute range column to avoid repeated daterange() calls
                    -- This provides ~10-15% performance improvement in eclipse detection
                    daterange(valid_from, valid_until) as valid_range
                FROM source_data;
            
NOTICE:  Seq Scan on source1 source_table (actual rows=1.00 loops=1)
NOTICE:    Buffers: shared hit=1
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_initial (source_row_id);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_initial (id);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_initial (id, source_row_id);
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE source_initial;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_with_eclipsed_flag ON COMMIT DROP AS
                SELECT
                    s1.*,
                    eclipse_info.is_eclipsed,
                    eclipse_info.eclipsed_by
                FROM source_initial s1
                CROSS JOIN LATERAL (
                    SELECT
                        -- OPTIMIZATION: Use pre-computed valid_range instead of repeated daterange() calls
                        -- Combined with composite index, this provides 30-40 percent total performance improvement
                        COALESCE(sql_saga.covers_without_gaps(s2.valid_range, s1.valid_range ORDER BY s2.valid_from), false) as is_eclipsed,
                        array_agg(s2.source_row_id) as eclipsed_by
                    FROM source_initial s2
                    WHERE
                        (
                            (NOT s1.natural_identity_column_values_are_null AND (false))
                            OR
                            (s1.natural_identity_column_values_are_null AND s1.causal_id = s2.causal_id)
                        )
                        AND
                        -- Only consider newer rows (higher row_id) as potential eclipsers.
                        s2.source_row_id > s1.source_row_id
                ) eclipse_info;
            
NOTICE:  Nested Loop (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_initial s1 (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Aggregate (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Sort (actual rows=0.00 loops=1)
NOTICE:                Sort Key: s2.valid_from
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Result (actual rows=0.00 loops=1)
NOTICE:                      One-Time Filter: s1.natural_identity_column_values_are_null
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  Seq Scan on source_initial s2 (actual rows=0.00 loops=1)
NOTICE:                            Filter: ((source_row_id > s1.source_row_id) AND (s1.causal_id = causal_id))
NOTICE:                            Rows Removed by Filter: 1
NOTICE:                            Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=43, local read=3
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE target_rows ON COMMIT DROP AS
                SELECT
                    id,  /* v_non_temporal_lookup_cols_select_list_no_alias_prefix */ -- (non-temporal identity columns)
                    lower(t.valid_range) as valid_from, -- The temporal identity column (e.g., valid_from)
                    NULL::integer /* v_causal_column_type */ as causal_id, -- Target rows do not originate from a source row, so causal_id is NULL. Type is introspected.
                    jsonb_build_object('id', id) /* v_stable_pk_cols_jsonb_build_bare */ as stable_pk_payload,
                    upper(t.valid_range) as valid_until,
                    jsonb_build_object('value', value) /* v_target_data_cols_jsonb_build_bare */ AS data_payload,
                    jsonb_build_object() /* v_target_ephemeral_cols_jsonb_build_bare */ AS ephemeral_payload,
                    jsonb_strip_nulls(jsonb_build_object()) /* v_target_nk_json_expr */ AS canonical_nk_json
                FROM (
                        SELECT * FROM (
                            SELECT DISTINCT ON (u.id, u.valid_from) * FROM (
                                (
                                    SELECT * FROM tmpc.target inner_t
                                    WHERE (inner_t.id) IN (SELECT DISTINCT si.id FROM source_initial si WHERE (si.id) IS NOT NULL)
                                )
                            ) u
                        )
                    ) AS t  /* v_target_rows_filter */
            
NOTICE:  Subquery Scan on unnamed_subquery (actual rows=1.00 loops=1)
NOTICE:    Buffers: shared hit=3, local hit=1
NOTICE:    ->  Unique (actual rows=1.00 loops=1)
NOTICE:          Buffers: shared hit=3, local hit=1
NOTICE:          ->  Incremental Sort (actual rows=1.00 loops=1)
NOTICE:                Sort Key: inner_t.id, inner_t.valid_from
NOTICE:                Presorted Key: inner_t.id
NOTICE:                Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:                Buffers: shared hit=3, local hit=1
NOTICE:                ->  Nested Loop (actual rows=1.00 loops=1)
NOTICE:                      Buffers: shared hit=3, local hit=1
NOTICE:                      ->  Unique (actual rows=1.00 loops=1)
NOTICE:                            Buffers: local hit=1
NOTICE:                            ->  Sort (actual rows=1.00 loops=1)
NOTICE:                                  Sort Key: si.id
NOTICE:                                  Sort Method: quicksort  Memory: 25kB
NOTICE:                                  Buffers: local hit=1
NOTICE:                                  ->  Seq Scan on source_initial si (actual rows=1.00 loops=1)
NOTICE:                                        Filter: (id IS NOT NULL)
NOTICE:                                        Buffers: local hit=1
NOTICE:                      ->  Index Scan using target_pkey on target inner_t (actual rows=1.00 loops=1)
NOTICE:                            Index Cond: (id = si.id)
NOTICE:                            Index Searches: 1
NOTICE:                            Buffers: shared hit=3
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON target_rows (id);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON target_rows USING gist (daterange(valid_from, valid_until));
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE target_rows;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_matches ON COMMIT DROP AS
                SELECT
                    source_row.*,
                    target_row.stable_pk_payload as discovered_stable_pk_payload,
                    target_row.id AS discovered_id_1 /* v_propagated_id_cols_list */
                FROM source_with_eclipsed_flag source_row
                LEFT JOIN target_rows target_row ON (source_row.id IS NOT DISTINCT FROM target_row.id /* v_source_rows_exists_join_expr */)
            
NOTICE:  Nested Loop Left Join (actual rows=1.00 loops=1)
NOTICE:    Join Filter: (NOT (source_row.id IS DISTINCT FROM target_row.id))
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_with_eclipsed_flag source_row (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Materialize (actual rows=1.00 loops=1)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on target_rows target_row (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=44, local read=1
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_aggregates ON COMMIT DROP AS
                SELECT
                    source_row_id,
                    count(DISTINCT discovered_stable_pk_payload) as match_count,
                    jsonb_agg(DISTINCT discovered_stable_pk_payload) as conflicting_ids
                FROM source_rows_with_matches
                GROUP BY source_row_id
            
NOTICE:  GroupAggregate (actual rows=1.00 loops=1)
NOTICE:    Group Key: source_row_id
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=1.00 loops=1)
NOTICE:          Sort Key: source_row_id, discovered_stable_pk_payload
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on source_rows_with_matches (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=38
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_discovery ON COMMIT DROP AS
                SELECT
                    m.*,
                    a.match_count,
                    a.conflicting_ids,
                    (a.match_count > 1) as is_ambiguous
                FROM source_rows_with_matches m
                JOIN source_rows_with_aggregates a ON m.source_row_id = a.source_row_id
            
NOTICE:  Hash Join (actual rows=1.00 loops=1)
NOTICE:    Hash Cond: (a.source_row_id = m.source_row_id)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_rows_with_aggregates a (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Hash (actual rows=1.00 loops=1)
NOTICE:          Buckets: 1024  Batches: 1  Memory Usage: 9kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on source_rows_with_matches m (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=10
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows ON COMMIT DROP AS
                SELECT DISTINCT ON (p.source_row_id)
                    p.source_row_id, p.causal_id, p.valid_from, p.valid_until, p.data_payload, p.ephemeral_payload,
                    (p.stable_identity_columns_are_null AND p.discovered_stable_pk_payload IS NULL) as stable_identity_columns_are_null,
                    p.natural_identity_column_values_are_null, p.is_identifiable,
                    p.is_ambiguous, p.conflicting_ids, p.is_eclipsed, p.eclipsed_by,
                    p.temporal_columns_are_consistent,
                    COALESCE(p.stable_pk_payload, p.discovered_stable_pk_payload) as stable_pk_payload,
                    (p.discovered_stable_pk_payload IS NOT NULL) /* v_target_entity_exists_expr */ as target_entity_exists,
                    COALESCE(p.id, p.discovered_id_1) AS id /* v_coalesced_id_cols_list */
                FROM source_rows_with_discovery p
                ORDER BY p.source_row_id, p.discovered_stable_pk_payload
            
NOTICE:  Unique (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=1.00 loops=1)
NOTICE:          Sort Key: source_row_id, discovered_stable_pk_payload
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on source_rows_with_discovery p (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=42
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_new_flag ON COMMIT DROP AS
                SELECT *, NOT target_entity_exists as is_new_entity
                FROM source_rows
            
NOTICE:  Seq Scan on source_rows (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=36
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_nk_json ON COMMIT DROP AS
                SELECT source_row.*, jsonb_strip_nulls(jsonb_build_object()) /* v_lookup_keys_as_jsonb_expr */ as nk_json, ARRAY(SELECT e FROM unnest(ARRAY[]::TEXT[]) e WHERE e IS NOT NULL ORDER BY e) /* v_lookup_keys_as_array_expr */ as nk_non_null_keys_array
                FROM source_rows_with_new_flag source_row
            
NOTICE:  Seq Scan on source_rows_with_new_flag source_row (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    InitPlan 1
NOTICE:      ->  Sort (actual rows=0.00 loops=1)
NOTICE:            Sort Key: e.e
NOTICE:            Sort Method: quicksort  Memory: 25kB
NOTICE:            ->  Function Scan on unnest e (actual rows=0.00 loops=1)
NOTICE:                  Filter: (e IS NOT NULL)
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=38
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_rows_with_nk_json USING GIN (nk_json);
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE source_rows_with_nk_json;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_canonical_key ON COMMIT DROP AS
                SELECT *, (CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (id IS NULL) THEN COALESCE(id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(id::text, '_NULL_')
            END /* v_grouping_key_expr */) as grouping_key
                FROM (
                    SELECT
                        s1.*,
                        s2.nk_json as canonical_nk_json
                    FROM source_rows_with_nk_json s1
                    LEFT JOIN LATERAL (
                        SELECT s2_inner.nk_json, s2_inner.nk_non_null_keys_array
                        FROM source_rows_with_nk_json s2_inner
                        WHERE s1.is_new_entity AND s2_inner.is_new_entity AND s2_inner.nk_json @> s1.nk_json
                        ORDER BY array_length(s2_inner.nk_non_null_keys_array, 1) DESC, s2_inner.nk_non_null_keys_array::text DESC
                        LIMIT 1
                    ) s2 ON true
                ) s
            
NOTICE:  Nested Loop Left Join (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Seq Scan on source_rows_with_nk_json s1 (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Limit (actual rows=0.00 loops=1)
NOTICE:          ->  Sort (actual rows=0.00 loops=1)
NOTICE:                Sort Key: (array_length(s2_inner.nk_non_null_keys_array, 1)) DESC, ((s2_inner.nk_non_null_keys_array)::text) DESC
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                ->  Result (actual rows=0.00 loops=1)
NOTICE:                      One-Time Filter: s1.is_new_entity
NOTICE:                      ->  Seq Scan on source_rows_with_nk_json s2_inner (never executed)
NOTICE:                            Filter: (is_new_entity AND (nk_json @> s1.nk_json))
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=64, local hit=1
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_early_feedback ON COMMIT DROP AS
                SELECT
                    s.*,
                    CASE
                        WHEN s.is_ambiguous
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row is ambiguous. It matches multiple distinct target entities: ' || s.conflicting_ids::text )
                        WHEN NOT s.is_identifiable AND s.is_new_entity
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row is unidentifiable. It has NULL for all stable identity columns ' || replace('{id}'::text, '"', '') || ' and all natural keys ' || replace(NULL::text, '"', '') )
                        WHEN NOT s.temporal_columns_are_consistent
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row has inconsistent temporal columns. Column "' || 'valid_until' || '" must be equal to column "' || NULL || '" + ' || NULL || '.' )
                        WHEN s.is_eclipsed
                        THEN jsonb_build_object( 'operation', 'SKIP_ECLIPSED'::text, 'message', 'Source row was eclipsed by row_ids=' || s.eclipsed_by::text || ' in the same batch.' )
                        ELSE NULL
                    END as early_feedback
                FROM source_rows_with_canonical_key s
            
NOTICE:  Seq Scan on source_rows_with_canonical_key s (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=46
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE active_source_rows ON COMMIT DROP AS
                SELECT source_row.*
                FROM source_rows_with_early_feedback source_row
                WHERE source_row.early_feedback IS NULL
                AND CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                    WHEN 'MERGE_ENTITY_PATCH' THEN true
                    WHEN 'MERGE_ENTITY_REPLACE' THEN true
                    WHEN 'MERGE_ENTITY_UPSERT' THEN true
                    WHEN 'INSERT_NEW_ENTITIES' THEN NOT source_row.target_entity_exists
                    WHEN 'PATCH_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'REPLACE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'DELETE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'UPDATE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    ELSE false
                END
            
NOTICE:  Seq Scan on source_rows_with_early_feedback source_row (actual rows=1.00 loops=1)
NOTICE:    Filter: (early_feedback IS NULL)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=48
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON active_source_rows (id);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON active_source_rows (grouping_key);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON active_source_rows USING gist (daterange(valid_from, valid_until));
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE active_source_rows;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE all_rows ON COMMIT DROP AS
                SELECT id,  /* v_non_temporal_lookup_cols_select_list_no_alias_prefix */ causal_id, valid_from, valid_until, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, temporal_columns_are_consistent, canonical_nk_json FROM active_source_rows
                UNION ALL
                SELECT
                    target_row.id,  /* v_non_temporal_tr_qualified_lookup_cols_prefix */
                    target_row.causal_id,
                    target_row.valid_from,
                    target_row.valid_until,
                    false as is_new_entity,
                    target_row.stable_pk_payload,
                    false as stable_identity_columns_are_null,
                    false as natural_identity_column_values_are_null,
                    true as is_identifiable,
                    false as is_ambiguous,
                    NULL::jsonb as conflicting_ids,
                    true as temporal_columns_are_consistent,
                    target_row.canonical_nk_json
                FROM target_rows target_row
            
NOTICE:  Append (actual rows=2.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on active_source_rows (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Seq Scan on target_rows target_row (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=53, local read=2
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points_raw ON COMMIT DROP AS
                SELECT id, causal_id, valid_from AS point, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json FROM all_rows
                UNION ALL
                SELECT id, causal_id, valid_until AS point, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json FROM all_rows
            
NOTICE:  Append (actual rows=4.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on all_rows (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Seq Scan on all_rows all_rows_1 (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=28
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points_unified ON COMMIT DROP AS
                SELECT
                    *,
                    CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (id IS NULL) THEN COALESCE(id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(id::text, '_NULL_')
            END AS grouping_key,
                    CASE
                        WHEN is_new_entity THEN causal_id
                        ELSE FIRST_VALUE(causal_id) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (id IS NULL) THEN COALESCE(id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS LAST)
                    END as unified_causal_id,
                    FIRST_VALUE(stable_pk_payload) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (id IS NULL) THEN COALESCE(id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS FIRST) as unified_stable_pk_payload,
                    FIRST_VALUE(canonical_nk_json) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (id IS NULL) THEN COALESCE(id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS FIRST) as unified_canonical_nk_json
                FROM time_points_raw
            
NOTICE:  WindowAgg (actual rows=4.00 loops=1)
NOTICE:    Window: w2 AS (PARTITION BY (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN (id IS NOT NULL) THEN COALESCE((id)::text, '_NULL_'::text) ELSE (causal_id)::text END) ELSE ('existing_entity__'::text || COALESCE((id)::text, '_NULL_'::text)) END) ORDER BY causal_id)
NOTICE:    Storage: Memory  Maximum Storage: 17kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Incremental Sort (actual rows=4.00 loops=1)
NOTICE:          Sort Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN (id IS NOT NULL) THEN COALESCE((id)::text, '_NULL_'::text) ELSE (causal_id)::text END) ELSE ('existing_entity__'::text || COALESCE((id)::text, '_NULL_'::text)) END), causal_id
NOTICE:          Presorted Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN (id IS NOT NULL) THEN COALESCE((id)::text, '_NULL_'::text) ELSE (causal_id)::text END) ELSE ('existing_entity__'::text || COALESCE((id)::text, '_NULL_'::text)) END)
NOTICE:          Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  WindowAgg (actual rows=4.00 loops=1)
NOTICE:                Window: w1 AS (PARTITION BY (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN (id IS NOT NULL) THEN COALESCE((id)::text, '_NULL_'::text) ELSE (causal_id)::text END) ELSE ('existing_entity__'::text || COALESCE((id)::text, '_NULL_'::text)) END) ORDER BY causal_id)
NOTICE:                Storage: Memory  Maximum Storage: 17kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Sort (actual rows=4.00 loops=1)
NOTICE:                      Sort Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN (id IS NOT NULL) THEN COALESCE((id)::text, '_NULL_'::text) ELSE (causal_id)::text END) ELSE ('existing_entity__'::text || COALESCE((id)::text, '_NULL_'::text)) END), causal_id NULLS FIRST
NOTICE:                      Sort Method: quicksort  Memory: 25kB
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  Seq Scan on time_points_raw (actual rows=4.00 loops=1)
NOTICE:                            Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=24
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points_with_unified_ids ON COMMIT DROP AS
                SELECT
                    tpu.grouping_key,
                    tpu.id,
                    tpu.unified_causal_id as causal_id,
                    tpu.point,
                    tpu.is_new_entity,
                    tpu.unified_stable_pk_payload as stable_pk_payload,
                    tpu.stable_identity_columns_are_null,
                    tpu.natural_identity_column_values_are_null,
                    tpu.is_identifiable,
                    tpu.is_ambiguous,
                    tpu.conflicting_ids,
                    tpu.unified_canonical_nk_json as canonical_nk_json
                FROM time_points_unified tpu
            
NOTICE:  Seq Scan on time_points_unified tpu (actual rows=4.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=32
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points ON COMMIT DROP AS
                SELECT DISTINCT ON (grouping_key, point) *
                FROM time_points_with_unified_ids
                ORDER BY grouping_key, point, causal_id DESC NULLS LAST
            
NOTICE:  Unique (actual rows=4.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=4.00 loops=1)
NOTICE:          Sort Key: grouping_key, point, causal_id DESC NULLS LAST
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on time_points_with_unified_ids (actual rows=4.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=26
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE atomic_segments ON COMMIT DROP AS
                SELECT grouping_key, id, causal_id, point as valid_from, next_point as valid_until, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json
                FROM (
                    SELECT *, LEAD(point) OVER (PARTITION BY grouping_key ORDER BY point) as next_point
                    FROM time_points
                ) with_lead
                WHERE point IS NOT NULL AND next_point IS NOT NULL AND point < next_point
            
NOTICE:  Subquery Scan on with_lead (actual rows=3.00 loops=1)
NOTICE:    Filter: ((with_lead.point IS NOT NULL) AND (with_lead.next_point IS NOT NULL) AND (with_lead.point < with_lead.next_point))
NOTICE:    Rows Removed by Filter: 1
NOTICE:    Buffers: local hit=1
NOTICE:    ->  WindowAgg (actual rows=4.00 loops=1)
NOTICE:          Window: w1 AS (PARTITION BY time_points.grouping_key ORDER BY time_points.point)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Sort (actual rows=4.00 loops=1)
NOTICE:                Sort Key: time_points.grouping_key, time_points.point
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Seq Scan on time_points (actual rows=4.00 loops=1)
NOTICE:                      Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=26
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE existing_segments_with_target ON COMMIT DROP AS
                SELECT
                    seg.grouping_key, seg.canonical_nk_json, seg.id, seg.causal_id, seg.is_new_entity, seg.is_identifiable, seg.is_ambiguous, seg.conflicting_ids, seg.stable_identity_columns_are_null, seg.natural_identity_column_values_are_null, seg.valid_from, seg.valid_until,
                    target_row.valid_from as t_valid_from, target_row.valid_until as t_valid_until,
                    target_row.data_payload as t_data_payload, target_row.ephemeral_payload as t_ephemeral_payload, seg.stable_pk_payload, 
                    target_row.stable_pk_payload as target_stable_pk_payload,
                    NULL::jsonb as trace
                FROM atomic_segments seg
                LEFT JOIN target_rows target_row 
                    ON (target_row.id = seg.id OR (target_row.id IS NULL AND seg.id IS NULL)) 
                    AND daterange(seg.valid_from, seg.valid_until) <@ daterange(target_row.valid_from, target_row.valid_until)
                WHERE NOT seg.is_new_entity
            
NOTICE:  Nested Loop Left Join (actual rows=3.00 loops=1)
NOTICE:    Join Filter: (((target_row.id = seg.id) OR ((target_row.id IS NULL) AND (seg.id IS NULL))) AND (daterange(seg.valid_from, seg.valid_until) <@ daterange(target_row.valid_from, target_row.valid_until)))
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on atomic_segments seg (actual rows=3.00 loops=1)
NOTICE:          Filter: (NOT is_new_entity)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Materialize (actual rows=1.00 loops=3)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on target_rows target_row (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=39
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE new_segments_no_target ON COMMIT DROP AS
                SELECT
                    seg.grouping_key, seg.canonical_nk_json, seg.id, seg.causal_id, seg.is_new_entity, seg.is_identifiable, seg.is_ambiguous, seg.conflicting_ids, seg.stable_identity_columns_are_null, seg.natural_identity_column_values_are_null, seg.valid_from, seg.valid_until,
                    NULL::date as t_valid_from, NULL::date as t_valid_until,
                    NULL::jsonb as t_data_payload, NULL::jsonb as t_ephemeral_payload, seg.stable_pk_payload,
                    NULL::jsonb as target_stable_pk_payload,
                    NULL::jsonb as trace
                FROM atomic_segments seg
                WHERE seg.is_new_entity
            
NOTICE:  Seq Scan on atomic_segments seg (actual rows=0.00 loops=1)
NOTICE:    Filter: is_new_entity
NOTICE:    Rows Removed by Filter: 3
NOTICE:    Buffers: local hit=1
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments_with_payloads ON COMMIT DROP AS
                WITH all_segments AS (
                    SELECT * FROM existing_segments_with_target
                    UNION ALL
                    SELECT * FROM new_segments_no_target
                )
                SELECT
                    with_base_payload.*,
                    with_base_payload.stable_pk_payload as propagated_stable_pk_payload
                FROM (
                    SELECT
                        seg.*, source_payloads.source_row_id, source_payloads.contributing_row_ids, 
                        source_payloads.data_payload as s_data_payload, source_payloads.ephemeral_payload as s_ephemeral_payload,
                        source_payloads.causal_id as s_causal_id, source_payloads.causal_id as direct_source_causal_id,
                        source_payloads.valid_from AS s_valid_from, source_payloads.valid_until AS s_valid_until
                    FROM all_segments seg
                    
                        LEFT JOIN LATERAL (
                            WITH RECURSIVE ordered_sources AS (
                                SELECT
                                    source_row.source_row_id, source_row.data_payload, source_row.ephemeral_payload,
                                    source_row.valid_from, source_row.valid_until, source_row.causal_id,
                                    row_number() OVER (ORDER BY source_row.source_row_id) as rn
                                FROM active_source_rows source_row
                                WHERE 
                ((seg.is_new_entity AND source_row.grouping_key = seg.grouping_key)
                OR
                (NOT seg.is_new_entity AND (source_row.id = seg.id OR (source_row.id IS NULL AND seg.id IS NULL))))
             -- v_lateral_join_sr_to_seg
                                AND daterange(seg.valid_from, seg.valid_until) <@ daterange(source_row.valid_from, source_row.valid_until)
                            ),
                            running_payload AS (
                                SELECT rn, source_row_id, data_payload, ephemeral_payload, valid_from, valid_until, causal_id, ARRAY[source_row_id::BIGINT] as contributing_row_ids
                                FROM ordered_sources WHERE rn = 1
                                UNION ALL
                                SELECT
                                    s.rn, s.source_row_id,
                                    r.data_payload || jsonb_strip_nulls(s.data_payload),
                                    r.ephemeral_payload || jsonb_strip_nulls(s.ephemeral_payload),
                                    s.valid_from, s.valid_until, s.causal_id,
                                    r.contributing_row_ids || s.source_row_id::BIGINT
                                FROM running_payload r JOIN ordered_sources s ON s.rn = r.rn + 1
                            )
                            SELECT source_row_id, data_payload, ephemeral_payload, valid_from, valid_until, causal_id, contributing_row_ids
                            FROM running_payload
                            ORDER BY rn DESC
                            LIMIT 1
                        ) source_payloads ON true
                    
                    WHERE (source_payloads.data_payload IS NOT NULL OR seg.t_data_payload IS NOT NULL)
                    AND CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                        WHEN 'PATCH_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        WHEN 'REPLACE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        WHEN 'DELETE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        WHEN 'UPDATE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        ELSE true
                    END
                ) with_base_payload
            
NOTICE:  Nested Loop Left Join (actual rows=3.00 loops=1)
NOTICE:    Filter: ((running_payload.data_payload IS NOT NULL) OR (existing_segments_with_target.t_data_payload IS NOT NULL))
NOTICE:    Buffers: local hit=4
NOTICE:    ->  Append (actual rows=3.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on existing_segments_with_target (actual rows=3.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:          ->  Seq Scan on new_segments_no_target (actual rows=0.00 loops=1)
NOTICE:    ->  Limit (actual rows=0.33 loops=3)
NOTICE:          Buffers: local hit=3
NOTICE:          CTE ordered_sources
NOTICE:            ->  WindowAgg (actual rows=0.33 loops=3)
NOTICE:                  Window: w1 AS (ORDER BY source_row.source_row_id ROWS UNBOUNDED PRECEDING)
NOTICE:                  Storage: Memory  Maximum Storage: 17kB
NOTICE:                  Buffers: local hit=3
NOTICE:                  ->  Sort (actual rows=0.33 loops=3)
NOTICE:                        Sort Key: source_row.source_row_id
NOTICE:                        Sort Method: quicksort  Memory: 25kB
NOTICE:                        Buffers: local hit=3
NOTICE:                        ->  Seq Scan on active_source_rows source_row (actual rows=0.33 loops=3)
NOTICE:                              Filter: (((existing_segments_with_target.is_new_entity AND (grouping_key = existing_segments_with_target.grouping_key)) OR ((NOT existing_segments_with_target.is_new_entity) AND ((id = existing_segments_with_target.id) OR ((id IS NULL) AND (existing_segments_with_target.id IS NULL))))) AND (daterange(existing_segments_with_target.valid_from, existing_segments_with_target.valid_until) <@ daterange(valid_from, valid_until)))
NOTICE:                              Rows Removed by Filter: 1
NOTICE:                              Buffers: local hit=3
NOTICE:          CTE running_payload
NOTICE:            ->  Recursive Union (actual rows=0.33 loops=3)
NOTICE:                  Storage: Memory  Maximum Storage: 33kB
NOTICE:                  Buffers: local hit=3
NOTICE:                  ->  CTE Scan on ordered_sources (actual rows=0.33 loops=3)
NOTICE:                        Filter: (rn = 1)
NOTICE:                        Storage: Memory  Maximum Storage: 17kB
NOTICE:                        Buffers: local hit=3
NOTICE:                  ->  Hash Join (actual rows=0.00 loops=3)
NOTICE:                        Hash Cond: ((r.rn + 1) = s.rn)
NOTICE:                        ->  WorkTable Scan on running_payload r (actual rows=0.50 loops=2)
NOTICE:                        ->  Hash (actual rows=0.50 loops=2)
NOTICE:                              Buckets: 1024  Batches: 1  Memory Usage: 9kB
NOTICE:                              ->  CTE Scan on ordered_sources s (actual rows=0.50 loops=2)
NOTICE:                                    Storage: Memory  Maximum Storage: 17kB
NOTICE:          ->  Sort (actual rows=0.33 loops=3)
NOTICE:                Sort Key: running_payload.rn DESC
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                Buffers: local hit=3
NOTICE:                ->  CTE Scan on running_payload (actual rows=0.33 loops=3)
NOTICE:                      Storage: Memory  Maximum Storage: 17kB
NOTICE:                      Buffers: local hit=3
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=95
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments_with_propagated_ids ON COMMIT DROP AS
                SELECT
                    *,
                    FIRST_VALUE(canonical_nk_json) OVER (PARTITION BY grouping_key ORDER BY valid_from) as unified_canonical_nk_json,
                    COALESCE(
                        contributing_row_ids,
                        (array_concat_agg(contributing_row_ids) FILTER (WHERE contributing_row_ids IS NOT NULL) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (array_concat_agg(contributing_row_ids) FILTER (WHERE contributing_row_ids IS NOT NULL) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_contributing_row_ids,
                    COALESCE(
                        s_valid_from,
                        (max(s_valid_from) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (max(s_valid_from) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_s_valid_from,
                    COALESCE(
                        s_valid_until,
                        (max(s_valid_until) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (max(s_valid_until) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_s_valid_until
                FROM (
                    SELECT
                        *,
                        sum(CASE WHEN source_row_id IS NOT NULL THEN 1 ELSE 0 END) OVER (PARTITION BY grouping_key, t_valid_from ORDER BY valid_from) AS look_behind_grp,
                        sum(CASE WHEN source_row_id IS NOT NULL THEN 1 ELSE 0 END) OVER (PARTITION BY grouping_key, t_valid_from ORDER BY valid_from DESC) AS look_ahead_grp
                    FROM resolved_atomic_segments_with_payloads /* v_resolver_from */
                ) with_grp
            
NOTICE:  WindowAgg (actual rows=3.00 loops=1)
NOTICE:    Window: w3 AS (PARTITION BY with_grp.grouping_key ORDER BY with_grp.valid_from)
NOTICE:    Storage: Memory  Maximum Storage: 17kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Incremental Sort (actual rows=3.00 loops=1)
NOTICE:          Sort Key: with_grp.grouping_key, with_grp.valid_from
NOTICE:          Presorted Key: with_grp.grouping_key
NOTICE:          Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:                Window: w2 AS (PARTITION BY with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_behind_grp)
NOTICE:                Storage: Memory  Maximum Storage: 17kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Incremental Sort (actual rows=3.00 loops=1)
NOTICE:                      Sort Key: with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_behind_grp
NOTICE:                      Presorted Key: with_grp.grouping_key, with_grp.t_valid_from
NOTICE:                      Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:                            Window: w1 AS (PARTITION BY with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_ahead_grp)
NOTICE:                            Storage: Memory  Maximum Storage: 17kB
NOTICE:                            Buffers: local hit=1
NOTICE:                            ->  Incremental Sort (actual rows=3.00 loops=1)
NOTICE:                                  Sort Key: with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_ahead_grp
NOTICE:                                  Presorted Key: with_grp.grouping_key, with_grp.t_valid_from
NOTICE:                                  Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:                                  Buffers: local hit=1
NOTICE:                                  ->  Subquery Scan on with_grp (actual rows=3.00 loops=1)
NOTICE:                                        Buffers: local hit=1
NOTICE:                                        ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:                                              Window: w2 AS (PARTITION BY resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from ORDER BY resolved_atomic_segments_with_payloads.valid_from)
NOTICE:                                              Storage: Memory  Maximum Storage: 17kB
NOTICE:                                              Buffers: local hit=1
NOTICE:                                              ->  Incremental Sort (actual rows=3.00 loops=1)
NOTICE:                                                    Sort Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from, resolved_atomic_segments_with_payloads.valid_from
NOTICE:                                                    Presorted Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from
NOTICE:                                                    Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:                                                    Buffers: local hit=1
NOTICE:                                                    ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:                                                          Window: w1 AS (PARTITION BY resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from ORDER BY resolved_atomic_segments_with_payloads.valid_from)
NOTICE:                                                          Storage: Memory  Maximum Storage: 17kB
NOTICE:                                                          Buffers: local hit=1
NOTICE:                                                          ->  Sort (actual rows=3.00 loops=1)
NOTICE:                                                                Sort Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from, resolved_atomic_segments_with_payloads.valid_from DESC
NOTICE:                                                                Sort Method: quicksort  Memory: 25kB
NOTICE:                                                                Buffers: local hit=1
NOTICE:                                                                ->  Seq Scan on resolved_atomic_segments_with_payloads (actual rows=3.00 loops=1)
NOTICE:                                                                      Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=58
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments ON COMMIT DROP AS
                SELECT
                    grouping_key, id,  stable_identity_columns_are_null, natural_identity_column_values_are_null, is_new_entity, is_identifiable, is_ambiguous, conflicting_ids, unified_canonical_nk_json,
                    valid_from, valid_until, t_valid_from, t_valid_until, propagated_s_valid_from, propagated_s_valid_until, propagated_contributing_row_ids, causal_id, propagated_stable_pk_payload as stable_pk_payload,
                    propagated_contributing_row_ids IS NULL AS unaffected_target_only_segment,
                    s_data_payload, t_data_payload,
                    sql_saga.get_allen_relation(propagated_s_valid_from, propagated_s_valid_until, t_valid_from, t_valid_until) AS s_t_relation,
                    COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb) as data_payload,
                    md5((jsonb_strip_nulls(COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb)))::text) as data_hash,
                    CASE 
                        WHEN s_data_payload IS NULL THEN t_ephemeral_payload
                        ELSE COALESCE(t_ephemeral_payload, '{}'::jsonb) || COALESCE(s_ephemeral_payload, '{}'::jsonb)
                    END as ephemeral_payload,
                    CASE WHEN 'f'::boolean
                        THEN trace || jsonb_build_object( 'cte', 'ras', 'propagated_stable_pk_payload', propagated_stable_pk_payload, 'final_data_payload', COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb), 'final_ephemeral_payload', CASE WHEN s_data_payload IS NULL THEN t_ephemeral_payload ELSE COALESCE(t_ephemeral_payload, '{}'::jsonb) || COALESCE(s_ephemeral_payload, '{}'::jsonb) END )
                        ELSE NULL
                    END as trace,
                    CASE WHEN s_data_payload IS NOT NULL THEN 1 ELSE 2 END as priority
                FROM resolved_atomic_segments_with_propagated_ids
            
NOTICE:  Seq Scan on resolved_atomic_segments_with_propagated_ids (actual rows=3.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=70
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE island_group ON COMMIT DROP AS
                SELECT
                    *,
                    SUM(is_island_start) OVER (PARTITION BY grouping_key ORDER BY valid_from) as island_group_id
                FROM (
                    SELECT
                        *,
                        CASE
                            WHEN prev_valid_until IS NULL
                            OR prev_valid_until <> valid_from
                            OR (prev_data_hash IS DISTINCT FROM data_hash)
                            THEN 1
                            ELSE 0
                        END as is_island_start
                    FROM (
                        SELECT
                            *,
                            LAG(valid_until) OVER w as prev_valid_until,
                            LAG(data_hash) OVER w as prev_data_hash,
                            LAG(data_payload) OVER w as prev_data_payload
                        FROM resolved_atomic_segments ras
                        WHERE ras.data_payload IS NOT NULL
                        WINDOW w AS (PARTITION BY grouping_key ORDER BY valid_from)
                    ) s1
                ) s2
            
NOTICE:  WindowAgg (actual rows=3.00 loops=1)
NOTICE:    Window: w1 AS (PARTITION BY s1.grouping_key ORDER BY s1.valid_from)
NOTICE:    Storage: Memory  Maximum Storage: 17kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Subquery Scan on s1 (actual rows=3.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:          ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:                Window: w AS (PARTITION BY ras.grouping_key ORDER BY ras.valid_from)
NOTICE:                Storage: Memory  Maximum Storage: 17kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Sort (actual rows=3.00 loops=1)
NOTICE:                      Sort Key: ras.grouping_key, ras.valid_from
NOTICE:                      Sort Method: quicksort  Memory: 25kB
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  Seq Scan on resolved_atomic_segments ras (actual rows=3.00 loops=1)
NOTICE:                            Filter: (data_payload IS NOT NULL)
NOTICE:                            Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=56
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE coalesced_final_segments ON COMMIT DROP AS
                SELECT
                    grouping_key, max(id) as id,
                    sql_saga.first(causal_id ORDER BY valid_from) as causal_id,
                    sql_saga.first(stable_identity_columns_are_null ORDER BY valid_from) as stable_identity_columns_are_null,
                    sql_saga.first(natural_identity_column_values_are_null ORDER BY valid_from) as natural_identity_column_values_are_null,
                    sql_saga.first(is_new_entity ORDER BY valid_from) as is_new_entity,
                    sql_saga.first(is_identifiable ORDER BY valid_from) as is_identifiable,
                    sql_saga.first(is_ambiguous ORDER BY valid_from) as is_ambiguous,
                    sql_saga.first(conflicting_ids ORDER BY valid_from) as conflicting_ids,
                    sql_saga.first(unified_canonical_nk_json ORDER BY valid_from) as canonical_nk_json,
                    sql_saga.first(s_t_relation ORDER BY valid_from) as s_t_relation,
                    sql_saga.first(t_valid_from ORDER BY valid_from) as ancestor_valid_from,
                    MIN(valid_from) as valid_from,
                    MAX(valid_until) as valid_until,
                    (sql_saga.first(data_payload ORDER BY valid_from DESC) || sql_saga.first(ephemeral_payload ORDER BY valid_from DESC)) as data_payload,
                    sql_saga.first(stable_pk_payload ORDER BY valid_from DESC) as stable_pk_payload,
                    bool_and(unaffected_target_only_segment) as unaffected_target_only_segment,
                    (SELECT array_agg(DISTINCT e) FROM unnest(array_concat_agg(propagated_contributing_row_ids)) e WHERE e IS NOT NULL) as row_ids,
                    CASE WHEN 'f'::boolean
                        THEN jsonb_build_object( 'cte', 'coalesced', 'island_group_id', island_group_id, 'coalesced_stable_pk_payload', sql_saga.first(stable_pk_payload ORDER BY valid_from DESC), 'final_payload', sql_saga.first(data_payload ORDER BY valid_from DESC), 'final_payload_sans_ephemeral', sql_saga.first(data_payload - '{}'::text[] ORDER BY valid_from DESC), 'atomic_traces', jsonb_agg((trace || jsonb_build_object('data_hash', data_hash, 'prev_data_hash', prev_data_hash, 'prev_data_payload', prev_data_payload)) ORDER BY valid_from) )
                        ELSE NULL
                    END as trace
                FROM island_group
                GROUP BY grouping_key, island_group_id
            
NOTICE:  GroupAggregate (actual rows=3.00 loops=1)
NOTICE:    Group Key: island_group.grouping_key, island_group.island_group_id
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=3.00 loops=1)
NOTICE:          Sort Key: island_group.grouping_key, island_group.island_group_id, island_group.valid_from
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on island_group (actual rows=3.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:    SubPlan 1
NOTICE:      ->  Aggregate (actual rows=1.00 loops=3)
NOTICE:            ->  Sort (actual rows=1.00 loops=3)
NOTICE:                  Sort Key: e.e
NOTICE:                  Sort Method: quicksort  Memory: 25kB
NOTICE:                  ->  Function Scan on unnest e (actual rows=1.00 loops=3)
NOTICE:                        Filter: (e IS NOT NULL)
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=68
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE diff ON COMMIT DROP AS
                SELECT
                    final_seg.grouping_key, COALESCE(final_seg.id, target_seg.id) as id, COALESCE(final_seg.causal_id, target_seg.causal_id) as causal_id,
                    COALESCE(final_seg.is_new_entity, false) as is_new_entity,
                    COALESCE(final_seg.is_identifiable, true) as is_identifiable,
                    COALESCE(final_seg.is_ambiguous, false) as is_ambiguous,
                    final_seg.conflicting_ids,
                    final_seg.canonical_nk_json,
                    COALESCE(final_seg.stable_identity_columns_are_null, false) as stable_identity_columns_are_null,
                    COALESCE(final_seg.natural_identity_column_values_are_null, false) as natural_identity_column_values_are_null,
                    final_seg.valid_from AS f_from, final_seg.valid_until AS f_until, final_seg.data_payload AS f_data, final_seg.row_ids AS f_row_ids, final_seg.stable_pk_payload, final_seg.s_t_relation,
                    CASE WHEN 'f'::boolean
                        THEN final_seg.trace || jsonb_build_object('cte', 'diff', 'diff_stable_pk_payload', final_seg.stable_pk_payload, 'final_seg_causal_id', final_seg.causal_id, 'final_payload_vs_target_payload', jsonb_build_object('f', final_seg.data_payload, 't', target_seg.data_payload))
                        ELSE NULL
                    END as trace,
                    final_seg.unaffected_target_only_segment,
                    target_seg.valid_from as t_from, target_seg.valid_until as t_until, (target_seg.data_payload || target_seg.ephemeral_payload) as t_data,
                    sql_saga.get_allen_relation(target_seg.valid_from, target_seg.valid_until, final_seg.valid_from, final_seg.valid_until) as b_a_relation
                FROM coalesced_final_segments AS final_seg
                FULL OUTER JOIN target_rows AS target_seg ON final_seg.grouping_key = ('existing_entity__' || COALESCE(target_seg.id::text, '_NULL_')) AND final_seg.ancestor_valid_from = target_seg.valid_from
            
NOTICE:  Hash Full Join (actual rows=3.00 loops=1)
NOTICE:    Hash Cond: ((final_seg.grouping_key = ('existing_entity__'::text || COALESCE((target_seg.id)::text, '_NULL_'::text))) AND (final_seg.ancestor_valid_from = target_seg.valid_from))
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on coalesced_final_segments final_seg (actual rows=3.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Hash (actual rows=1.00 loops=1)
NOTICE:          Buckets: 1024  Batches: 1  Memory Usage: 9kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on target_rows target_seg (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=42
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE diff_ranked ON COMMIT DROP AS
                SELECT
                    d.*,
                    CASE
                        WHEN d.t_from IS NULL OR d.f_from IS NULL THEN NULL
                        WHEN d.f_from = d.t_from AND d.f_until = d.t_until AND d.f_data IS NOT DISTINCT FROM d.t_data THEN NULL
                        ELSE
                            row_number() OVER (
                                PARTITION BY d.grouping_key, d.t_from
                                ORDER BY
                                    CASE WHEN d.f_from = d.t_from THEN 1 ELSE 2 END,
                                    CASE WHEN d.f_data - '{}'::text[] IS NOT DISTINCT FROM d.t_data - '{}'::text[] THEN 1 ELSE 2 END,
                                    d.f_from,
                                    d.f_until
                            )
                    END as update_rank
                FROM diff d
            
NOTICE:  WindowAgg (actual rows=3.00 loops=1)
NOTICE:    Window: w1 AS (PARTITION BY grouping_key, t_from ORDER BY (CASE WHEN (f_from = t_from) THEN 1 ELSE 2 END), (CASE WHEN (NOT ((f_data - '{}'::text[]) IS DISTINCT FROM (t_data - '{}'::text[]))) THEN 1 ELSE 2 END), f_from, f_until ROWS UNBOUNDED PRECEDING)
NOTICE:    Storage: Memory  Maximum Storage: 17kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=3.00 loops=1)
NOTICE:          Sort Key: grouping_key, t_from, (CASE WHEN (f_from = t_from) THEN 1 ELSE 2 END), (CASE WHEN (NOT ((f_data - '{}'::text[]) IS DISTINCT FROM (t_data - '{}'::text[]))) THEN 1 ELSE 2 END), f_from, f_until
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on diff d (actual rows=3.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=46
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE plan_with_op ON COMMIT DROP AS
                (
                    SELECT * FROM (
                        SELECT
                            d.f_row_ids as row_ids, d.s_t_relation, d.is_new_entity,
                            CASE
                                WHEN d.is_ambiguous THEN 'ERROR'::sql_saga.temporal_merge_plan_action
                                WHEN d.is_new_entity AND NOT d.is_identifiable THEN 'ERROR'::sql_saga.temporal_merge_plan_action
                                WHEN d.t_from IS NULL THEN 'INSERT'::sql_saga.temporal_merge_plan_action
                                WHEN d.f_from IS NULL THEN 'DELETE'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank = 1 THEN 'UPDATE'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank > 1 THEN 'INSERT'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank IS NULL THEN
                                    CASE
                                        WHEN d.unaffected_target_only_segment THEN NULL
                                        ELSE 'SKIP_IDENTICAL'::sql_saga.temporal_merge_plan_action
                                    END
                                ELSE 'ERROR'::sql_saga.temporal_merge_plan_action
                            END as operation,
                            id,
                            CASE
                                WHEN d.is_new_entity AND d.canonical_nk_json IS NOT NULL
                                THEN d.canonical_nk_json || COALESCE(d.stable_pk_payload, '{}'::jsonb)
                                ELSE jsonb_build_object('id', d.id) || COALESCE(d.stable_pk_payload, '{}'::jsonb)
                            END as entity_keys_json,
                            jsonb_build_object('id', d.id) as identity_keys,
                            jsonb_build_object() as lookup_keys,
                            d.causal_id, d.t_from as old_valid_from, d.t_until as old_valid_until, d.f_from as new_valid_from, d.f_until as new_valid_until,
                            CASE
                                WHEN d.is_ambiguous THEN NULL
                                WHEN d.is_new_entity AND NOT d.is_identifiable THEN NULL
                                ELSE d.f_data
                            END as data,
                            CASE
                                WHEN d.is_ambiguous
                                THEN jsonb_build_object('error', format('Source row is ambiguous. It matches multiple distinct target entities: %s', d.conflicting_ids))
                                WHEN d.is_new_entity AND NOT d.is_identifiable
                                THEN jsonb_build_object('error', 'Source row is unidentifiable. It has NULL for all stable identity columns ' || replace('{id}'::text, '"', '') || ' and all natural keys ' || replace(NULL::text, '"', ''))
                                ELSE NULL
                            END as feedback,
                            d.b_a_relation, d.grouping_key,
                            CASE WHEN 'f'::boolean
                                THEN d.trace || jsonb_build_object( 'cte', 'plan_with_op', 'diff_is_new_entity', d.is_new_entity, 'diff_causal_id', d.causal_id, 'entity_keys_from_key_cols', jsonb_build_object('id', d.id), 'entity_keys_from_stable_pk', d.stable_pk_payload, 'final_entity_id_json', jsonb_build_object('id', d.id) || COALESCE(d.stable_pk_payload, '{}'::jsonb) )
                                ELSE NULL
                            END as trace
                        FROM diff_ranked d
                        WHERE d.f_row_ids IS NOT NULL OR d.t_data IS NOT NULL
                    ) with_op
                    WHERE with_op.operation IS NOT NULL
                )
                UNION ALL
                (
                    SELECT
                        ARRAY[source_row.source_row_id::BIGINT],
                        NULL::sql_saga.allen_interval_relation, source_row.is_new_entity,
                        COALESCE(
                            (source_row.early_feedback->>'operation')::sql_saga.temporal_merge_plan_action,
                            CASE
                                WHEN 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode = 'INSERT_NEW_ENTITIES' AND source_row.target_entity_exists THEN 'SKIP_FILTERED'::sql_saga.temporal_merge_plan_action
                                WHEN 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode IN ('PATCH_FOR_PORTION_OF', 'REPLACE_FOR_PORTION_OF', 'DELETE_FOR_PORTION_OF', 'UPDATE_FOR_PORTION_OF') AND NOT source_row.target_entity_exists THEN 'SKIP_NO_TARGET'::sql_saga.temporal_merge_plan_action
                                ELSE 'ERROR'::sql_saga.temporal_merge_plan_action
                            END
                        ),
                        id,
                        CASE
                            WHEN source_row.is_new_entity AND source_row.canonical_nk_json IS NOT NULL
                            THEN source_row.canonical_nk_json || COALESCE(source_row.stable_pk_payload, '{}'::jsonb)
                            ELSE jsonb_build_object('id', source_row.id)
                        END as entity_keys_json,
                        jsonb_build_object('id', source_row.id) as identity_keys, jsonb_build_object() as lookup_keys,
                        source_row.causal_id,
                        NULL, NULL, NULL, NULL, NULL,
                        CASE
                            WHEN source_row.early_feedback IS NOT NULL THEN jsonb_build_object('error', source_row.early_feedback->>'message')
                            ELSE jsonb_build_object('info', 'Source row was correctly filtered by the mode''s logic and did not result in a DML operation.')
                        END,
                        NULL,
                        CASE
                    WHEN source_row.is_new_entity
                    THEN 'new_entity__' || source_row.causal_id::text
                    ELSE 'existing_entity__' || COALESCE(source_row.id::text, '_NULL_')
                END AS grouping_key,
                        NULL::jsonb
                    FROM source_rows_with_early_feedback source_row
                    WHERE
                        source_row.early_feedback IS NOT NULL
                        OR NOT (
                            CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                            WHEN 'MERGE_ENTITY_PATCH' THEN true
                            WHEN 'MERGE_ENTITY_REPLACE' THEN true
                            WHEN 'MERGE_ENTITY_UPSERT' THEN true
                            WHEN 'INSERT_NEW_ENTITIES' THEN NOT source_row.target_entity_exists
                            WHEN 'PATCH_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'REPLACE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'DELETE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'UPDATE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            ELSE false
                        END
                    )
                )
            
NOTICE:  Append (actual rows=3.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on diff_ranked d (actual rows=3.00 loops=1)
NOTICE:          Filter: (((f_row_ids IS NOT NULL) OR (t_data IS NOT NULL)) AND (CASE WHEN is_ambiguous THEN 'ERROR'::sql_saga.temporal_merge_plan_action WHEN (is_new_entity AND (NOT is_identifiable)) THEN 'ERROR'::sql_saga.temporal_merge_plan_action WHEN (t_from IS NULL) THEN 'INSERT'::sql_saga.temporal_merge_plan_action WHEN (f_from IS NULL) THEN 'DELETE'::sql_saga.temporal_merge_plan_action WHEN (update_rank = 1) THEN 'UPDATE'::sql_saga.temporal_merge_plan_action WHEN (update_rank > 1) THEN 'INSERT'::sql_saga.temporal_merge_plan_action WHEN (update_rank IS NULL) THEN CASE WHEN unaffected_target_only_segment THEN NULL::sql_saga.temporal_merge_plan_action ELSE 'SKIP_IDENTICAL'::sql_saga.temporal_merge_plan_action END ELSE 'ERROR'::sql_saga.temporal_merge_plan_action END IS NOT NULL))
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Seq Scan on source_rows_with_early_feedback source_row (actual rows=0.00 loops=1)
NOTICE:          Filter: (early_feedback IS NOT NULL)
NOTICE:          Rows Removed by Filter: 1
NOTICE:          Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=50
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE plan ON COMMIT DROP AS
                SELECT
                    p.row_ids, p.operation, p.causal_id, p.is_new_entity,
                    p.id,
                    p.entity_keys_json as entity_keys,
                    p.identity_keys, p.lookup_keys,
                    p.s_t_relation, p.b_a_relation, p.old_valid_from, p.old_valid_until,
                    p.new_valid_from, p.new_valid_until, p.data, p.feedback, p.trace,
                    p.grouping_key,
                    CASE
                        WHEN p.operation <> 'UPDATE' THEN NULL::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from = p.old_valid_from AND p.new_valid_until = p.old_valid_until THEN 'NONE'::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from <= p.old_valid_from AND p.new_valid_until >= p.old_valid_until THEN 'GROW'::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from >= p.old_valid_from AND p.new_valid_until <= p.old_valid_until THEN 'SHRINK'::sql_saga.temporal_merge_update_effect
                        ELSE 'MOVE'::sql_saga.temporal_merge_update_effect
                    END AS update_effect
                FROM plan_with_op p
                LEFT JOIN source_rows_with_new_flag source_row ON source_row.source_row_id = p.row_ids[1]
            
NOTICE:  Hash Right Join (actual rows=3.00 loops=1)
NOTICE:    Hash Cond: (source_row.source_row_id = p.row_ids[1])
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_rows_with_new_flag source_row (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Hash (actual rows=3.00 loops=1)
NOTICE:          Buckets: 1024  Batches: 1  Memory Usage: 9kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on plan_with_op p (actual rows=3.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=42
NOTICE:  ---
NOTICE:  Explaining: 
                WITH RECURSIVE ordered_plan AS (
                    SELECT
                        row_number() OVER ( ORDER BY p.grouping_key, p.id, CASE p.operation WHEN 'DELETE' THEN 1 WHEN 'UPDATE' THEN 2 WHEN 'INSERT' THEN 3 ELSE 4 END, p.update_effect NULLS FIRST, CASE WHEN p.update_effect = 'MOVE' THEN 1 ELSE 0 END, CASE WHEN p.update_effect = 'MOVE' THEN COALESCE(p.old_valid_from, p.new_valid_from) END DESC NULLS LAST, COALESCE(p.old_valid_from, p.new_valid_from), (p.row_ids[1]) )::BIGINT as plan_op_seq,
                        p.*
                    FROM plan p
                ),
                -- For MOVE operations, group into the largest possible batches that can execute together.
                -- Within a single UPDATE statement, all rows see the pre-update state (MVCC snapshot).
                -- So if MOVE A's new_range overlaps MOVE B's old_range, they cannot be in the same statement.
                --
                -- Algorithm: Process MOVEs in plan_op_seq order (which is old_valid_from DESC).
                -- Track a multirange of old_ranges in the current batch. For each MOVE:
                -- - If new_range overlaps the batch's multirange, start a new batch
                -- - Add this MOVE's old_range to the batch's multirange
                --
                -- We use a recursive CTE to accumulate the multirange with reset-on-conflict logic.
                moves_ordered AS (
                    SELECT
                        o.plan_op_seq,
                        o.grouping_key,
                        o.old_valid_from,
                        o.old_valid_until,
                        o.new_valid_from,
                        o.new_valid_until,
                        row_number() OVER (PARTITION BY o.grouping_key ORDER BY o.plan_op_seq) AS move_seq
                    FROM ordered_plan o
                    WHERE o.operation = 'UPDATE' AND o.update_effect = 'MOVE'
                ),
                moves_with_batch AS (
                    -- Base case: first MOVE in each entity starts batch 0
                    SELECT 
                        plan_op_seq, grouping_key, old_valid_from, old_valid_until, 
                        new_valid_from, new_valid_until, move_seq,
                        0::bigint AS move_batch,
                        datemultirange(daterange(old_valid_from, old_valid_until, '[)')) AS batch_old_ranges
                    FROM moves_ordered
                    WHERE move_seq = 1
                    
                    UNION ALL
                    
                    -- Recursive case: check if new MOVE conflicts with batch's multirange
                    SELECT
                        m.plan_op_seq, m.grouping_key, m.old_valid_from, m.old_valid_until,
                        m.new_valid_from, m.new_valid_until, m.move_seq,
                        CASE 
                            WHEN daterange(m.new_valid_from, m.new_valid_until, '[)') && prev.batch_old_ranges
                            THEN prev.move_batch + 1  -- Conflict: start new batch
                            ELSE prev.move_batch      -- No conflict: same batch
                        END AS move_batch,
                        CASE 
                            WHEN daterange(m.new_valid_from, m.new_valid_until, '[)') && prev.batch_old_ranges
                            THEN datemultirange(daterange(m.old_valid_from, m.old_valid_until, '[)'))  -- Reset multirange
                            ELSE prev.batch_old_ranges + datemultirange(daterange(m.old_valid_from, m.old_valid_until, '[)'))  -- Add to multirange
                        END AS batch_old_ranges
                    FROM moves_ordered m
                    JOIN moves_with_batch prev 
                      ON prev.grouping_key = m.grouping_key 
                     AND prev.move_seq = m.move_seq - 1
                ),
                with_statement_seq AS (
                    SELECT
                        o.plan_op_seq,
                        CASE
                            WHEN o.operation = 'DELETE' THEN 1
                            WHEN o.operation = 'UPDATE' AND o.update_effect IN ('NONE', 'SHRINK') THEN 2
                            WHEN o.operation = 'UPDATE' AND o.update_effect = 'MOVE' THEN
                                -- Base of 3 for first MOVE batch, +1 for each subsequent batch
                                3 + COALESCE(mb.move_batch, 0)
                            WHEN o.operation = 'UPDATE' AND o.update_effect = 'GROW' THEN 1000000  -- Will be renumbered
                            WHEN o.operation = 'INSERT' THEN 2000000  -- Will be renumbered
                            ELSE 3000000
                        END::INT as raw_statement_seq,
                        o.row_ids, o.operation, o.update_effect, o.causal_id::TEXT, o.is_new_entity, o.entity_keys, o.identity_keys, o.lookup_keys, o.s_t_relation, o.b_a_relation, o.old_valid_from::TEXT,
                        o.old_valid_until::TEXT, o.new_valid_from::TEXT, o.new_valid_until::TEXT, o.data, o.feedback, CASE WHEN o.trace IS NOT NULL THEN o.trace || jsonb_build_object('final_grouping_key', o.grouping_key) ELSE NULL END as trace, o.grouping_key
                    FROM ordered_plan o
                    LEFT JOIN moves_with_batch mb ON mb.plan_op_seq = o.plan_op_seq
                )
                SELECT
                    plan_op_seq,
                    dense_rank() OVER (ORDER BY raw_statement_seq)::INT as statement_seq,
                    row_ids, operation, update_effect, causal_id, is_new_entity, entity_keys, identity_keys, lookup_keys, s_t_relation, b_a_relation, old_valid_from,
                    old_valid_until, new_valid_from, new_valid_until,
                    -- New range columns
                    CASE 
                        WHEN old_valid_from IS NOT NULL AND old_valid_until IS NOT NULL 
                        THEN daterange(old_valid_from::date, old_valid_until::date, '[)')::TEXT
                        ELSE NULL 
                    END as old_valid_range,
                    CASE 
                        WHEN new_valid_from IS NOT NULL AND new_valid_until IS NOT NULL 
                        THEN daterange(new_valid_from::date, new_valid_until::date, '[)')::TEXT
                        ELSE NULL 
                    END as new_valid_range,
                    data, feedback, trace, grouping_key
                FROM with_statement_seq
                ORDER BY plan_op_seq;
            
NOTICE:  Sort (actual rows=3.00 loops=1)
NOTICE:    Sort Key: o.plan_op_seq
NOTICE:    Sort Method: quicksort  Memory: 25kB
NOTICE:    Buffers: local hit=1
NOTICE:    CTE ordered_plan
NOTICE:      ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:            Window: w1 AS (ORDER BY p.grouping_key, p.id, (CASE p.operation WHEN 'DELETE'::sql_saga.temporal_merge_plan_action THEN 1 WHEN 'UPDATE'::sql_saga.temporal_merge_plan_action THEN 2 WHEN 'INSERT'::sql_saga.temporal_merge_plan_action THEN 3 ELSE 4 END), p.update_effect, (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN 1 ELSE 0 END), (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN COALESCE(p.old_valid_from, p.new_valid_from) ELSE NULL::date END), (COALESCE(p.old_valid_from, p.new_valid_from)), (p.row_ids[1]) ROWS UNBOUNDED PRECEDING)
NOTICE:            Storage: Memory  Maximum Storage: 17kB
NOTICE:            Buffers: local hit=1
NOTICE:            ->  Sort (actual rows=3.00 loops=1)
NOTICE:                  Sort Key: p.grouping_key, p.id, (CASE p.operation WHEN 'DELETE'::sql_saga.temporal_merge_plan_action THEN 1 WHEN 'UPDATE'::sql_saga.temporal_merge_plan_action THEN 2 WHEN 'INSERT'::sql_saga.temporal_merge_plan_action THEN 3 ELSE 4 END), p.update_effect NULLS FIRST, (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN 1 ELSE 0 END), (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN COALESCE(p.old_valid_from, p.new_valid_from) ELSE NULL::date END) DESC NULLS LAST, (COALESCE(p.old_valid_from, p.new_valid_from)), (p.row_ids[1])
NOTICE:                  Sort Method: quicksort  Memory: 25kB
NOTICE:                  Buffers: local hit=1
NOTICE:                  ->  Seq Scan on plan p (actual rows=3.00 loops=1)
NOTICE:                        Buffers: local hit=1
NOTICE:    CTE moves_ordered
NOTICE:      ->  WindowAgg (actual rows=0.00 loops=1)
NOTICE:            Window: w1 AS (PARTITION BY o_1.grouping_key ORDER BY o_1.plan_op_seq ROWS UNBOUNDED PRECEDING)
NOTICE:            ->  Incremental Sort (actual rows=0.00 loops=1)
NOTICE:                  Sort Key: o_1.grouping_key, o_1.plan_op_seq
NOTICE:                  Presorted Key: o_1.grouping_key
NOTICE:                  Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:                  ->  CTE Scan on ordered_plan o_1 (actual rows=0.00 loops=1)
NOTICE:                        Filter: ((operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect))
NOTICE:                        Rows Removed by Filter: 3
NOTICE:                        Storage: Memory  Maximum Storage: 17kB
NOTICE:    CTE moves_with_batch
NOTICE:      ->  Recursive Union (actual rows=0.00 loops=1)
NOTICE:            Storage: Memory  Maximum Storage: 33kB
NOTICE:            ->  CTE Scan on moves_ordered (actual rows=0.00 loops=1)
NOTICE:                  Filter: (move_seq = 1)
NOTICE:                  Storage: Memory  Maximum Storage: 17kB
NOTICE:            ->  Hash Join (actual rows=0.00 loops=1)
NOTICE:                  Hash Cond: ((prev.grouping_key = m.grouping_key) AND (prev.move_seq = (m.move_seq - 1)))
NOTICE:                  ->  WorkTable Scan on moves_with_batch prev (actual rows=0.00 loops=1)
NOTICE:                  ->  Hash (never executed)
NOTICE:                        ->  CTE Scan on moves_ordered m (never executed)
NOTICE:                              Storage: Memory  Maximum Storage: 17kB
NOTICE:    ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:          Window: w1 AS (ORDER BY ((CASE WHEN (o.operation = 'DELETE'::sql_saga.temporal_merge_plan_action) THEN '1'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = ANY ('{NONE,SHRINK}'::sql_saga.temporal_merge_update_effect[]))) THEN '2'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect)) THEN (3 + COALESCE(mb.move_batch, '0'::bigint)) WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'GROW'::sql_saga.temporal_merge_update_effect)) THEN '1000000'::bigint WHEN (o.operation = 'INSERT'::sql_saga.temporal_merge_plan_action) THEN '2000000'::bigint ELSE '3000000'::bigint END)::integer) ROWS UNBOUNDED PRECEDING)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Sort (actual rows=3.00 loops=1)
NOTICE:                Sort Key: ((CASE WHEN (o.operation = 'DELETE'::sql_saga.temporal_merge_plan_action) THEN '1'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = ANY ('{NONE,SHRINK}'::sql_saga.temporal_merge_update_effect[]))) THEN '2'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect)) THEN (3 + COALESCE(mb.move_batch, '0'::bigint)) WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'GROW'::sql_saga.temporal_merge_update_effect)) THEN '1000000'::bigint WHEN (o.operation = 'INSERT'::sql_saga.temporal_merge_plan_action) THEN '2000000'::bigint ELSE '3000000'::bigint END)::integer)
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Hash Left Join (actual rows=3.00 loops=1)
NOTICE:                      Hash Cond: (o.plan_op_seq = mb.plan_op_seq)
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  CTE Scan on ordered_plan o (actual rows=3.00 loops=1)
NOTICE:                            Storage: Memory  Maximum Storage: 17kB
NOTICE:                            Buffers: local hit=1
NOTICE:                      ->  Hash (actual rows=0.00 loops=1)
NOTICE:                            Buckets: 1024  Batches: 1  Memory Usage: 8kB
NOTICE:                            ->  CTE Scan on moves_with_batch mb (actual rows=0.00 loops=1)
NOTICE:                                  Storage: Memory  Maximum Storage: 17kB
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=40
\echo '--- OK: Verified that EXPLAIN plan is optimal. ---'
--- OK: Verified that EXPLAIN plan is optimal. ---
ROLLBACK TO SAVEPOINT s1;
--------------------------------------------------------------------------------
-- SCENARIO 2: Composite Natural Key (NOT NULL columns)
--------------------------------------------------------------------------------
SAVEPOINT s2;
\echo '\n--- Scenario 2: Composite Natural Key (NOT NULL columns) ---'

--- Scenario 2: Composite Natural Key (NOT NULL columns) ---
CREATE TABLE tmpc.target_nk_not_null (type TEXT NOT NULL, lu_id INT NOT NULL, value TEXT, valid_range daterange, valid_from date, valid_until date);
SELECT sql_saga.add_era('tmpc.target_nk_not_null', 'valid_range',
    valid_from_column_name => 'valid_from',
    valid_until_column_name => 'valid_until');
NOTICE:  sql_saga: Created trigger "target_nk_not_null_synchronize_temporal_columns_trigger" on table tmpc.target_nk_not_null to synchronize columns: valid_from, valid_until
 add_era 
---------
 t
(1 row)

CREATE TABLE tmpc.source_nk_not_null (row_id int, type text NOT NULL, lu_id int NOT NULL, value text, valid_from date, valid_until date);
\echo '--- Setting up tables with indexes and data for a realistic plan ---'
--- Setting up tables with indexes and data for a realistic plan ---
SELECT sql_saga.add_unique_key(table_oid => 'tmpc.target_nk_not_null'::regclass, column_names => ARRAY['type', 'lu_id'], key_type => 'primary');
NOTICE:  sql_saga: altering table tmpc.target_nk_not_null to add constraints: ALTER COLUMN type SET NOT NULL, ALTER COLUMN lu_id SET NOT NULL, ADD PRIMARY KEY (type, lu_id, valid_range WITHOUT OVERLAPS)
           add_unique_key            
-------------------------------------
 target_nk_not_null_type_lu_id_valid
(1 row)

INSERT INTO tmpc.target_nk_not_null (type, lu_id, valid_from, valid_until, value) SELECT 'A', i, '2023-01-01', '2024-01-01', 'LU' FROM generate_series(1, 1000) as i;
INSERT INTO tmpc.source_nk_not_null VALUES (1, 'A', 500, 'LU-patched', '2023-06-01', '2023-07-01');
ANALYZE tmpc.target_nk_not_null;
ANALYZE tmpc.source_nk_not_null;
\echo '\d tmpc.target_nk_not_null'
d tmpc.target_nk_not_null
\d tmpc.target_nk_not_null
             Table "tmpc.target_nk_not_null"
   Column    |   Type    | Collation | Nullable | Default 
-------------+-----------+-----------+----------+---------
 type        | text      |           | not null | 
 lu_id       | integer   |           | not null | 
 value       | text      |           |          | 
 valid_range | daterange |           | not null | 
 valid_from  | date      |           | not null | 
 valid_until | date      |           |          | 
Indexes:
    "target_nk_not_null_pkey" PRIMARY KEY (type, lu_id, valid_range WITHOUT OVERLAPS)
    "target_nk_not_null_type_lu_id_idx" btree (type, lu_id)
    "target_nk_not_null_type_lu_id_valid_range_idx" btree (type, lu_id, valid_range)
Check constraints:
    "target_nk_not_null_check" CHECK (valid_from < valid_until AND valid_from > '-infinity'::date)
    "target_nk_not_null_valid_check" CHECK (NOT isempty(valid_range))
Triggers:
    target_nk_not_null_valid_sync_temporal_trg BEFORE INSERT OR UPDATE OF valid_range, valid_from, valid_until ON tmpc.target_nk_not_null FOR EACH ROW EXECUTE FUNCTION sql_saga.tmpc_target_nk_not_null_valid_template_sync()

\echo '\d tmpc.source_nk_not_null'
d tmpc.source_nk_not_null
\d tmpc.source_nk_not_null
            Table "tmpc.source_nk_not_null"
   Column    |  Type   | Collation | Nullable | Default 
-------------+---------+-----------+----------+---------
 row_id      | integer |           |          | 
 type        | text    |           | not null | 
 lu_id       | integer |           | not null | 
 value       | text    |           |          | 
 valid_from  | date    |           |          | 
 valid_until | date    |           |          | 

\echo '\n--- Performance Monitoring: EXPLAIN the cached planner query (Natural Key, NOT NULL) ---'

--- Performance Monitoring: EXPLAIN the cached planner query (Natural Key, NOT NULL) ---
DO $$
DECLARE
    plan_sqls JSONB;
    sql_step JSONB;
    sql_stmt TEXT;
    rec RECORD;
    has_seq_scan BOOLEAN;
BEGIN
    -- Run once to populate cache. Output is discarded.
    PERFORM * FROM sql_saga.temporal_merge_plan(
        target_table => 'tmpc.target_nk_not_null'::regclass,
        source_table => 'tmpc.source_nk_not_null'::regclass,
        identity_columns => '{type,lu_id}'::text[],
        mode => 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode,
        era_name => 'valid'
    );

    -- Find the plan SQLs for the source table of this scenario.
    SELECT c.plan_sqls INTO plan_sqls
    FROM pg_temp.temporal_merge_plan_cache c
    WHERE c.cache_key LIKE '%' || 'tmpc.source_nk_not_null'::regclass::text;

    -- Clean up temp tables created by the first run so we can re-execute them.
    CALL sql_saga.temporal_merge_drop_temp_tables();

    RAISE NOTICE '--- Explaining % steps from cache', jsonb_array_length(plan_sqls);
    FOR sql_step IN SELECT * FROM jsonb_array_elements(plan_sqls)
    LOOP
        RAISE NOTICE '---';
        sql_stmt := sql_step->>'sql';

        IF sql_step->>'type' = 'setup' THEN
            RAISE NOTICE 'Executing setup: %', sql_stmt;
            EXECUTE sql_stmt;
        ELSE
            RAISE NOTICE 'Explaining: %', sql_stmt;
            has_seq_scan := false;
            FOR rec IN EXECUTE 'EXPLAIN (ANALYZE, COSTS OFF, TIMING OFF, SUMMARY OFF, BUFFERS) ' || sql_stmt
            LOOP
                RAISE NOTICE '%', rec."QUERY PLAN";
                -- We add a space to not match temp tables.
                IF rec."QUERY PLAN" LIKE '%Seq Scan on target_nk_not_null %' THEN
                    has_seq_scan := true;
                END IF;
            END LOOP;

            IF has_seq_scan THEN
                RAISE EXCEPTION 'Performance regression detected: EXPLAIN plan for non-null natural key contains a "Seq Scan on target_nk_not_null".';
            END IF;
        END IF;
    END LOOP;
END;
$$;
NOTICE:  --- Explaining 43 steps from cache
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_initial ON COMMIT DROP AS
                WITH source_data AS (
                    SELECT
                        source_table.row_id /* row_id_column */ as source_row_id,
                        source_table.row_id /* v_causal_select_expr */ as causal_id,
                        source_table.lu_id, source_table.type,  -- v_non_temporal_lookup_cols_select_list_prefix
                        source_table.valid_from as valid_from, source_table.valid_until as valid_until, /* v_source_temporal_cols_expr */
                        jsonb_strip_nulls(jsonb_build_object('value', source_table.value)) /* v_source_data_payload_expr */ AS data_payload,
                        jsonb_strip_nulls(jsonb_build_object()) /* v_source_ephemeral_payload_expr */ AS ephemeral_payload,
                        jsonb_build_object('type', source_table.type, 'lu_id', source_table.lu_id) /* v_stable_pk_cols_jsonb_build_source */ as stable_pk_payload,
                        source_table.type IS NULL AND source_table.lu_id IS NULL /* v_entity_id_check_is_null_expr */ as stable_identity_columns_are_null,
                        (true) /* v_lookup_cols_are_null_expr */ as natural_identity_column_values_are_null,
                        true /* v_is_identifiable_expr */ as is_identifiable,
                        true /* v_consistency_check_expr */ as temporal_columns_are_consistent
                    FROM tmpc.source_nk_not_null /* v_source_table_ident */ source_table
                )
                SELECT 
                    *,
                    -- OPTIMIZATION: Pre-compute range column to avoid repeated daterange() calls
                    -- This provides ~10-15% performance improvement in eclipse detection
                    daterange(valid_from, valid_until) as valid_range
                FROM source_data;
            
NOTICE:  Seq Scan on source_nk_not_null source_table (actual rows=1.00 loops=1)
NOTICE:    Buffers: shared hit=1
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_initial (source_row_id);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_initial (type, lu_id);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_initial (type, lu_id, source_row_id);
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE source_initial;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_with_eclipsed_flag ON COMMIT DROP AS
                SELECT
                    s1.*,
                    eclipse_info.is_eclipsed,
                    eclipse_info.eclipsed_by
                FROM source_initial s1
                CROSS JOIN LATERAL (
                    SELECT
                        -- OPTIMIZATION: Use pre-computed valid_range instead of repeated daterange() calls
                        -- Combined with composite index, this provides 30-40 percent total performance improvement
                        COALESCE(sql_saga.covers_without_gaps(s2.valid_range, s1.valid_range ORDER BY s2.valid_from), false) as is_eclipsed,
                        array_agg(s2.source_row_id) as eclipsed_by
                    FROM source_initial s2
                    WHERE
                        (
                            (NOT s1.natural_identity_column_values_are_null AND (false))
                            OR
                            (s1.natural_identity_column_values_are_null AND s1.causal_id = s2.causal_id)
                        )
                        AND
                        -- Only consider newer rows (higher row_id) as potential eclipsers.
                        s2.source_row_id > s1.source_row_id
                ) eclipse_info;
            
NOTICE:  Nested Loop (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_initial s1 (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Aggregate (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Sort (actual rows=0.00 loops=1)
NOTICE:                Sort Key: s2.valid_from
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Result (actual rows=0.00 loops=1)
NOTICE:                      One-Time Filter: s1.natural_identity_column_values_are_null
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  Seq Scan on source_initial s2 (actual rows=0.00 loops=1)
NOTICE:                            Filter: ((source_row_id > s1.source_row_id) AND (s1.causal_id = causal_id))
NOTICE:                            Rows Removed by Filter: 1
NOTICE:                            Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=46, local read=3
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE target_rows ON COMMIT DROP AS
                SELECT
                    lu_id, type,  /* v_non_temporal_lookup_cols_select_list_no_alias_prefix */ -- (non-temporal identity columns)
                    lower(t.valid_range) as valid_from, -- The temporal identity column (e.g., valid_from)
                    NULL::integer /* v_causal_column_type */ as causal_id, -- Target rows do not originate from a source row, so causal_id is NULL. Type is introspected.
                    jsonb_build_object('type', type, 'lu_id', lu_id) /* v_stable_pk_cols_jsonb_build_bare */ as stable_pk_payload,
                    upper(t.valid_range) as valid_until,
                    jsonb_build_object('value', value) /* v_target_data_cols_jsonb_build_bare */ AS data_payload,
                    jsonb_build_object() /* v_target_ephemeral_cols_jsonb_build_bare */ AS ephemeral_payload,
                    jsonb_strip_nulls(jsonb_build_object()) /* v_target_nk_json_expr */ AS canonical_nk_json
                FROM (
                        SELECT * FROM (
                            SELECT DISTINCT ON (u.type, u.lu_id, u.valid_from) * FROM (
                                (
                                    SELECT * FROM tmpc.target_nk_not_null inner_t
                                    WHERE (inner_t.type, inner_t.lu_id) IN (SELECT DISTINCT si.type, si.lu_id FROM source_initial si WHERE (si.type, si.lu_id) IS NOT NULL)
                                )
                            ) u
                        )
                    ) AS t  /* v_target_rows_filter */
            
NOTICE:  Subquery Scan on unnamed_subquery (actual rows=1.00 loops=1)
NOTICE:    Buffers: shared hit=3, local hit=1
NOTICE:    ->  Unique (actual rows=1.00 loops=1)
NOTICE:          Buffers: shared hit=3, local hit=1
NOTICE:          ->  Incremental Sort (actual rows=1.00 loops=1)
NOTICE:                Sort Key: inner_t.type, inner_t.lu_id, inner_t.valid_from
NOTICE:                Presorted Key: inner_t.type, inner_t.lu_id
NOTICE:                Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:                Buffers: shared hit=3, local hit=1
NOTICE:                ->  Nested Loop (actual rows=1.00 loops=1)
NOTICE:                      Buffers: shared hit=3, local hit=1
NOTICE:                      ->  Unique (actual rows=1.00 loops=1)
NOTICE:                            Buffers: local hit=1
NOTICE:                            ->  Sort (actual rows=1.00 loops=1)
NOTICE:                                  Sort Key: si.type, si.lu_id
NOTICE:                                  Sort Method: quicksort  Memory: 25kB
NOTICE:                                  Buffers: local hit=1
NOTICE:                                  ->  Seq Scan on source_initial si (actual rows=1.00 loops=1)
NOTICE:                                        Filter: ((type IS NOT NULL) AND (lu_id IS NOT NULL))
NOTICE:                                        Buffers: local hit=1
NOTICE:                      ->  Index Scan using target_nk_not_null_pkey on target_nk_not_null inner_t (actual rows=1.00 loops=1)
NOTICE:                            Index Cond: ((type = si.type) AND (lu_id = si.lu_id))
NOTICE:                            Index Searches: 1
NOTICE:                            Buffers: shared hit=3
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON target_rows (lu_id, type);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON target_rows USING gist (daterange(valid_from, valid_until));
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE target_rows;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_matches ON COMMIT DROP AS
                SELECT
                    source_row.*,
                    target_row.stable_pk_payload as discovered_stable_pk_payload,
                    target_row.lu_id AS discovered_id_1, target_row.type AS discovered_id_2 /* v_propagated_id_cols_list */
                FROM source_with_eclipsed_flag source_row
                LEFT JOIN target_rows target_row ON (source_row.type IS NOT DISTINCT FROM target_row.type AND source_row.lu_id IS NOT DISTINCT FROM target_row.lu_id /* v_source_rows_exists_join_expr */)
            
NOTICE:  Nested Loop Left Join (actual rows=1.00 loops=1)
NOTICE:    Join Filter: ((NOT (source_row.type IS DISTINCT FROM target_row.type)) AND (NOT (source_row.lu_id IS DISTINCT FROM target_row.lu_id)))
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_with_eclipsed_flag source_row (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Materialize (actual rows=1.00 loops=1)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on target_rows target_row (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=49, local read=1
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_aggregates ON COMMIT DROP AS
                SELECT
                    source_row_id,
                    count(DISTINCT discovered_stable_pk_payload) as match_count,
                    jsonb_agg(DISTINCT discovered_stable_pk_payload) as conflicting_ids
                FROM source_rows_with_matches
                GROUP BY source_row_id
            
NOTICE:  GroupAggregate (actual rows=1.00 loops=1)
NOTICE:    Group Key: source_row_id
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=1.00 loops=1)
NOTICE:          Sort Key: source_row_id, discovered_stable_pk_payload
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on source_rows_with_matches (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=42
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_discovery ON COMMIT DROP AS
                SELECT
                    m.*,
                    a.match_count,
                    a.conflicting_ids,
                    (a.match_count > 1) as is_ambiguous
                FROM source_rows_with_matches m
                JOIN source_rows_with_aggregates a ON m.source_row_id = a.source_row_id
            
NOTICE:  Hash Join (actual rows=1.00 loops=1)
NOTICE:    Hash Cond: (a.source_row_id = m.source_row_id)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_rows_with_aggregates a (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Hash (actual rows=1.00 loops=1)
NOTICE:          Buckets: 1024  Batches: 1  Memory Usage: 9kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on source_rows_with_matches m (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=10
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows ON COMMIT DROP AS
                SELECT DISTINCT ON (p.source_row_id)
                    p.source_row_id, p.causal_id, p.valid_from, p.valid_until, p.data_payload, p.ephemeral_payload,
                    (p.stable_identity_columns_are_null AND p.discovered_stable_pk_payload IS NULL) as stable_identity_columns_are_null,
                    p.natural_identity_column_values_are_null, p.is_identifiable,
                    p.is_ambiguous, p.conflicting_ids, p.is_eclipsed, p.eclipsed_by,
                    p.temporal_columns_are_consistent,
                    COALESCE(p.stable_pk_payload, p.discovered_stable_pk_payload) as stable_pk_payload,
                    (p.discovered_stable_pk_payload IS NOT NULL) /* v_target_entity_exists_expr */ as target_entity_exists,
                    COALESCE(p.lu_id, p.discovered_id_1) AS lu_id, COALESCE(p.type, p.discovered_id_2) AS type /* v_coalesced_id_cols_list */
                FROM source_rows_with_discovery p
                ORDER BY p.source_row_id, p.discovered_stable_pk_payload
            
NOTICE:  Unique (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=1.00 loops=1)
NOTICE:          Sort Key: source_row_id, discovered_stable_pk_payload
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on source_rows_with_discovery p (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=46
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_new_flag ON COMMIT DROP AS
                SELECT *, NOT target_entity_exists as is_new_entity
                FROM source_rows
            
NOTICE:  Seq Scan on source_rows (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=38
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_nk_json ON COMMIT DROP AS
                SELECT source_row.*, jsonb_strip_nulls(jsonb_build_object()) /* v_lookup_keys_as_jsonb_expr */ as nk_json, ARRAY(SELECT e FROM unnest(ARRAY[]::TEXT[]) e WHERE e IS NOT NULL ORDER BY e) /* v_lookup_keys_as_array_expr */ as nk_non_null_keys_array
                FROM source_rows_with_new_flag source_row
            
NOTICE:  Seq Scan on source_rows_with_new_flag source_row (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    InitPlan 1
NOTICE:      ->  Sort (actual rows=0.00 loops=1)
NOTICE:            Sort Key: e.e
NOTICE:            Sort Method: quicksort  Memory: 25kB
NOTICE:            ->  Function Scan on unnest e (actual rows=0.00 loops=1)
NOTICE:                  Filter: (e IS NOT NULL)
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=40
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_rows_with_nk_json USING GIN (nk_json);
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE source_rows_with_nk_json;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_canonical_key ON COMMIT DROP AS
                SELECT *, (CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (type IS NULL AND lu_id IS NULL) THEN COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_')
            END /* v_grouping_key_expr */) as grouping_key
                FROM (
                    SELECT
                        s1.*,
                        s2.nk_json as canonical_nk_json
                    FROM source_rows_with_nk_json s1
                    LEFT JOIN LATERAL (
                        SELECT s2_inner.nk_json, s2_inner.nk_non_null_keys_array
                        FROM source_rows_with_nk_json s2_inner
                        WHERE s1.is_new_entity AND s2_inner.is_new_entity AND s2_inner.nk_json @> s1.nk_json
                        ORDER BY array_length(s2_inner.nk_non_null_keys_array, 1) DESC, s2_inner.nk_non_null_keys_array::text DESC
                        LIMIT 1
                    ) s2 ON true
                ) s
            
NOTICE:  Nested Loop Left Join (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Seq Scan on source_rows_with_nk_json s1 (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Limit (actual rows=0.00 loops=1)
NOTICE:          ->  Sort (actual rows=0.00 loops=1)
NOTICE:                Sort Key: (array_length(s2_inner.nk_non_null_keys_array, 1)) DESC, ((s2_inner.nk_non_null_keys_array)::text) DESC
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                ->  Result (actual rows=0.00 loops=1)
NOTICE:                      One-Time Filter: s1.is_new_entity
NOTICE:                      ->  Seq Scan on source_rows_with_nk_json s2_inner (never executed)
NOTICE:                            Filter: (is_new_entity AND (nk_json @> s1.nk_json))
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=67, local hit=1
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_early_feedback ON COMMIT DROP AS
                SELECT
                    s.*,
                    CASE
                        WHEN s.is_ambiguous
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row is ambiguous. It matches multiple distinct target entities: ' || s.conflicting_ids::text )
                        WHEN NOT s.is_identifiable AND s.is_new_entity
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row is unidentifiable. It has NULL for all stable identity columns ' || replace('{type,lu_id}'::text, '"', '') || ' and all natural keys ' || replace(NULL::text, '"', '') )
                        WHEN NOT s.temporal_columns_are_consistent
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row has inconsistent temporal columns. Column "' || 'valid_until' || '" must be equal to column "' || NULL || '" + ' || NULL || '.' )
                        WHEN s.is_eclipsed
                        THEN jsonb_build_object( 'operation', 'SKIP_ECLIPSED'::text, 'message', 'Source row was eclipsed by row_ids=' || s.eclipsed_by::text || ' in the same batch.' )
                        ELSE NULL
                    END as early_feedback
                FROM source_rows_with_canonical_key s
            
NOTICE:  Seq Scan on source_rows_with_canonical_key s (actual rows=1.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=48
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE active_source_rows ON COMMIT DROP AS
                SELECT source_row.*
                FROM source_rows_with_early_feedback source_row
                WHERE source_row.early_feedback IS NULL
                AND CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                    WHEN 'MERGE_ENTITY_PATCH' THEN true
                    WHEN 'MERGE_ENTITY_REPLACE' THEN true
                    WHEN 'MERGE_ENTITY_UPSERT' THEN true
                    WHEN 'INSERT_NEW_ENTITIES' THEN NOT source_row.target_entity_exists
                    WHEN 'PATCH_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'REPLACE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'DELETE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'UPDATE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    ELSE false
                END
            
NOTICE:  Seq Scan on source_rows_with_early_feedback source_row (actual rows=1.00 loops=1)
NOTICE:    Filter: (early_feedback IS NULL)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=50
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON active_source_rows (lu_id, type);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON active_source_rows (grouping_key);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON active_source_rows USING gist (daterange(valid_from, valid_until));
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE active_source_rows;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE all_rows ON COMMIT DROP AS
                SELECT lu_id, type,  /* v_non_temporal_lookup_cols_select_list_no_alias_prefix */ causal_id, valid_from, valid_until, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, temporal_columns_are_consistent, canonical_nk_json FROM active_source_rows
                UNION ALL
                SELECT
                    target_row.lu_id, target_row.type,  /* v_non_temporal_tr_qualified_lookup_cols_prefix */
                    target_row.causal_id,
                    target_row.valid_from,
                    target_row.valid_until,
                    false as is_new_entity,
                    target_row.stable_pk_payload,
                    false as stable_identity_columns_are_null,
                    false as natural_identity_column_values_are_null,
                    true as is_identifiable,
                    false as is_ambiguous,
                    NULL::jsonb as conflicting_ids,
                    true as temporal_columns_are_consistent,
                    target_row.canonical_nk_json
                FROM target_rows target_row
            
NOTICE:  Append (actual rows=2.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on active_source_rows (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Seq Scan on target_rows target_row (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=56, local read=2
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points_raw ON COMMIT DROP AS
                SELECT lu_id, type, causal_id, valid_from AS point, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json FROM all_rows
                UNION ALL
                SELECT lu_id, type, causal_id, valid_until AS point, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json FROM all_rows
            
NOTICE:  Append (actual rows=4.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on all_rows (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Seq Scan on all_rows all_rows_1 (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=30
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points_unified ON COMMIT DROP AS
                SELECT
                    *,
                    CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (type IS NULL AND lu_id IS NULL) THEN COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_')
            END AS grouping_key,
                    CASE
                        WHEN is_new_entity THEN causal_id
                        ELSE FIRST_VALUE(causal_id) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (type IS NULL AND lu_id IS NULL) THEN COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS LAST)
                    END as unified_causal_id,
                    FIRST_VALUE(stable_pk_payload) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (type IS NULL AND lu_id IS NULL) THEN COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS FIRST) as unified_stable_pk_payload,
                    FIRST_VALUE(canonical_nk_json) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (type IS NULL AND lu_id IS NULL) THEN COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS FIRST) as unified_canonical_nk_json
                FROM time_points_raw
            
NOTICE:  WindowAgg (actual rows=4.00 loops=1)
NOTICE:    Window: w2 AS (PARTITION BY (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((type IS NOT NULL) OR (lu_id IS NOT NULL)) THEN ((COALESCE(type, '_NULL_'::text) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) ELSE (causal_id)::text END) ELSE ((('existing_entity__'::text || COALESCE(type, '_NULL_'::text)) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) END) ORDER BY causal_id)
NOTICE:    Storage: Memory  Maximum Storage: 17kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Incremental Sort (actual rows=4.00 loops=1)
NOTICE:          Sort Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((type IS NOT NULL) OR (lu_id IS NOT NULL)) THEN ((COALESCE(type, '_NULL_'::text) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) ELSE (causal_id)::text END) ELSE ((('existing_entity__'::text || COALESCE(type, '_NULL_'::text)) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) END), causal_id
NOTICE:          Presorted Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((type IS NOT NULL) OR (lu_id IS NOT NULL)) THEN ((COALESCE(type, '_NULL_'::text) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) ELSE (causal_id)::text END) ELSE ((('existing_entity__'::text || COALESCE(type, '_NULL_'::text)) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) END)
NOTICE:          Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  WindowAgg (actual rows=4.00 loops=1)
NOTICE:                Window: w1 AS (PARTITION BY (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((type IS NOT NULL) OR (lu_id IS NOT NULL)) THEN ((COALESCE(type, '_NULL_'::text) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) ELSE (causal_id)::text END) ELSE ((('existing_entity__'::text || COALESCE(type, '_NULL_'::text)) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) END) ORDER BY causal_id)
NOTICE:                Storage: Memory  Maximum Storage: 17kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Sort (actual rows=4.00 loops=1)
NOTICE:                      Sort Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((type IS NOT NULL) OR (lu_id IS NOT NULL)) THEN ((COALESCE(type, '_NULL_'::text) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) ELSE (causal_id)::text END) ELSE ((('existing_entity__'::text || COALESCE(type, '_NULL_'::text)) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) END), causal_id NULLS FIRST
NOTICE:                      Sort Method: quicksort  Memory: 25kB
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  Seq Scan on time_points_raw (actual rows=4.00 loops=1)
NOTICE:                            Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=26
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points_with_unified_ids ON COMMIT DROP AS
                SELECT
                    tpu.grouping_key,
                    tpu.lu_id, tpu.type,
                    tpu.unified_causal_id as causal_id,
                    tpu.point,
                    tpu.is_new_entity,
                    tpu.unified_stable_pk_payload as stable_pk_payload,
                    tpu.stable_identity_columns_are_null,
                    tpu.natural_identity_column_values_are_null,
                    tpu.is_identifiable,
                    tpu.is_ambiguous,
                    tpu.conflicting_ids,
                    tpu.unified_canonical_nk_json as canonical_nk_json
                FROM time_points_unified tpu
            
NOTICE:  Seq Scan on time_points_unified tpu (actual rows=4.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=34
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points ON COMMIT DROP AS
                SELECT DISTINCT ON (grouping_key, point) *
                FROM time_points_with_unified_ids
                ORDER BY grouping_key, point, causal_id DESC NULLS LAST
            
NOTICE:  Unique (actual rows=4.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=4.00 loops=1)
NOTICE:          Sort Key: grouping_key, point, causal_id DESC NULLS LAST
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on time_points_with_unified_ids (actual rows=4.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=28
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE atomic_segments ON COMMIT DROP AS
                SELECT grouping_key, lu_id, type, causal_id, point as valid_from, next_point as valid_until, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json
                FROM (
                    SELECT *, LEAD(point) OVER (PARTITION BY grouping_key ORDER BY point) as next_point
                    FROM time_points
                ) with_lead
                WHERE point IS NOT NULL AND next_point IS NOT NULL AND point < next_point
            
NOTICE:  Subquery Scan on with_lead (actual rows=3.00 loops=1)
NOTICE:    Filter: ((with_lead.point IS NOT NULL) AND (with_lead.next_point IS NOT NULL) AND (with_lead.point < with_lead.next_point))
NOTICE:    Rows Removed by Filter: 1
NOTICE:    Buffers: local hit=1
NOTICE:    ->  WindowAgg (actual rows=4.00 loops=1)
NOTICE:          Window: w1 AS (PARTITION BY time_points.grouping_key ORDER BY time_points.point)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Sort (actual rows=4.00 loops=1)
NOTICE:                Sort Key: time_points.grouping_key, time_points.point
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Seq Scan on time_points (actual rows=4.00 loops=1)
NOTICE:                      Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=28
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE existing_segments_with_target ON COMMIT DROP AS
                SELECT
                    seg.grouping_key, seg.canonical_nk_json, seg.lu_id, seg.type, seg.causal_id, seg.is_new_entity, seg.is_identifiable, seg.is_ambiguous, seg.conflicting_ids, seg.stable_identity_columns_are_null, seg.natural_identity_column_values_are_null, seg.valid_from, seg.valid_until,
                    target_row.valid_from as t_valid_from, target_row.valid_until as t_valid_until,
                    target_row.data_payload as t_data_payload, target_row.ephemeral_payload as t_ephemeral_payload, seg.stable_pk_payload, 
                    target_row.stable_pk_payload as target_stable_pk_payload,
                    NULL::jsonb as trace
                FROM atomic_segments seg
                LEFT JOIN target_rows target_row 
                    ON (target_row.lu_id = seg.lu_id OR (target_row.lu_id IS NULL AND seg.lu_id IS NULL)) AND (target_row.type = seg.type OR (target_row.type IS NULL AND seg.type IS NULL)) 
                    AND daterange(seg.valid_from, seg.valid_until) <@ daterange(target_row.valid_from, target_row.valid_until)
                WHERE NOT seg.is_new_entity
            
NOTICE:  Nested Loop Left Join (actual rows=3.00 loops=1)
NOTICE:    Join Filter: (((target_row.lu_id = seg.lu_id) OR ((target_row.lu_id IS NULL) AND (seg.lu_id IS NULL))) AND ((target_row.type = seg.type) OR ((target_row.type IS NULL) AND (seg.type IS NULL))) AND (daterange(seg.valid_from, seg.valid_until) <@ daterange(target_row.valid_from, target_row.valid_until)))
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on atomic_segments seg (actual rows=3.00 loops=1)
NOTICE:          Filter: (NOT is_new_entity)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Materialize (actual rows=1.00 loops=3)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on target_rows target_row (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=41
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE new_segments_no_target ON COMMIT DROP AS
                SELECT
                    seg.grouping_key, seg.canonical_nk_json, seg.lu_id, seg.type, seg.causal_id, seg.is_new_entity, seg.is_identifiable, seg.is_ambiguous, seg.conflicting_ids, seg.stable_identity_columns_are_null, seg.natural_identity_column_values_are_null, seg.valid_from, seg.valid_until,
                    NULL::date as t_valid_from, NULL::date as t_valid_until,
                    NULL::jsonb as t_data_payload, NULL::jsonb as t_ephemeral_payload, seg.stable_pk_payload,
                    NULL::jsonb as target_stable_pk_payload,
                    NULL::jsonb as trace
                FROM atomic_segments seg
                WHERE seg.is_new_entity
            
NOTICE:  Seq Scan on atomic_segments seg (actual rows=0.00 loops=1)
NOTICE:    Filter: is_new_entity
NOTICE:    Rows Removed by Filter: 3
NOTICE:    Buffers: local hit=1
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments_with_payloads ON COMMIT DROP AS
                WITH all_segments AS (
                    SELECT * FROM existing_segments_with_target
                    UNION ALL
                    SELECT * FROM new_segments_no_target
                )
                SELECT
                    with_base_payload.*,
                    with_base_payload.stable_pk_payload as propagated_stable_pk_payload
                FROM (
                    SELECT
                        seg.*, source_payloads.source_row_id, source_payloads.contributing_row_ids, 
                        source_payloads.data_payload as s_data_payload, source_payloads.ephemeral_payload as s_ephemeral_payload,
                        source_payloads.causal_id as s_causal_id, source_payloads.causal_id as direct_source_causal_id,
                        source_payloads.valid_from AS s_valid_from, source_payloads.valid_until AS s_valid_until
                    FROM all_segments seg
                    
                        LEFT JOIN LATERAL (
                            WITH RECURSIVE ordered_sources AS (
                                SELECT
                                    source_row.source_row_id, source_row.data_payload, source_row.ephemeral_payload,
                                    source_row.valid_from, source_row.valid_until, source_row.causal_id,
                                    row_number() OVER (ORDER BY source_row.source_row_id) as rn
                                FROM active_source_rows source_row
                                WHERE 
                ((seg.is_new_entity AND source_row.grouping_key = seg.grouping_key)
                OR
                (NOT seg.is_new_entity AND (source_row.lu_id = seg.lu_id OR (source_row.lu_id IS NULL AND seg.lu_id IS NULL)) AND (source_row.type = seg.type OR (source_row.type IS NULL AND seg.type IS NULL))))
             -- v_lateral_join_sr_to_seg
                                AND daterange(seg.valid_from, seg.valid_until) <@ daterange(source_row.valid_from, source_row.valid_until)
                            ),
                            running_payload AS (
                                SELECT rn, source_row_id, data_payload, ephemeral_payload, valid_from, valid_until, causal_id, ARRAY[source_row_id::BIGINT] as contributing_row_ids
                                FROM ordered_sources WHERE rn = 1
                                UNION ALL
                                SELECT
                                    s.rn, s.source_row_id,
                                    r.data_payload || jsonb_strip_nulls(s.data_payload),
                                    r.ephemeral_payload || jsonb_strip_nulls(s.ephemeral_payload),
                                    s.valid_from, s.valid_until, s.causal_id,
                                    r.contributing_row_ids || s.source_row_id::BIGINT
                                FROM running_payload r JOIN ordered_sources s ON s.rn = r.rn + 1
                            )
                            SELECT source_row_id, data_payload, ephemeral_payload, valid_from, valid_until, causal_id, contributing_row_ids
                            FROM running_payload
                            ORDER BY rn DESC
                            LIMIT 1
                        ) source_payloads ON true
                    
                    WHERE (source_payloads.data_payload IS NOT NULL OR seg.t_data_payload IS NOT NULL)
                    AND CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                        WHEN 'PATCH_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        WHEN 'REPLACE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        WHEN 'DELETE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        WHEN 'UPDATE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        ELSE true
                    END
                ) with_base_payload
            
NOTICE:  Nested Loop Left Join (actual rows=3.00 loops=1)
NOTICE:    Filter: ((running_payload.data_payload IS NOT NULL) OR (existing_segments_with_target.t_data_payload IS NOT NULL))
NOTICE:    Buffers: local hit=4
NOTICE:    ->  Append (actual rows=3.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on existing_segments_with_target (actual rows=3.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:          ->  Seq Scan on new_segments_no_target (actual rows=0.00 loops=1)
NOTICE:    ->  Limit (actual rows=0.33 loops=3)
NOTICE:          Buffers: local hit=3
NOTICE:          CTE ordered_sources
NOTICE:            ->  WindowAgg (actual rows=0.33 loops=3)
NOTICE:                  Window: w1 AS (ORDER BY source_row.source_row_id ROWS UNBOUNDED PRECEDING)
NOTICE:                  Storage: Memory  Maximum Storage: 17kB
NOTICE:                  Buffers: local hit=3
NOTICE:                  ->  Sort (actual rows=0.33 loops=3)
NOTICE:                        Sort Key: source_row.source_row_id
NOTICE:                        Sort Method: quicksort  Memory: 25kB
NOTICE:                        Buffers: local hit=3
NOTICE:                        ->  Seq Scan on active_source_rows source_row (actual rows=0.33 loops=3)
NOTICE:                              Filter: (((existing_segments_with_target.is_new_entity AND (grouping_key = existing_segments_with_target.grouping_key)) OR ((NOT existing_segments_with_target.is_new_entity) AND ((lu_id = existing_segments_with_target.lu_id) OR ((lu_id IS NULL) AND (existing_segments_with_target.lu_id IS NULL))) AND ((type = existing_segments_with_target.type) OR ((type IS NULL) AND (existing_segments_with_target.type IS NULL))))) AND (daterange(existing_segments_with_target.valid_from, existing_segments_with_target.valid_until) <@ daterange(valid_from, valid_until)))
NOTICE:                              Rows Removed by Filter: 1
NOTICE:                              Buffers: local hit=3
NOTICE:          CTE running_payload
NOTICE:            ->  Recursive Union (actual rows=0.33 loops=3)
NOTICE:                  Storage: Memory  Maximum Storage: 33kB
NOTICE:                  Buffers: local hit=3
NOTICE:                  ->  CTE Scan on ordered_sources (actual rows=0.33 loops=3)
NOTICE:                        Filter: (rn = 1)
NOTICE:                        Storage: Memory  Maximum Storage: 17kB
NOTICE:                        Buffers: local hit=3
NOTICE:                  ->  Hash Join (actual rows=0.00 loops=3)
NOTICE:                        Hash Cond: ((r.rn + 1) = s.rn)
NOTICE:                        ->  WorkTable Scan on running_payload r (actual rows=0.50 loops=2)
NOTICE:                        ->  Hash (actual rows=0.50 loops=2)
NOTICE:                              Buckets: 1024  Batches: 1  Memory Usage: 9kB
NOTICE:                              ->  CTE Scan on ordered_sources s (actual rows=0.50 loops=2)
NOTICE:                                    Storage: Memory  Maximum Storage: 17kB
NOTICE:          ->  Sort (actual rows=0.33 loops=3)
NOTICE:                Sort Key: running_payload.rn DESC
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                Buffers: local hit=3
NOTICE:                ->  CTE Scan on running_payload (actual rows=0.33 loops=3)
NOTICE:                      Storage: Memory  Maximum Storage: 17kB
NOTICE:                      Buffers: local hit=3
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=99
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments_with_propagated_ids ON COMMIT DROP AS
                SELECT
                    *,
                    FIRST_VALUE(canonical_nk_json) OVER (PARTITION BY grouping_key ORDER BY valid_from) as unified_canonical_nk_json,
                    COALESCE(
                        contributing_row_ids,
                        (array_concat_agg(contributing_row_ids) FILTER (WHERE contributing_row_ids IS NOT NULL) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (array_concat_agg(contributing_row_ids) FILTER (WHERE contributing_row_ids IS NOT NULL) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_contributing_row_ids,
                    COALESCE(
                        s_valid_from,
                        (max(s_valid_from) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (max(s_valid_from) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_s_valid_from,
                    COALESCE(
                        s_valid_until,
                        (max(s_valid_until) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (max(s_valid_until) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_s_valid_until
                FROM (
                    SELECT
                        *,
                        sum(CASE WHEN source_row_id IS NOT NULL THEN 1 ELSE 0 END) OVER (PARTITION BY grouping_key, t_valid_from ORDER BY valid_from) AS look_behind_grp,
                        sum(CASE WHEN source_row_id IS NOT NULL THEN 1 ELSE 0 END) OVER (PARTITION BY grouping_key, t_valid_from ORDER BY valid_from DESC) AS look_ahead_grp
                    FROM resolved_atomic_segments_with_payloads /* v_resolver_from */
                ) with_grp
            
NOTICE:  WindowAgg (actual rows=3.00 loops=1)
NOTICE:    Window: w3 AS (PARTITION BY with_grp.grouping_key ORDER BY with_grp.valid_from)
NOTICE:    Storage: Memory  Maximum Storage: 18kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Incremental Sort (actual rows=3.00 loops=1)
NOTICE:          Sort Key: with_grp.grouping_key, with_grp.valid_from
NOTICE:          Presorted Key: with_grp.grouping_key
NOTICE:          Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 26kB  Peak Memory: 26kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:                Window: w2 AS (PARTITION BY with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_behind_grp)
NOTICE:                Storage: Memory  Maximum Storage: 17kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Incremental Sort (actual rows=3.00 loops=1)
NOTICE:                      Sort Key: with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_behind_grp
NOTICE:                      Presorted Key: with_grp.grouping_key, with_grp.t_valid_from
NOTICE:                      Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 26kB  Peak Memory: 26kB
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:                            Window: w1 AS (PARTITION BY with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_ahead_grp)
NOTICE:                            Storage: Memory  Maximum Storage: 17kB
NOTICE:                            Buffers: local hit=1
NOTICE:                            ->  Incremental Sort (actual rows=3.00 loops=1)
NOTICE:                                  Sort Key: with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_ahead_grp
NOTICE:                                  Presorted Key: with_grp.grouping_key, with_grp.t_valid_from
NOTICE:                                  Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:                                  Buffers: local hit=1
NOTICE:                                  ->  Subquery Scan on with_grp (actual rows=3.00 loops=1)
NOTICE:                                        Buffers: local hit=1
NOTICE:                                        ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:                                              Window: w2 AS (PARTITION BY resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from ORDER BY resolved_atomic_segments_with_payloads.valid_from)
NOTICE:                                              Storage: Memory  Maximum Storage: 17kB
NOTICE:                                              Buffers: local hit=1
NOTICE:                                              ->  Incremental Sort (actual rows=3.00 loops=1)
NOTICE:                                                    Sort Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from, resolved_atomic_segments_with_payloads.valid_from
NOTICE:                                                    Presorted Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from
NOTICE:                                                    Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:                                                    Buffers: local hit=1
NOTICE:                                                    ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:                                                          Window: w1 AS (PARTITION BY resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from ORDER BY resolved_atomic_segments_with_payloads.valid_from)
NOTICE:                                                          Storage: Memory  Maximum Storage: 17kB
NOTICE:                                                          Buffers: local hit=1
NOTICE:                                                          ->  Sort (actual rows=3.00 loops=1)
NOTICE:                                                                Sort Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from, resolved_atomic_segments_with_payloads.valid_from DESC
NOTICE:                                                                Sort Method: quicksort  Memory: 25kB
NOTICE:                                                                Buffers: local hit=1
NOTICE:                                                                ->  Seq Scan on resolved_atomic_segments_with_payloads (actual rows=3.00 loops=1)
NOTICE:                                                                      Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=60
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments ON COMMIT DROP AS
                SELECT
                    grouping_key, lu_id, type,  stable_identity_columns_are_null, natural_identity_column_values_are_null, is_new_entity, is_identifiable, is_ambiguous, conflicting_ids, unified_canonical_nk_json,
                    valid_from, valid_until, t_valid_from, t_valid_until, propagated_s_valid_from, propagated_s_valid_until, propagated_contributing_row_ids, causal_id, propagated_stable_pk_payload as stable_pk_payload,
                    propagated_contributing_row_ids IS NULL AS unaffected_target_only_segment,
                    s_data_payload, t_data_payload,
                    sql_saga.get_allen_relation(propagated_s_valid_from, propagated_s_valid_until, t_valid_from, t_valid_until) AS s_t_relation,
                    COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb) as data_payload,
                    md5((jsonb_strip_nulls(COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb)))::text) as data_hash,
                    CASE 
                        WHEN s_data_payload IS NULL THEN t_ephemeral_payload
                        ELSE COALESCE(t_ephemeral_payload, '{}'::jsonb) || COALESCE(s_ephemeral_payload, '{}'::jsonb)
                    END as ephemeral_payload,
                    CASE WHEN 'f'::boolean
                        THEN trace || jsonb_build_object( 'cte', 'ras', 'propagated_stable_pk_payload', propagated_stable_pk_payload, 'final_data_payload', COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb), 'final_ephemeral_payload', CASE WHEN s_data_payload IS NULL THEN t_ephemeral_payload ELSE COALESCE(t_ephemeral_payload, '{}'::jsonb) || COALESCE(s_ephemeral_payload, '{}'::jsonb) END )
                        ELSE NULL
                    END as trace,
                    CASE WHEN s_data_payload IS NOT NULL THEN 1 ELSE 2 END as priority
                FROM resolved_atomic_segments_with_propagated_ids
            
NOTICE:  Seq Scan on resolved_atomic_segments_with_propagated_ids (actual rows=3.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=72
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE island_group ON COMMIT DROP AS
                SELECT
                    *,
                    SUM(is_island_start) OVER (PARTITION BY grouping_key ORDER BY valid_from) as island_group_id
                FROM (
                    SELECT
                        *,
                        CASE
                            WHEN prev_valid_until IS NULL
                            OR prev_valid_until <> valid_from
                            OR (prev_data_hash IS DISTINCT FROM data_hash)
                            THEN 1
                            ELSE 0
                        END as is_island_start
                    FROM (
                        SELECT
                            *,
                            LAG(valid_until) OVER w as prev_valid_until,
                            LAG(data_hash) OVER w as prev_data_hash,
                            LAG(data_payload) OVER w as prev_data_payload
                        FROM resolved_atomic_segments ras
                        WHERE ras.data_payload IS NOT NULL
                        WINDOW w AS (PARTITION BY grouping_key ORDER BY valid_from)
                    ) s1
                ) s2
            
NOTICE:  WindowAgg (actual rows=3.00 loops=1)
NOTICE:    Window: w1 AS (PARTITION BY s1.grouping_key ORDER BY s1.valid_from)
NOTICE:    Storage: Memory  Maximum Storage: 18kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Subquery Scan on s1 (actual rows=3.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:          ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:                Window: w AS (PARTITION BY ras.grouping_key ORDER BY ras.valid_from)
NOTICE:                Storage: Memory  Maximum Storage: 17kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Sort (actual rows=3.00 loops=1)
NOTICE:                      Sort Key: ras.grouping_key, ras.valid_from
NOTICE:                      Sort Method: quicksort  Memory: 25kB
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  Seq Scan on resolved_atomic_segments ras (actual rows=3.00 loops=1)
NOTICE:                            Filter: (data_payload IS NOT NULL)
NOTICE:                            Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=58
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE coalesced_final_segments ON COMMIT DROP AS
                SELECT
                    grouping_key, max(lu_id) as lu_id, max(type) as type,
                    sql_saga.first(causal_id ORDER BY valid_from) as causal_id,
                    sql_saga.first(stable_identity_columns_are_null ORDER BY valid_from) as stable_identity_columns_are_null,
                    sql_saga.first(natural_identity_column_values_are_null ORDER BY valid_from) as natural_identity_column_values_are_null,
                    sql_saga.first(is_new_entity ORDER BY valid_from) as is_new_entity,
                    sql_saga.first(is_identifiable ORDER BY valid_from) as is_identifiable,
                    sql_saga.first(is_ambiguous ORDER BY valid_from) as is_ambiguous,
                    sql_saga.first(conflicting_ids ORDER BY valid_from) as conflicting_ids,
                    sql_saga.first(unified_canonical_nk_json ORDER BY valid_from) as canonical_nk_json,
                    sql_saga.first(s_t_relation ORDER BY valid_from) as s_t_relation,
                    sql_saga.first(t_valid_from ORDER BY valid_from) as ancestor_valid_from,
                    MIN(valid_from) as valid_from,
                    MAX(valid_until) as valid_until,
                    (sql_saga.first(data_payload ORDER BY valid_from DESC) || sql_saga.first(ephemeral_payload ORDER BY valid_from DESC)) as data_payload,
                    sql_saga.first(stable_pk_payload ORDER BY valid_from DESC) as stable_pk_payload,
                    bool_and(unaffected_target_only_segment) as unaffected_target_only_segment,
                    (SELECT array_agg(DISTINCT e) FROM unnest(array_concat_agg(propagated_contributing_row_ids)) e WHERE e IS NOT NULL) as row_ids,
                    CASE WHEN 'f'::boolean
                        THEN jsonb_build_object( 'cte', 'coalesced', 'island_group_id', island_group_id, 'coalesced_stable_pk_payload', sql_saga.first(stable_pk_payload ORDER BY valid_from DESC), 'final_payload', sql_saga.first(data_payload ORDER BY valid_from DESC), 'final_payload_sans_ephemeral', sql_saga.first(data_payload - '{}'::text[] ORDER BY valid_from DESC), 'atomic_traces', jsonb_agg((trace || jsonb_build_object('data_hash', data_hash, 'prev_data_hash', prev_data_hash, 'prev_data_payload', prev_data_payload)) ORDER BY valid_from) )
                        ELSE NULL
                    END as trace
                FROM island_group
                GROUP BY grouping_key, island_group_id
            
NOTICE:  GroupAggregate (actual rows=3.00 loops=1)
NOTICE:    Group Key: island_group.grouping_key, island_group.island_group_id
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=3.00 loops=1)
NOTICE:          Sort Key: island_group.grouping_key, island_group.island_group_id, island_group.valid_from
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on island_group (actual rows=3.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:    SubPlan 1
NOTICE:      ->  Aggregate (actual rows=1.00 loops=3)
NOTICE:            ->  Sort (actual rows=1.00 loops=3)
NOTICE:                  Sort Key: e.e
NOTICE:                  Sort Method: quicksort  Memory: 25kB
NOTICE:                  ->  Function Scan on unnest e (actual rows=1.00 loops=3)
NOTICE:                        Filter: (e IS NOT NULL)
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=70
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE diff ON COMMIT DROP AS
                SELECT
                    final_seg.grouping_key, COALESCE(final_seg.lu_id, target_seg.lu_id) as lu_id, COALESCE(final_seg.type, target_seg.type) as type, COALESCE(final_seg.causal_id, target_seg.causal_id) as causal_id,
                    COALESCE(final_seg.is_new_entity, false) as is_new_entity,
                    COALESCE(final_seg.is_identifiable, true) as is_identifiable,
                    COALESCE(final_seg.is_ambiguous, false) as is_ambiguous,
                    final_seg.conflicting_ids,
                    final_seg.canonical_nk_json,
                    COALESCE(final_seg.stable_identity_columns_are_null, false) as stable_identity_columns_are_null,
                    COALESCE(final_seg.natural_identity_column_values_are_null, false) as natural_identity_column_values_are_null,
                    final_seg.valid_from AS f_from, final_seg.valid_until AS f_until, final_seg.data_payload AS f_data, final_seg.row_ids AS f_row_ids, final_seg.stable_pk_payload, final_seg.s_t_relation,
                    CASE WHEN 'f'::boolean
                        THEN final_seg.trace || jsonb_build_object('cte', 'diff', 'diff_stable_pk_payload', final_seg.stable_pk_payload, 'final_seg_causal_id', final_seg.causal_id, 'final_payload_vs_target_payload', jsonb_build_object('f', final_seg.data_payload, 't', target_seg.data_payload))
                        ELSE NULL
                    END as trace,
                    final_seg.unaffected_target_only_segment,
                    target_seg.valid_from as t_from, target_seg.valid_until as t_until, (target_seg.data_payload || target_seg.ephemeral_payload) as t_data,
                    sql_saga.get_allen_relation(target_seg.valid_from, target_seg.valid_until, final_seg.valid_from, final_seg.valid_until) as b_a_relation
                FROM coalesced_final_segments AS final_seg
                FULL OUTER JOIN target_rows AS target_seg ON final_seg.grouping_key = ('existing_entity__' || COALESCE(target_seg.type::text, '_NULL_') || '__' || COALESCE(target_seg.lu_id::text, '_NULL_')) AND final_seg.ancestor_valid_from = target_seg.valid_from
            
NOTICE:  Hash Full Join (actual rows=3.00 loops=1)
NOTICE:    Hash Cond: ((final_seg.grouping_key = ((('existing_entity__'::text || COALESCE(target_seg.type, '_NULL_'::text)) || '__'::text) || COALESCE((target_seg.lu_id)::text, '_NULL_'::text))) AND (final_seg.ancestor_valid_from = target_seg.valid_from))
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on coalesced_final_segments final_seg (actual rows=3.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Hash (actual rows=1.00 loops=1)
NOTICE:          Buckets: 1024  Batches: 1  Memory Usage: 9kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on target_rows target_seg (actual rows=1.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=44
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE diff_ranked ON COMMIT DROP AS
                SELECT
                    d.*,
                    CASE
                        WHEN d.t_from IS NULL OR d.f_from IS NULL THEN NULL
                        WHEN d.f_from = d.t_from AND d.f_until = d.t_until AND d.f_data IS NOT DISTINCT FROM d.t_data THEN NULL
                        ELSE
                            row_number() OVER (
                                PARTITION BY d.grouping_key, d.t_from
                                ORDER BY
                                    CASE WHEN d.f_from = d.t_from THEN 1 ELSE 2 END,
                                    CASE WHEN d.f_data - '{}'::text[] IS NOT DISTINCT FROM d.t_data - '{}'::text[] THEN 1 ELSE 2 END,
                                    d.f_from,
                                    d.f_until
                            )
                    END as update_rank
                FROM diff d
            
NOTICE:  WindowAgg (actual rows=3.00 loops=1)
NOTICE:    Window: w1 AS (PARTITION BY grouping_key, t_from ORDER BY (CASE WHEN (f_from = t_from) THEN 1 ELSE 2 END), (CASE WHEN (NOT ((f_data - '{}'::text[]) IS DISTINCT FROM (t_data - '{}'::text[]))) THEN 1 ELSE 2 END), f_from, f_until ROWS UNBOUNDED PRECEDING)
NOTICE:    Storage: Memory  Maximum Storage: 17kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=3.00 loops=1)
NOTICE:          Sort Key: grouping_key, t_from, (CASE WHEN (f_from = t_from) THEN 1 ELSE 2 END), (CASE WHEN (NOT ((f_data - '{}'::text[]) IS DISTINCT FROM (t_data - '{}'::text[]))) THEN 1 ELSE 2 END), f_from, f_until
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on diff d (actual rows=3.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=48
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE plan_with_op ON COMMIT DROP AS
                (
                    SELECT * FROM (
                        SELECT
                            d.f_row_ids as row_ids, d.s_t_relation, d.is_new_entity,
                            CASE
                                WHEN d.is_ambiguous THEN 'ERROR'::sql_saga.temporal_merge_plan_action
                                WHEN d.is_new_entity AND NOT d.is_identifiable THEN 'ERROR'::sql_saga.temporal_merge_plan_action
                                WHEN d.t_from IS NULL THEN 'INSERT'::sql_saga.temporal_merge_plan_action
                                WHEN d.f_from IS NULL THEN 'DELETE'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank = 1 THEN 'UPDATE'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank > 1 THEN 'INSERT'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank IS NULL THEN
                                    CASE
                                        WHEN d.unaffected_target_only_segment THEN NULL
                                        ELSE 'SKIP_IDENTICAL'::sql_saga.temporal_merge_plan_action
                                    END
                                ELSE 'ERROR'::sql_saga.temporal_merge_plan_action
                            END as operation,
                            lu_id, type,
                            CASE
                                WHEN d.is_new_entity AND d.canonical_nk_json IS NOT NULL
                                THEN d.canonical_nk_json || COALESCE(d.stable_pk_payload, '{}'::jsonb)
                                ELSE jsonb_build_object('lu_id', d.lu_id, 'type', d.type) || COALESCE(d.stable_pk_payload, '{}'::jsonb)
                            END as entity_keys_json,
                            jsonb_build_object('type', d.type, 'lu_id', d.lu_id) as identity_keys,
                            jsonb_build_object() as lookup_keys,
                            d.causal_id, d.t_from as old_valid_from, d.t_until as old_valid_until, d.f_from as new_valid_from, d.f_until as new_valid_until,
                            CASE
                                WHEN d.is_ambiguous THEN NULL
                                WHEN d.is_new_entity AND NOT d.is_identifiable THEN NULL
                                ELSE d.f_data
                            END as data,
                            CASE
                                WHEN d.is_ambiguous
                                THEN jsonb_build_object('error', format('Source row is ambiguous. It matches multiple distinct target entities: %s', d.conflicting_ids))
                                WHEN d.is_new_entity AND NOT d.is_identifiable
                                THEN jsonb_build_object('error', 'Source row is unidentifiable. It has NULL for all stable identity columns ' || replace('{type,lu_id}'::text, '"', '') || ' and all natural keys ' || replace(NULL::text, '"', ''))
                                ELSE NULL
                            END as feedback,
                            d.b_a_relation, d.grouping_key,
                            CASE WHEN 'f'::boolean
                                THEN d.trace || jsonb_build_object( 'cte', 'plan_with_op', 'diff_is_new_entity', d.is_new_entity, 'diff_causal_id', d.causal_id, 'entity_keys_from_key_cols', jsonb_build_object('lu_id', d.lu_id, 'type', d.type), 'entity_keys_from_stable_pk', d.stable_pk_payload, 'final_entity_id_json', jsonb_build_object('lu_id', d.lu_id, 'type', d.type) || COALESCE(d.stable_pk_payload, '{}'::jsonb) )
                                ELSE NULL
                            END as trace
                        FROM diff_ranked d
                        WHERE d.f_row_ids IS NOT NULL OR d.t_data IS NOT NULL
                    ) with_op
                    WHERE with_op.operation IS NOT NULL
                )
                UNION ALL
                (
                    SELECT
                        ARRAY[source_row.source_row_id::BIGINT],
                        NULL::sql_saga.allen_interval_relation, source_row.is_new_entity,
                        COALESCE(
                            (source_row.early_feedback->>'operation')::sql_saga.temporal_merge_plan_action,
                            CASE
                                WHEN 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode = 'INSERT_NEW_ENTITIES' AND source_row.target_entity_exists THEN 'SKIP_FILTERED'::sql_saga.temporal_merge_plan_action
                                WHEN 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode IN ('PATCH_FOR_PORTION_OF', 'REPLACE_FOR_PORTION_OF', 'DELETE_FOR_PORTION_OF', 'UPDATE_FOR_PORTION_OF') AND NOT source_row.target_entity_exists THEN 'SKIP_NO_TARGET'::sql_saga.temporal_merge_plan_action
                                ELSE 'ERROR'::sql_saga.temporal_merge_plan_action
                            END
                        ),
                        lu_id, type,
                        CASE
                            WHEN source_row.is_new_entity AND source_row.canonical_nk_json IS NOT NULL
                            THEN source_row.canonical_nk_json || COALESCE(source_row.stable_pk_payload, '{}'::jsonb)
                            ELSE jsonb_build_object('lu_id', source_row.lu_id, 'type', source_row.type)
                        END as entity_keys_json,
                        jsonb_build_object('type', source_row.type, 'lu_id', source_row.lu_id) as identity_keys, jsonb_build_object() as lookup_keys,
                        source_row.causal_id,
                        NULL, NULL, NULL, NULL, NULL,
                        CASE
                            WHEN source_row.early_feedback IS NOT NULL THEN jsonb_build_object('error', source_row.early_feedback->>'message')
                            ELSE jsonb_build_object('info', 'Source row was correctly filtered by the mode''s logic and did not result in a DML operation.')
                        END,
                        NULL,
                        CASE
                    WHEN source_row.is_new_entity
                    THEN 'new_entity__' || source_row.causal_id::text
                    ELSE 'existing_entity__' || COALESCE(source_row.type::text, '_NULL_') || '__' || COALESCE(source_row.lu_id::text, '_NULL_')
                END AS grouping_key,
                        NULL::jsonb
                    FROM source_rows_with_early_feedback source_row
                    WHERE
                        source_row.early_feedback IS NOT NULL
                        OR NOT (
                            CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                            WHEN 'MERGE_ENTITY_PATCH' THEN true
                            WHEN 'MERGE_ENTITY_REPLACE' THEN true
                            WHEN 'MERGE_ENTITY_UPSERT' THEN true
                            WHEN 'INSERT_NEW_ENTITIES' THEN NOT source_row.target_entity_exists
                            WHEN 'PATCH_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'REPLACE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'DELETE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'UPDATE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            ELSE false
                        END
                    )
                )
            
NOTICE:  Append (actual rows=3.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on diff_ranked d (actual rows=3.00 loops=1)
NOTICE:          Filter: (((f_row_ids IS NOT NULL) OR (t_data IS NOT NULL)) AND (CASE WHEN is_ambiguous THEN 'ERROR'::sql_saga.temporal_merge_plan_action WHEN (is_new_entity AND (NOT is_identifiable)) THEN 'ERROR'::sql_saga.temporal_merge_plan_action WHEN (t_from IS NULL) THEN 'INSERT'::sql_saga.temporal_merge_plan_action WHEN (f_from IS NULL) THEN 'DELETE'::sql_saga.temporal_merge_plan_action WHEN (update_rank = 1) THEN 'UPDATE'::sql_saga.temporal_merge_plan_action WHEN (update_rank > 1) THEN 'INSERT'::sql_saga.temporal_merge_plan_action WHEN (update_rank IS NULL) THEN CASE WHEN unaffected_target_only_segment THEN NULL::sql_saga.temporal_merge_plan_action ELSE 'SKIP_IDENTICAL'::sql_saga.temporal_merge_plan_action END ELSE 'ERROR'::sql_saga.temporal_merge_plan_action END IS NOT NULL))
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Seq Scan on source_rows_with_early_feedback source_row (actual rows=0.00 loops=1)
NOTICE:          Filter: (early_feedback IS NOT NULL)
NOTICE:          Rows Removed by Filter: 1
NOTICE:          Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=52
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE plan ON COMMIT DROP AS
                SELECT
                    p.row_ids, p.operation, p.causal_id, p.is_new_entity,
                    p.lu_id, p.type,
                    p.entity_keys_json as entity_keys,
                    p.identity_keys, p.lookup_keys,
                    p.s_t_relation, p.b_a_relation, p.old_valid_from, p.old_valid_until,
                    p.new_valid_from, p.new_valid_until, p.data, p.feedback, p.trace,
                    p.grouping_key,
                    CASE
                        WHEN p.operation <> 'UPDATE' THEN NULL::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from = p.old_valid_from AND p.new_valid_until = p.old_valid_until THEN 'NONE'::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from <= p.old_valid_from AND p.new_valid_until >= p.old_valid_until THEN 'GROW'::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from >= p.old_valid_from AND p.new_valid_until <= p.old_valid_until THEN 'SHRINK'::sql_saga.temporal_merge_update_effect
                        ELSE 'MOVE'::sql_saga.temporal_merge_update_effect
                    END AS update_effect
                FROM plan_with_op p
                LEFT JOIN source_rows_with_new_flag source_row ON source_row.source_row_id = p.row_ids[1]
            
NOTICE:  Hash Right Join (actual rows=3.00 loops=1)
NOTICE:    Hash Cond: (source_row.source_row_id = p.row_ids[1])
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_rows_with_new_flag source_row (actual rows=1.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Hash (actual rows=3.00 loops=1)
NOTICE:          Buckets: 1024  Batches: 1  Memory Usage: 9kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on plan_with_op p (actual rows=3.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=44
NOTICE:  ---
NOTICE:  Explaining: 
                WITH RECURSIVE ordered_plan AS (
                    SELECT
                        row_number() OVER ( ORDER BY p.grouping_key, p.lu_id, p.type, CASE p.operation WHEN 'DELETE' THEN 1 WHEN 'UPDATE' THEN 2 WHEN 'INSERT' THEN 3 ELSE 4 END, p.update_effect NULLS FIRST, CASE WHEN p.update_effect = 'MOVE' THEN 1 ELSE 0 END, CASE WHEN p.update_effect = 'MOVE' THEN COALESCE(p.old_valid_from, p.new_valid_from) END DESC NULLS LAST, COALESCE(p.old_valid_from, p.new_valid_from), (p.row_ids[1]) )::BIGINT as plan_op_seq,
                        p.*
                    FROM plan p
                ),
                -- For MOVE operations, group into the largest possible batches that can execute together.
                -- Within a single UPDATE statement, all rows see the pre-update state (MVCC snapshot).
                -- So if MOVE A's new_range overlaps MOVE B's old_range, they cannot be in the same statement.
                --
                -- Algorithm: Process MOVEs in plan_op_seq order (which is old_valid_from DESC).
                -- Track a multirange of old_ranges in the current batch. For each MOVE:
                -- - If new_range overlaps the batch's multirange, start a new batch
                -- - Add this MOVE's old_range to the batch's multirange
                --
                -- We use a recursive CTE to accumulate the multirange with reset-on-conflict logic.
                moves_ordered AS (
                    SELECT
                        o.plan_op_seq,
                        o.grouping_key,
                        o.old_valid_from,
                        o.old_valid_until,
                        o.new_valid_from,
                        o.new_valid_until,
                        row_number() OVER (PARTITION BY o.grouping_key ORDER BY o.plan_op_seq) AS move_seq
                    FROM ordered_plan o
                    WHERE o.operation = 'UPDATE' AND o.update_effect = 'MOVE'
                ),
                moves_with_batch AS (
                    -- Base case: first MOVE in each entity starts batch 0
                    SELECT 
                        plan_op_seq, grouping_key, old_valid_from, old_valid_until, 
                        new_valid_from, new_valid_until, move_seq,
                        0::bigint AS move_batch,
                        datemultirange(daterange(old_valid_from, old_valid_until, '[)')) AS batch_old_ranges
                    FROM moves_ordered
                    WHERE move_seq = 1
                    
                    UNION ALL
                    
                    -- Recursive case: check if new MOVE conflicts with batch's multirange
                    SELECT
                        m.plan_op_seq, m.grouping_key, m.old_valid_from, m.old_valid_until,
                        m.new_valid_from, m.new_valid_until, m.move_seq,
                        CASE 
                            WHEN daterange(m.new_valid_from, m.new_valid_until, '[)') && prev.batch_old_ranges
                            THEN prev.move_batch + 1  -- Conflict: start new batch
                            ELSE prev.move_batch      -- No conflict: same batch
                        END AS move_batch,
                        CASE 
                            WHEN daterange(m.new_valid_from, m.new_valid_until, '[)') && prev.batch_old_ranges
                            THEN datemultirange(daterange(m.old_valid_from, m.old_valid_until, '[)'))  -- Reset multirange
                            ELSE prev.batch_old_ranges + datemultirange(daterange(m.old_valid_from, m.old_valid_until, '[)'))  -- Add to multirange
                        END AS batch_old_ranges
                    FROM moves_ordered m
                    JOIN moves_with_batch prev 
                      ON prev.grouping_key = m.grouping_key 
                     AND prev.move_seq = m.move_seq - 1
                ),
                with_statement_seq AS (
                    SELECT
                        o.plan_op_seq,
                        CASE
                            WHEN o.operation = 'DELETE' THEN 1
                            WHEN o.operation = 'UPDATE' AND o.update_effect IN ('NONE', 'SHRINK') THEN 2
                            WHEN o.operation = 'UPDATE' AND o.update_effect = 'MOVE' THEN
                                -- Base of 3 for first MOVE batch, +1 for each subsequent batch
                                3 + COALESCE(mb.move_batch, 0)
                            WHEN o.operation = 'UPDATE' AND o.update_effect = 'GROW' THEN 1000000  -- Will be renumbered
                            WHEN o.operation = 'INSERT' THEN 2000000  -- Will be renumbered
                            ELSE 3000000
                        END::INT as raw_statement_seq,
                        o.row_ids, o.operation, o.update_effect, o.causal_id::TEXT, o.is_new_entity, o.entity_keys, o.identity_keys, o.lookup_keys, o.s_t_relation, o.b_a_relation, o.old_valid_from::TEXT,
                        o.old_valid_until::TEXT, o.new_valid_from::TEXT, o.new_valid_until::TEXT, o.data, o.feedback, CASE WHEN o.trace IS NOT NULL THEN o.trace || jsonb_build_object('final_grouping_key', o.grouping_key) ELSE NULL END as trace, o.grouping_key
                    FROM ordered_plan o
                    LEFT JOIN moves_with_batch mb ON mb.plan_op_seq = o.plan_op_seq
                )
                SELECT
                    plan_op_seq,
                    dense_rank() OVER (ORDER BY raw_statement_seq)::INT as statement_seq,
                    row_ids, operation, update_effect, causal_id, is_new_entity, entity_keys, identity_keys, lookup_keys, s_t_relation, b_a_relation, old_valid_from,
                    old_valid_until, new_valid_from, new_valid_until,
                    -- New range columns
                    CASE 
                        WHEN old_valid_from IS NOT NULL AND old_valid_until IS NOT NULL 
                        THEN daterange(old_valid_from::date, old_valid_until::date, '[)')::TEXT
                        ELSE NULL 
                    END as old_valid_range,
                    CASE 
                        WHEN new_valid_from IS NOT NULL AND new_valid_until IS NOT NULL 
                        THEN daterange(new_valid_from::date, new_valid_until::date, '[)')::TEXT
                        ELSE NULL 
                    END as new_valid_range,
                    data, feedback, trace, grouping_key
                FROM with_statement_seq
                ORDER BY plan_op_seq;
            
NOTICE:  Sort (actual rows=3.00 loops=1)
NOTICE:    Sort Key: o.plan_op_seq
NOTICE:    Sort Method: quicksort  Memory: 25kB
NOTICE:    Buffers: local hit=1
NOTICE:    CTE ordered_plan
NOTICE:      ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:            Window: w1 AS (ORDER BY p.grouping_key, p.lu_id, p.type, (CASE p.operation WHEN 'DELETE'::sql_saga.temporal_merge_plan_action THEN 1 WHEN 'UPDATE'::sql_saga.temporal_merge_plan_action THEN 2 WHEN 'INSERT'::sql_saga.temporal_merge_plan_action THEN 3 ELSE 4 END), p.update_effect, (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN 1 ELSE 0 END), (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN COALESCE(p.old_valid_from, p.new_valid_from) ELSE NULL::date END), (COALESCE(p.old_valid_from, p.new_valid_from)), (p.row_ids[1]) ROWS UNBOUNDED PRECEDING)
NOTICE:            Storage: Memory  Maximum Storage: 17kB
NOTICE:            Buffers: local hit=1
NOTICE:            ->  Sort (actual rows=3.00 loops=1)
NOTICE:                  Sort Key: p.grouping_key, p.lu_id, p.type, (CASE p.operation WHEN 'DELETE'::sql_saga.temporal_merge_plan_action THEN 1 WHEN 'UPDATE'::sql_saga.temporal_merge_plan_action THEN 2 WHEN 'INSERT'::sql_saga.temporal_merge_plan_action THEN 3 ELSE 4 END), p.update_effect NULLS FIRST, (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN 1 ELSE 0 END), (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN COALESCE(p.old_valid_from, p.new_valid_from) ELSE NULL::date END) DESC NULLS LAST, (COALESCE(p.old_valid_from, p.new_valid_from)), (p.row_ids[1])
NOTICE:                  Sort Method: quicksort  Memory: 25kB
NOTICE:                  Buffers: local hit=1
NOTICE:                  ->  Seq Scan on plan p (actual rows=3.00 loops=1)
NOTICE:                        Buffers: local hit=1
NOTICE:    CTE moves_ordered
NOTICE:      ->  WindowAgg (actual rows=0.00 loops=1)
NOTICE:            Window: w1 AS (PARTITION BY o_1.grouping_key ORDER BY o_1.plan_op_seq ROWS UNBOUNDED PRECEDING)
NOTICE:            ->  Incremental Sort (actual rows=0.00 loops=1)
NOTICE:                  Sort Key: o_1.grouping_key, o_1.plan_op_seq
NOTICE:                  Presorted Key: o_1.grouping_key
NOTICE:                  Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:                  ->  CTE Scan on ordered_plan o_1 (actual rows=0.00 loops=1)
NOTICE:                        Filter: ((operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect))
NOTICE:                        Rows Removed by Filter: 3
NOTICE:                        Storage: Memory  Maximum Storage: 17kB
NOTICE:    CTE moves_with_batch
NOTICE:      ->  Recursive Union (actual rows=0.00 loops=1)
NOTICE:            Storage: Memory  Maximum Storage: 33kB
NOTICE:            ->  CTE Scan on moves_ordered (actual rows=0.00 loops=1)
NOTICE:                  Filter: (move_seq = 1)
NOTICE:                  Storage: Memory  Maximum Storage: 17kB
NOTICE:            ->  Hash Join (actual rows=0.00 loops=1)
NOTICE:                  Hash Cond: ((prev.grouping_key = m.grouping_key) AND (prev.move_seq = (m.move_seq - 1)))
NOTICE:                  ->  WorkTable Scan on moves_with_batch prev (actual rows=0.00 loops=1)
NOTICE:                  ->  Hash (never executed)
NOTICE:                        ->  CTE Scan on moves_ordered m (never executed)
NOTICE:                              Storage: Memory  Maximum Storage: 17kB
NOTICE:    ->  WindowAgg (actual rows=3.00 loops=1)
NOTICE:          Window: w1 AS (ORDER BY ((CASE WHEN (o.operation = 'DELETE'::sql_saga.temporal_merge_plan_action) THEN '1'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = ANY ('{NONE,SHRINK}'::sql_saga.temporal_merge_update_effect[]))) THEN '2'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect)) THEN (3 + COALESCE(mb.move_batch, '0'::bigint)) WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'GROW'::sql_saga.temporal_merge_update_effect)) THEN '1000000'::bigint WHEN (o.operation = 'INSERT'::sql_saga.temporal_merge_plan_action) THEN '2000000'::bigint ELSE '3000000'::bigint END)::integer) ROWS UNBOUNDED PRECEDING)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Sort (actual rows=3.00 loops=1)
NOTICE:                Sort Key: ((CASE WHEN (o.operation = 'DELETE'::sql_saga.temporal_merge_plan_action) THEN '1'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = ANY ('{NONE,SHRINK}'::sql_saga.temporal_merge_update_effect[]))) THEN '2'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect)) THEN (3 + COALESCE(mb.move_batch, '0'::bigint)) WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'GROW'::sql_saga.temporal_merge_update_effect)) THEN '1000000'::bigint WHEN (o.operation = 'INSERT'::sql_saga.temporal_merge_plan_action) THEN '2000000'::bigint ELSE '3000000'::bigint END)::integer)
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Hash Left Join (actual rows=3.00 loops=1)
NOTICE:                      Hash Cond: (o.plan_op_seq = mb.plan_op_seq)
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  CTE Scan on ordered_plan o (actual rows=3.00 loops=1)
NOTICE:                            Storage: Memory  Maximum Storage: 17kB
NOTICE:                            Buffers: local hit=1
NOTICE:                      ->  Hash (actual rows=0.00 loops=1)
NOTICE:                            Buckets: 1024  Batches: 1  Memory Usage: 8kB
NOTICE:                            ->  CTE Scan on moves_with_batch mb (actual rows=0.00 loops=1)
NOTICE:                                  Storage: Memory  Maximum Storage: 17kB
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=42
\echo '--- OK: Verified that EXPLAIN plan for non-null natural key is optimal. ---'
--- OK: Verified that EXPLAIN plan for non-null natural key is optimal. ---
ROLLBACK TO SAVEPOINT s2;
--------------------------------------------------------------------------------
-- SCENARIO 3: Composite Natural Key with NULLable XOR columns
--------------------------------------------------------------------------------
SAVEPOINT s3;
\echo '\n--- Scenario 3: Composite Natural Key with NULLable XOR columns ---'

--- Scenario 3: Composite Natural Key with NULLable XOR columns ---
CREATE TABLE tmpc.target_nk (type TEXT NOT NULL, lu_id INT, es_id INT, value TEXT, valid_range daterange, valid_from date, valid_until date,
    CONSTRAINT lu_or_es_id_check CHECK ((lu_id IS NOT NULL AND es_id IS NULL) OR (lu_id IS NULL AND es_id IS NOT NULL))
);
SELECT sql_saga.add_era('tmpc.target_nk', 'valid_range',
    valid_from_column_name => 'valid_from',
    valid_until_column_name => 'valid_until');
NOTICE:  sql_saga: Created trigger "target_nk_synchronize_temporal_columns_trigger" on table tmpc.target_nk to synchronize columns: valid_from, valid_until
 add_era 
---------
 t
(1 row)

CREATE TABLE tmpc.source_nk (row_id int, type text NOT NULL, lu_id int, es_id int, value text, valid_from date, valid_until date,
    CONSTRAINT source_lu_or_es_id_check CHECK ((lu_id IS NOT NULL AND es_id IS NULL) OR (lu_id IS NULL AND es_id IS NOT NULL))
);
\echo '--- Setting up natural key tables with partial indexes and data ---'
--- Setting up natural key tables with partial indexes and data ---
SELECT sql_saga.add_unique_key(table_oid => 'tmpc.target_nk'::regclass, column_names => ARRAY['type', 'lu_id'], key_type => 'predicated', predicate => 'es_id IS NULL');
NOTICE:  sql_saga: altering table tmpc.target_nk to add constraints: ADD CONSTRAINT target_nk_type_lu_id_valid_excl EXCLUDE USING gist (type WITH =, lu_id WITH =, valid_range WITH &&) WHERE (es_id IS NULL) DEFERRABLE
       add_unique_key       
----------------------------
 target_nk_type_lu_id_valid
(1 row)

SELECT sql_saga.add_unique_key(table_oid => 'tmpc.target_nk'::regclass, column_names => ARRAY['type', 'es_id'], key_type => 'predicated', predicate => 'lu_id IS NULL');
NOTICE:  sql_saga: altering table tmpc.target_nk to add constraints: ADD CONSTRAINT target_nk_type_es_id_valid_excl EXCLUDE USING gist (type WITH =, es_id WITH =, valid_range WITH &&) WHERE (lu_id IS NULL) DEFERRABLE
       add_unique_key       
----------------------------
 target_nk_type_es_id_valid
(1 row)

INSERT INTO tmpc.target_nk (type, lu_id, es_id, valid_from, valid_until, value) SELECT 'A', i, NULL, '2023-01-01', '2024-01-01', 'LU' FROM generate_series(1, 1000) as i;
INSERT INTO tmpc.target_nk (type, lu_id, es_id, valid_from, valid_until, value) SELECT 'B', NULL, i, '2023-01-01', '2024-01-01', 'ES' FROM generate_series(1, 1000) as i;
INSERT INTO tmpc.source_nk VALUES (1, 'A', 500, NULL, 'LU-patched', '2023-06-01', '2023-07-01');
INSERT INTO tmpc.source_nk VALUES (2, 'B', NULL, 500, 'ES-patched', '2023-06-01', '2023-07-01');
ANALYZE tmpc.target_nk;
ANALYZE tmpc.source_nk;
\echo '\d tmpc.target_nk'
d tmpc.target_nk
\d tmpc.target_nk
                  Table "tmpc.target_nk"
   Column    |   Type    | Collation | Nullable | Default 
-------------+-----------+-----------+----------+---------
 type        | text      |           | not null | 
 lu_id       | integer   |           |          | 
 es_id       | integer   |           |          | 
 value       | text      |           |          | 
 valid_range | daterange |           | not null | 
 valid_from  | date      |           | not null | 
 valid_until | date      |           |          | 
Indexes:
    "target_nk_type_es_id_idx" btree (type, es_id)
    "target_nk_type_es_id_valid_excl" EXCLUDE USING gist (type WITH =, es_id WITH =, valid_range WITH &&) WHERE (lu_id IS NULL) DEFERRABLE
    "target_nk_type_es_id_valid_range_idx" btree (type, es_id, valid_range)
    "target_nk_type_lu_id_idx" btree (type, lu_id)
    "target_nk_type_lu_id_valid_excl" EXCLUDE USING gist (type WITH =, lu_id WITH =, valid_range WITH &&) WHERE (es_id IS NULL) DEFERRABLE
    "target_nk_type_lu_id_valid_range_idx" btree (type, lu_id, valid_range)
Check constraints:
    "lu_or_es_id_check" CHECK (lu_id IS NOT NULL AND es_id IS NULL OR lu_id IS NULL AND es_id IS NOT NULL)
    "target_nk_check" CHECK (valid_from < valid_until AND valid_from > '-infinity'::date)
    "target_nk_valid_check" CHECK (NOT isempty(valid_range))
Triggers:
    target_nk_valid_sync_temporal_trg BEFORE INSERT OR UPDATE OF valid_range, valid_from, valid_until ON tmpc.target_nk FOR EACH ROW EXECUTE FUNCTION sql_saga.tmpc_target_nk_valid_template_sync()

\echo '\d tmpc.source_nk'
d tmpc.source_nk
\d tmpc.source_nk
                 Table "tmpc.source_nk"
   Column    |  Type   | Collation | Nullable | Default 
-------------+---------+-----------+----------+---------
 row_id      | integer |           |          | 
 type        | text    |           | not null | 
 lu_id       | integer |           |          | 
 es_id       | integer |           |          | 
 value       | text    |           |          | 
 valid_from  | date    |           |          | 
 valid_until | date    |           |          | 
Check constraints:
    "source_lu_or_es_id_check" CHECK (lu_id IS NOT NULL AND es_id IS NULL OR lu_id IS NULL AND es_id IS NOT NULL)

\echo '\n--- Performance Monitoring: EXPLAIN the cached planner query (Natural Key, NULLable) ---'

--- Performance Monitoring: EXPLAIN the cached planner query (Natural Key, NULLable) ---
DO $$
DECLARE
    plan_sqls JSONB;
    sql_step JSONB;
    sql_stmt TEXT;
    rec RECORD;
    has_seq_scan BOOLEAN;
BEGIN
    -- Run once to populate cache. Output is discarded.
    PERFORM * FROM sql_saga.temporal_merge_plan(
        target_table => 'tmpc.target_nk'::regclass,
        source_table => 'tmpc.source_nk'::regclass,
        identity_columns => '{type,lu_id,es_id}'::text[],
        mode => 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode,
        era_name => 'valid'
    );

    -- Find the plan SQLs for the source table of this scenario.
    SELECT c.plan_sqls INTO plan_sqls
    FROM pg_temp.temporal_merge_plan_cache c
    WHERE c.cache_key LIKE '%' || 'tmpc.source_nk'::regclass::text;

    -- Clean up temp tables created by the first run so we can re-execute them.
    CALL sql_saga.temporal_merge_drop_temp_tables();

    RAISE NOTICE '--- Explaining % steps from cache', jsonb_array_length(plan_sqls);
    FOR sql_step IN SELECT * FROM jsonb_array_elements(plan_sqls)
    LOOP
        RAISE NOTICE '---';
        sql_stmt := sql_step->>'sql';

        IF sql_step->>'type' = 'setup' THEN
            RAISE NOTICE 'Executing setup: %', sql_stmt;
            EXECUTE sql_stmt;
        ELSE
            RAISE NOTICE 'Explaining: %', sql_stmt;
            has_seq_scan := false;
            FOR rec IN EXECUTE 'EXPLAIN (ANALYZE, COSTS OFF, TIMING OFF, SUMMARY OFF, BUFFERS) ' || sql_stmt
            LOOP
                RAISE NOTICE '%', rec."QUERY PLAN";
                -- We add a space to not match temp tables or other target tables.
                IF rec."QUERY PLAN" LIKE '%Seq Scan on target_nk %' THEN
                    has_seq_scan := true;
                END IF;
            END LOOP;

            IF has_seq_scan THEN
                RAISE EXCEPTION 'Performance regression detected: EXPLAIN plan for nullable natural key contains a "Seq Scan on target_nk".';
            END IF;
        END IF;
    END LOOP;
END;
$$;
NOTICE:  --- Explaining 43 steps from cache
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_initial ON COMMIT DROP AS
                WITH source_data AS (
                    SELECT
                        source_table.row_id /* row_id_column */ as source_row_id,
                        source_table.row_id /* v_causal_select_expr */ as causal_id,
                        source_table.es_id, source_table.lu_id, source_table.type,  -- v_non_temporal_lookup_cols_select_list_prefix
                        source_table.valid_from as valid_from, source_table.valid_until as valid_until, /* v_source_temporal_cols_expr */
                        jsonb_strip_nulls(jsonb_build_object('value', source_table.value)) /* v_source_data_payload_expr */ AS data_payload,
                        jsonb_strip_nulls(jsonb_build_object()) /* v_source_ephemeral_payload_expr */ AS ephemeral_payload,
                        jsonb_build_object('type', source_table.type, 'lu_id', source_table.lu_id, 'es_id', source_table.es_id) /* v_stable_pk_cols_jsonb_build_source */ as stable_pk_payload,
                        source_table.type IS NULL AND source_table.lu_id IS NULL AND source_table.es_id IS NULL /* v_entity_id_check_is_null_expr */ as stable_identity_columns_are_null,
                        (true) /* v_lookup_cols_are_null_expr */ as natural_identity_column_values_are_null,
                        true /* v_is_identifiable_expr */ as is_identifiable,
                        true /* v_consistency_check_expr */ as temporal_columns_are_consistent
                    FROM tmpc.source_nk /* v_source_table_ident */ source_table
                )
                SELECT 
                    *,
                    -- OPTIMIZATION: Pre-compute range column to avoid repeated daterange() calls
                    -- This provides ~10-15% performance improvement in eclipse detection
                    daterange(valid_from, valid_until) as valid_range
                FROM source_data;
            
NOTICE:  Seq Scan on source_nk source_table (actual rows=2.00 loops=1)
NOTICE:    Buffers: shared hit=1
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_initial (source_row_id);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_initial (type, lu_id, es_id);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_initial (type, lu_id, es_id, source_row_id);
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE source_initial;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_with_eclipsed_flag ON COMMIT DROP AS
                SELECT
                    s1.*,
                    eclipse_info.is_eclipsed,
                    eclipse_info.eclipsed_by
                FROM source_initial s1
                CROSS JOIN LATERAL (
                    SELECT
                        -- OPTIMIZATION: Use pre-computed valid_range instead of repeated daterange() calls
                        -- Combined with composite index, this provides 30-40 percent total performance improvement
                        COALESCE(sql_saga.covers_without_gaps(s2.valid_range, s1.valid_range ORDER BY s2.valid_from), false) as is_eclipsed,
                        array_agg(s2.source_row_id) as eclipsed_by
                    FROM source_initial s2
                    WHERE
                        (
                            (NOT s1.natural_identity_column_values_are_null AND (false))
                            OR
                            (s1.natural_identity_column_values_are_null AND s1.causal_id = s2.causal_id)
                        )
                        AND
                        -- Only consider newer rows (higher row_id) as potential eclipsers.
                        s2.source_row_id > s1.source_row_id
                ) eclipse_info;
            
NOTICE:  Nested Loop (actual rows=2.00 loops=1)
NOTICE:    Buffers: local hit=3
NOTICE:    ->  Seq Scan on source_initial s1 (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Aggregate (actual rows=1.00 loops=2)
NOTICE:          Buffers: local hit=2
NOTICE:          ->  Sort (actual rows=0.00 loops=2)
NOTICE:                Sort Key: s2.valid_from
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                Buffers: local hit=2
NOTICE:                ->  Result (actual rows=0.00 loops=2)
NOTICE:                      One-Time Filter: s1.natural_identity_column_values_are_null
NOTICE:                      Buffers: local hit=2
NOTICE:                      ->  Seq Scan on source_initial s2 (actual rows=0.00 loops=2)
NOTICE:                            Filter: ((source_row_id > s1.source_row_id) AND (s1.causal_id = causal_id))
NOTICE:                            Rows Removed by Filter: 2
NOTICE:                            Buffers: local hit=2
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=49, local read=3
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE target_rows ON COMMIT DROP AS
                SELECT
                    es_id, lu_id, type,  /* v_non_temporal_lookup_cols_select_list_no_alias_prefix */ -- (non-temporal identity columns)
                    lower(t.valid_range) as valid_from, -- The temporal identity column (e.g., valid_from)
                    NULL::integer /* v_causal_column_type */ as causal_id, -- Target rows do not originate from a source row, so causal_id is NULL. Type is introspected.
                    jsonb_build_object('type', type, 'lu_id', lu_id, 'es_id', es_id) /* v_stable_pk_cols_jsonb_build_bare */ as stable_pk_payload,
                    upper(t.valid_range) as valid_until,
                    jsonb_build_object('value', value) /* v_target_data_cols_jsonb_build_bare */ AS data_payload,
                    jsonb_build_object() /* v_target_ephemeral_cols_jsonb_build_bare */ AS ephemeral_payload,
                    jsonb_strip_nulls(jsonb_build_object()) /* v_target_nk_json_expr */ AS canonical_nk_json
                FROM (
                        SELECT * FROM (
                            SELECT DISTINCT ON (u.type, u.lu_id, u.es_id, u.valid_from) * FROM (
                                (
                                        SELECT DISTINCT ON (inner_t.type, inner_t.lu_id, inner_t.es_id, inner_t.valid_from) inner_t.*
                                        FROM tmpc.target_nk inner_t
                                        JOIN (SELECT DISTINCT si.type, si.lu_id, si.es_id FROM source_initial si) AS si ON ((si.type = inner_t.type OR (si.type IS NULL AND inner_t.type IS NULL)) AND (si.lu_id = inner_t.lu_id OR (si.lu_id IS NULL AND inner_t.lu_id IS NULL)) AND (si.es_id = inner_t.es_id OR (si.es_id IS NULL AND inner_t.es_id IS NULL)))
                                    )
                            ) u
                        )
                    ) AS t  /* v_target_rows_filter */
            
NOTICE:  Subquery Scan on unnamed_subquery (actual rows=2.00 loops=1)
NOTICE:    Buffers: shared hit=29, local hit=1
NOTICE:    ->  Unique (actual rows=2.00 loops=1)
NOTICE:          Buffers: shared hit=29, local hit=1
NOTICE:          ->  Unique (actual rows=2.00 loops=1)
NOTICE:                Buffers: shared hit=29, local hit=1
NOTICE:                ->  Sort (actual rows=2.00 loops=1)
NOTICE:                      Sort Key: inner_t.type, inner_t.lu_id, inner_t.es_id, inner_t.valid_from
NOTICE:                      Sort Method: quicksort  Memory: 25kB
NOTICE:                      Buffers: shared hit=29, local hit=1
NOTICE:                      ->  Nested Loop (actual rows=2.00 loops=1)
NOTICE:                            Buffers: shared hit=29, local hit=1
NOTICE:                            ->  Unique (actual rows=2.00 loops=1)
NOTICE:                                  Buffers: local hit=1
NOTICE:                                  ->  Sort (actual rows=2.00 loops=1)
NOTICE:                                        Sort Key: si.type, si.lu_id, si.es_id
NOTICE:                                        Sort Method: quicksort  Memory: 25kB
NOTICE:                                        Buffers: local hit=1
NOTICE:                                        ->  Seq Scan on source_initial si (actual rows=2.00 loops=1)
NOTICE:                                              Buffers: local hit=1
NOTICE:                            ->  Bitmap Heap Scan on target_nk inner_t (actual rows=1.00 loops=2)
NOTICE:                                  Recheck Cond: ((si.type = type) OR (type IS NULL))
NOTICE:                                  Filter: (((si.type = type) OR ((si.type IS NULL) AND (type IS NULL))) AND ((si.lu_id = lu_id) OR ((si.lu_id IS NULL) AND (lu_id IS NULL))) AND ((si.es_id = es_id) OR ((si.es_id IS NULL) AND (es_id IS NULL))))
NOTICE:                                  Rows Removed by Filter: 999
NOTICE:                                  Heap Blocks: exact=18
NOTICE:                                  Buffers: shared hit=29
NOTICE:                                  ->  BitmapOr (actual rows=0.00 loops=2)
NOTICE:                                        Buffers: shared hit=11
NOTICE:                                        ->  Bitmap Index Scan on target_nk_type_es_id_idx (actual rows=1000.00 loops=2)
NOTICE:                                              Index Cond: (type = si.type)
NOTICE:                                              Index Searches: 2
NOTICE:                                              Buffers: shared hit=7
NOTICE:                                        ->  Bitmap Index Scan on target_nk_type_es_id_idx (actual rows=0.00 loops=2)
NOTICE:                                              Index Cond: (type IS NULL)
NOTICE:                                              Index Searches: 2
NOTICE:                                              Buffers: shared hit=4
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON target_rows (es_id, lu_id, type);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON target_rows USING gist (daterange(valid_from, valid_until));
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE target_rows;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_matches ON COMMIT DROP AS
                SELECT
                    source_row.*,
                    target_row.stable_pk_payload as discovered_stable_pk_payload,
                    target_row.es_id AS discovered_id_1, target_row.lu_id AS discovered_id_2, target_row.type AS discovered_id_3 /* v_propagated_id_cols_list */
                FROM source_with_eclipsed_flag source_row
                LEFT JOIN target_rows target_row ON (source_row.type IS NOT DISTINCT FROM target_row.type AND source_row.lu_id IS NOT DISTINCT FROM target_row.lu_id AND source_row.es_id IS NOT DISTINCT FROM target_row.es_id /* v_source_rows_exists_join_expr */)
            
NOTICE:  Nested Loop Left Join (actual rows=2.00 loops=1)
NOTICE:    Join Filter: ((NOT (source_row.type IS DISTINCT FROM target_row.type)) AND (NOT (source_row.lu_id IS DISTINCT FROM target_row.lu_id)) AND (NOT (source_row.es_id IS DISTINCT FROM target_row.es_id)))
NOTICE:    Rows Removed by Join Filter: 2
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_with_eclipsed_flag source_row (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Materialize (actual rows=2.00 loops=2)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on target_rows target_row (actual rows=2.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=54, local read=1
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_aggregates ON COMMIT DROP AS
                SELECT
                    source_row_id,
                    count(DISTINCT discovered_stable_pk_payload) as match_count,
                    jsonb_agg(DISTINCT discovered_stable_pk_payload) as conflicting_ids
                FROM source_rows_with_matches
                GROUP BY source_row_id
            
NOTICE:  GroupAggregate (actual rows=2.00 loops=1)
NOTICE:    Group Key: source_row_id
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=2.00 loops=1)
NOTICE:          Sort Key: source_row_id, discovered_stable_pk_payload
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on source_rows_with_matches (actual rows=2.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=46
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_discovery ON COMMIT DROP AS
                SELECT
                    m.*,
                    a.match_count,
                    a.conflicting_ids,
                    (a.match_count > 1) as is_ambiguous
                FROM source_rows_with_matches m
                JOIN source_rows_with_aggregates a ON m.source_row_id = a.source_row_id
            
NOTICE:  Hash Join (actual rows=2.00 loops=1)
NOTICE:    Hash Cond: (a.source_row_id = m.source_row_id)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_rows_with_aggregates a (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Hash (actual rows=2.00 loops=1)
NOTICE:          Buckets: 1024  Batches: 1  Memory Usage: 9kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on source_rows_with_matches m (actual rows=2.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=10
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows ON COMMIT DROP AS
                SELECT DISTINCT ON (p.source_row_id)
                    p.source_row_id, p.causal_id, p.valid_from, p.valid_until, p.data_payload, p.ephemeral_payload,
                    (p.stable_identity_columns_are_null AND p.discovered_stable_pk_payload IS NULL) as stable_identity_columns_are_null,
                    p.natural_identity_column_values_are_null, p.is_identifiable,
                    p.is_ambiguous, p.conflicting_ids, p.is_eclipsed, p.eclipsed_by,
                    p.temporal_columns_are_consistent,
                    COALESCE(p.stable_pk_payload, p.discovered_stable_pk_payload) as stable_pk_payload,
                    (p.discovered_stable_pk_payload IS NOT NULL) /* v_target_entity_exists_expr */ as target_entity_exists,
                    COALESCE(p.es_id, p.discovered_id_1) AS es_id, COALESCE(p.lu_id, p.discovered_id_2) AS lu_id, COALESCE(p.type, p.discovered_id_3) AS type /* v_coalesced_id_cols_list */
                FROM source_rows_with_discovery p
                ORDER BY p.source_row_id, p.discovered_stable_pk_payload
            
NOTICE:  Unique (actual rows=2.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=2.00 loops=1)
NOTICE:          Sort Key: source_row_id, discovered_stable_pk_payload
NOTICE:          Sort Method: quicksort  Memory: 25kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on source_rows_with_discovery p (actual rows=2.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=50
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_new_flag ON COMMIT DROP AS
                SELECT *, NOT target_entity_exists as is_new_entity
                FROM source_rows
            
NOTICE:  Seq Scan on source_rows (actual rows=2.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=40
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_nk_json ON COMMIT DROP AS
                SELECT source_row.*, jsonb_strip_nulls(jsonb_build_object()) /* v_lookup_keys_as_jsonb_expr */ as nk_json, ARRAY(SELECT e FROM unnest(ARRAY[]::TEXT[]) e WHERE e IS NOT NULL ORDER BY e) /* v_lookup_keys_as_array_expr */ as nk_non_null_keys_array
                FROM source_rows_with_new_flag source_row
            
NOTICE:  Seq Scan on source_rows_with_new_flag source_row (actual rows=2.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    InitPlan 1
NOTICE:      ->  Sort (actual rows=0.00 loops=1)
NOTICE:            Sort Key: e.e
NOTICE:            Sort Method: quicksort  Memory: 25kB
NOTICE:            ->  Function Scan on unnest e (actual rows=0.00 loops=1)
NOTICE:                  Filter: (e IS NOT NULL)
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=42
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON source_rows_with_nk_json USING GIN (nk_json);
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE source_rows_with_nk_json;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_canonical_key ON COMMIT DROP AS
                SELECT *, (CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (type IS NULL AND lu_id IS NULL AND es_id IS NULL) THEN COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') || '__' || COALESCE(es_id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') || '__' || COALESCE(es_id::text, '_NULL_')
            END /* v_grouping_key_expr */) as grouping_key
                FROM (
                    SELECT
                        s1.*,
                        s2.nk_json as canonical_nk_json
                    FROM source_rows_with_nk_json s1
                    LEFT JOIN LATERAL (
                        SELECT s2_inner.nk_json, s2_inner.nk_non_null_keys_array
                        FROM source_rows_with_nk_json s2_inner
                        WHERE s1.is_new_entity AND s2_inner.is_new_entity AND s2_inner.nk_json @> s1.nk_json
                        ORDER BY array_length(s2_inner.nk_non_null_keys_array, 1) DESC, s2_inner.nk_non_null_keys_array::text DESC
                        LIMIT 1
                    ) s2 ON true
                ) s
            
NOTICE:  Nested Loop Left Join (actual rows=2.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Seq Scan on source_rows_with_nk_json s1 (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Limit (actual rows=0.00 loops=2)
NOTICE:          ->  Sort (actual rows=0.00 loops=2)
NOTICE:                Sort Key: (array_length(s2_inner.nk_non_null_keys_array, 1)) DESC, ((s2_inner.nk_non_null_keys_array)::text) DESC
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                ->  Result (actual rows=0.00 loops=2)
NOTICE:                      One-Time Filter: s1.is_new_entity
NOTICE:                      ->  Seq Scan on source_rows_with_nk_json s2_inner (never executed)
NOTICE:                            Filter: (is_new_entity AND (nk_json @> s1.nk_json))
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=70, local hit=1
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE source_rows_with_early_feedback ON COMMIT DROP AS
                SELECT
                    s.*,
                    CASE
                        WHEN s.is_ambiguous
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row is ambiguous. It matches multiple distinct target entities: ' || s.conflicting_ids::text )
                        WHEN NOT s.is_identifiable AND s.is_new_entity
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row is unidentifiable. It has NULL for all stable identity columns ' || replace('{type,lu_id,es_id}'::text, '"', '') || ' and all natural keys ' || replace(NULL::text, '"', '') )
                        WHEN NOT s.temporal_columns_are_consistent
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row has inconsistent temporal columns. Column "' || 'valid_until' || '" must be equal to column "' || NULL || '" + ' || NULL || '.' )
                        WHEN s.is_eclipsed
                        THEN jsonb_build_object( 'operation', 'SKIP_ECLIPSED'::text, 'message', 'Source row was eclipsed by row_ids=' || s.eclipsed_by::text || ' in the same batch.' )
                        ELSE NULL
                    END as early_feedback
                FROM source_rows_with_canonical_key s
            
NOTICE:  Seq Scan on source_rows_with_canonical_key s (actual rows=2.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=50
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE active_source_rows ON COMMIT DROP AS
                SELECT source_row.*
                FROM source_rows_with_early_feedback source_row
                WHERE source_row.early_feedback IS NULL
                AND CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                    WHEN 'MERGE_ENTITY_PATCH' THEN true
                    WHEN 'MERGE_ENTITY_REPLACE' THEN true
                    WHEN 'MERGE_ENTITY_UPSERT' THEN true
                    WHEN 'INSERT_NEW_ENTITIES' THEN NOT source_row.target_entity_exists
                    WHEN 'PATCH_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'REPLACE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'DELETE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'UPDATE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    ELSE false
                END
            
NOTICE:  Seq Scan on source_rows_with_early_feedback source_row (actual rows=2.00 loops=1)
NOTICE:    Filter: (early_feedback IS NULL)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=52
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON active_source_rows (es_id, lu_id, type);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON active_source_rows (grouping_key);
NOTICE:  ---
NOTICE:  Executing setup: CREATE INDEX ON active_source_rows USING gist (daterange(valid_from, valid_until));
NOTICE:  ---
NOTICE:  Executing setup: ANALYZE active_source_rows;
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE all_rows ON COMMIT DROP AS
                SELECT es_id, lu_id, type,  /* v_non_temporal_lookup_cols_select_list_no_alias_prefix */ causal_id, valid_from, valid_until, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, temporal_columns_are_consistent, canonical_nk_json FROM active_source_rows
                UNION ALL
                SELECT
                    target_row.es_id, target_row.lu_id, target_row.type,  /* v_non_temporal_tr_qualified_lookup_cols_prefix */
                    target_row.causal_id,
                    target_row.valid_from,
                    target_row.valid_until,
                    false as is_new_entity,
                    target_row.stable_pk_payload,
                    false as stable_identity_columns_are_null,
                    false as natural_identity_column_values_are_null,
                    true as is_identifiable,
                    false as is_ambiguous,
                    NULL::jsonb as conflicting_ids,
                    true as temporal_columns_are_consistent,
                    target_row.canonical_nk_json
                FROM target_rows target_row
            
NOTICE:  Append (actual rows=4.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on active_source_rows (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Seq Scan on target_rows target_row (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=59, local read=2
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points_raw ON COMMIT DROP AS
                SELECT es_id, lu_id, type, causal_id, valid_from AS point, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json FROM all_rows
                UNION ALL
                SELECT es_id, lu_id, type, causal_id, valid_until AS point, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json FROM all_rows
            
NOTICE:  Append (actual rows=8.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on all_rows (actual rows=4.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Seq Scan on all_rows all_rows_1 (actual rows=4.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=32
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points_unified ON COMMIT DROP AS
                SELECT
                    *,
                    CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (type IS NULL AND lu_id IS NULL AND es_id IS NULL) THEN COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') || '__' || COALESCE(es_id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') || '__' || COALESCE(es_id::text, '_NULL_')
            END AS grouping_key,
                    CASE
                        WHEN is_new_entity THEN causal_id
                        ELSE FIRST_VALUE(causal_id) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (type IS NULL AND lu_id IS NULL AND es_id IS NULL) THEN COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') || '__' || COALESCE(es_id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') || '__' || COALESCE(es_id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS LAST)
                    END as unified_causal_id,
                    FIRST_VALUE(stable_pk_payload) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (type IS NULL AND lu_id IS NULL AND es_id IS NULL) THEN COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') || '__' || COALESCE(es_id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') || '__' || COALESCE(es_id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS FIRST) as unified_stable_pk_payload,
                    FIRST_VALUE(canonical_nk_json) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN NOT (type IS NULL AND lu_id IS NULL AND es_id IS NULL) THEN COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') || '__' || COALESCE(es_id::text, '_NULL_') ELSE causal_id::text END
                ELSE 'existing_entity__' || COALESCE(type::text, '_NULL_') || '__' || COALESCE(lu_id::text, '_NULL_') || '__' || COALESCE(es_id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS FIRST) as unified_canonical_nk_json
                FROM time_points_raw
            
NOTICE:  WindowAgg (actual rows=8.00 loops=1)
NOTICE:    Window: w2 AS (PARTITION BY (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((type IS NOT NULL) OR (lu_id IS NOT NULL) OR (es_id IS NOT NULL)) THEN ((((COALESCE(type, '_NULL_'::text) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((es_id)::text, '_NULL_'::text)) ELSE (causal_id)::text END) ELSE ((((('existing_entity__'::text || COALESCE(type, '_NULL_'::text)) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((es_id)::text, '_NULL_'::text)) END) ORDER BY causal_id)
NOTICE:    Storage: Memory  Maximum Storage: 17kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Incremental Sort (actual rows=8.00 loops=1)
NOTICE:          Sort Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((type IS NOT NULL) OR (lu_id IS NOT NULL) OR (es_id IS NOT NULL)) THEN ((((COALESCE(type, '_NULL_'::text) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((es_id)::text, '_NULL_'::text)) ELSE (causal_id)::text END) ELSE ((((('existing_entity__'::text || COALESCE(type, '_NULL_'::text)) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((es_id)::text, '_NULL_'::text)) END), causal_id
NOTICE:          Presorted Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((type IS NOT NULL) OR (lu_id IS NOT NULL) OR (es_id IS NOT NULL)) THEN ((((COALESCE(type, '_NULL_'::text) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((es_id)::text, '_NULL_'::text)) ELSE (causal_id)::text END) ELSE ((((('existing_entity__'::text || COALESCE(type, '_NULL_'::text)) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((es_id)::text, '_NULL_'::text)) END)
NOTICE:          Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 26kB  Peak Memory: 26kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  WindowAgg (actual rows=8.00 loops=1)
NOTICE:                Window: w1 AS (PARTITION BY (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((type IS NOT NULL) OR (lu_id IS NOT NULL) OR (es_id IS NOT NULL)) THEN ((((COALESCE(type, '_NULL_'::text) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((es_id)::text, '_NULL_'::text)) ELSE (causal_id)::text END) ELSE ((((('existing_entity__'::text || COALESCE(type, '_NULL_'::text)) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((es_id)::text, '_NULL_'::text)) END) ORDER BY causal_id)
NOTICE:                Storage: Memory  Maximum Storage: 17kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Sort (actual rows=8.00 loops=1)
NOTICE:                      Sort Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((type IS NOT NULL) OR (lu_id IS NOT NULL) OR (es_id IS NOT NULL)) THEN ((((COALESCE(type, '_NULL_'::text) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((es_id)::text, '_NULL_'::text)) ELSE (causal_id)::text END) ELSE ((((('existing_entity__'::text || COALESCE(type, '_NULL_'::text)) || '__'::text) || COALESCE((lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((es_id)::text, '_NULL_'::text)) END), causal_id NULLS FIRST
NOTICE:                      Sort Method: quicksort  Memory: 26kB
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  Seq Scan on time_points_raw (actual rows=8.00 loops=1)
NOTICE:                            Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=28
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points_with_unified_ids ON COMMIT DROP AS
                SELECT
                    tpu.grouping_key,
                    tpu.es_id, tpu.lu_id, tpu.type,
                    tpu.unified_causal_id as causal_id,
                    tpu.point,
                    tpu.is_new_entity,
                    tpu.unified_stable_pk_payload as stable_pk_payload,
                    tpu.stable_identity_columns_are_null,
                    tpu.natural_identity_column_values_are_null,
                    tpu.is_identifiable,
                    tpu.is_ambiguous,
                    tpu.conflicting_ids,
                    tpu.unified_canonical_nk_json as canonical_nk_json
                FROM time_points_unified tpu
            
NOTICE:  Seq Scan on time_points_unified tpu (actual rows=8.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=36
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE time_points ON COMMIT DROP AS
                SELECT DISTINCT ON (grouping_key, point) *
                FROM time_points_with_unified_ids
                ORDER BY grouping_key, point, causal_id DESC NULLS LAST
            
NOTICE:  Unique (actual rows=8.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=8.00 loops=1)
NOTICE:          Sort Key: grouping_key, point, causal_id DESC NULLS LAST
NOTICE:          Sort Method: quicksort  Memory: 26kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on time_points_with_unified_ids (actual rows=8.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=30
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE atomic_segments ON COMMIT DROP AS
                SELECT grouping_key, es_id, lu_id, type, causal_id, point as valid_from, next_point as valid_until, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, natural_identity_column_values_are_null, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json
                FROM (
                    SELECT *, LEAD(point) OVER (PARTITION BY grouping_key ORDER BY point) as next_point
                    FROM time_points
                ) with_lead
                WHERE point IS NOT NULL AND next_point IS NOT NULL AND point < next_point
            
NOTICE:  Subquery Scan on with_lead (actual rows=6.00 loops=1)
NOTICE:    Filter: ((with_lead.point IS NOT NULL) AND (with_lead.next_point IS NOT NULL) AND (with_lead.point < with_lead.next_point))
NOTICE:    Rows Removed by Filter: 2
NOTICE:    Buffers: local hit=1
NOTICE:    ->  WindowAgg (actual rows=8.00 loops=1)
NOTICE:          Window: w1 AS (PARTITION BY time_points.grouping_key ORDER BY time_points.point)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Sort (actual rows=8.00 loops=1)
NOTICE:                Sort Key: time_points.grouping_key, time_points.point
NOTICE:                Sort Method: quicksort  Memory: 26kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Seq Scan on time_points (actual rows=8.00 loops=1)
NOTICE:                      Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=30
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE existing_segments_with_target ON COMMIT DROP AS
                SELECT
                    seg.grouping_key, seg.canonical_nk_json, seg.es_id, seg.lu_id, seg.type, seg.causal_id, seg.is_new_entity, seg.is_identifiable, seg.is_ambiguous, seg.conflicting_ids, seg.stable_identity_columns_are_null, seg.natural_identity_column_values_are_null, seg.valid_from, seg.valid_until,
                    target_row.valid_from as t_valid_from, target_row.valid_until as t_valid_until,
                    target_row.data_payload as t_data_payload, target_row.ephemeral_payload as t_ephemeral_payload, seg.stable_pk_payload, 
                    target_row.stable_pk_payload as target_stable_pk_payload,
                    NULL::jsonb as trace
                FROM atomic_segments seg
                LEFT JOIN target_rows target_row 
                    ON (target_row.es_id = seg.es_id OR (target_row.es_id IS NULL AND seg.es_id IS NULL)) AND (target_row.lu_id = seg.lu_id OR (target_row.lu_id IS NULL AND seg.lu_id IS NULL)) AND (target_row.type = seg.type OR (target_row.type IS NULL AND seg.type IS NULL)) 
                    AND daterange(seg.valid_from, seg.valid_until) <@ daterange(target_row.valid_from, target_row.valid_until)
                WHERE NOT seg.is_new_entity
            
NOTICE:  Nested Loop Left Join (actual rows=6.00 loops=1)
NOTICE:    Join Filter: (((target_row.es_id = seg.es_id) OR ((target_row.es_id IS NULL) AND (seg.es_id IS NULL))) AND ((target_row.lu_id = seg.lu_id) OR ((target_row.lu_id IS NULL) AND (seg.lu_id IS NULL))) AND ((target_row.type = seg.type) OR ((target_row.type IS NULL) AND (seg.type IS NULL))) AND (daterange(seg.valid_from, seg.valid_until) <@ daterange(target_row.valid_from, target_row.valid_until)))
NOTICE:    Rows Removed by Join Filter: 6
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on atomic_segments seg (actual rows=6.00 loops=1)
NOTICE:          Filter: (NOT is_new_entity)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Materialize (actual rows=2.00 loops=6)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on target_rows target_row (actual rows=2.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=43
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE new_segments_no_target ON COMMIT DROP AS
                SELECT
                    seg.grouping_key, seg.canonical_nk_json, seg.es_id, seg.lu_id, seg.type, seg.causal_id, seg.is_new_entity, seg.is_identifiable, seg.is_ambiguous, seg.conflicting_ids, seg.stable_identity_columns_are_null, seg.natural_identity_column_values_are_null, seg.valid_from, seg.valid_until,
                    NULL::date as t_valid_from, NULL::date as t_valid_until,
                    NULL::jsonb as t_data_payload, NULL::jsonb as t_ephemeral_payload, seg.stable_pk_payload,
                    NULL::jsonb as target_stable_pk_payload,
                    NULL::jsonb as trace
                FROM atomic_segments seg
                WHERE seg.is_new_entity
            
NOTICE:  Seq Scan on atomic_segments seg (actual rows=0.00 loops=1)
NOTICE:    Filter: is_new_entity
NOTICE:    Rows Removed by Filter: 6
NOTICE:    Buffers: local hit=1
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments_with_payloads ON COMMIT DROP AS
                WITH all_segments AS (
                    SELECT * FROM existing_segments_with_target
                    UNION ALL
                    SELECT * FROM new_segments_no_target
                )
                SELECT
                    with_base_payload.*,
                    with_base_payload.stable_pk_payload as propagated_stable_pk_payload
                FROM (
                    SELECT
                        seg.*, source_payloads.source_row_id, source_payloads.contributing_row_ids, 
                        source_payloads.data_payload as s_data_payload, source_payloads.ephemeral_payload as s_ephemeral_payload,
                        source_payloads.causal_id as s_causal_id, source_payloads.causal_id as direct_source_causal_id,
                        source_payloads.valid_from AS s_valid_from, source_payloads.valid_until AS s_valid_until
                    FROM all_segments seg
                    
                        LEFT JOIN LATERAL (
                            WITH RECURSIVE ordered_sources AS (
                                SELECT
                                    source_row.source_row_id, source_row.data_payload, source_row.ephemeral_payload,
                                    source_row.valid_from, source_row.valid_until, source_row.causal_id,
                                    row_number() OVER (ORDER BY source_row.source_row_id) as rn
                                FROM active_source_rows source_row
                                WHERE 
                ((seg.is_new_entity AND source_row.grouping_key = seg.grouping_key)
                OR
                (NOT seg.is_new_entity AND (source_row.es_id = seg.es_id OR (source_row.es_id IS NULL AND seg.es_id IS NULL)) AND (source_row.lu_id = seg.lu_id OR (source_row.lu_id IS NULL AND seg.lu_id IS NULL)) AND (source_row.type = seg.type OR (source_row.type IS NULL AND seg.type IS NULL))))
             -- v_lateral_join_sr_to_seg
                                AND daterange(seg.valid_from, seg.valid_until) <@ daterange(source_row.valid_from, source_row.valid_until)
                            ),
                            running_payload AS (
                                SELECT rn, source_row_id, data_payload, ephemeral_payload, valid_from, valid_until, causal_id, ARRAY[source_row_id::BIGINT] as contributing_row_ids
                                FROM ordered_sources WHERE rn = 1
                                UNION ALL
                                SELECT
                                    s.rn, s.source_row_id,
                                    r.data_payload || jsonb_strip_nulls(s.data_payload),
                                    r.ephemeral_payload || jsonb_strip_nulls(s.ephemeral_payload),
                                    s.valid_from, s.valid_until, s.causal_id,
                                    r.contributing_row_ids || s.source_row_id::BIGINT
                                FROM running_payload r JOIN ordered_sources s ON s.rn = r.rn + 1
                            )
                            SELECT source_row_id, data_payload, ephemeral_payload, valid_from, valid_until, causal_id, contributing_row_ids
                            FROM running_payload
                            ORDER BY rn DESC
                            LIMIT 1
                        ) source_payloads ON true
                    
                    WHERE (source_payloads.data_payload IS NOT NULL OR seg.t_data_payload IS NOT NULL)
                    AND CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                        WHEN 'PATCH_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        WHEN 'REPLACE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        WHEN 'DELETE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        WHEN 'UPDATE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                        ELSE true
                    END
                ) with_base_payload
            
NOTICE:  Nested Loop Left Join (actual rows=6.00 loops=1)
NOTICE:    Filter: ((running_payload.data_payload IS NOT NULL) OR (existing_segments_with_target.t_data_payload IS NOT NULL))
NOTICE:    Buffers: local hit=7
NOTICE:    ->  Append (actual rows=6.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on existing_segments_with_target (actual rows=6.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:          ->  Seq Scan on new_segments_no_target (actual rows=0.00 loops=1)
NOTICE:    ->  Limit (actual rows=0.33 loops=6)
NOTICE:          Buffers: local hit=6
NOTICE:          CTE ordered_sources
NOTICE:            ->  WindowAgg (actual rows=0.33 loops=6)
NOTICE:                  Window: w1 AS (ORDER BY source_row.source_row_id ROWS UNBOUNDED PRECEDING)
NOTICE:                  Storage: Memory  Maximum Storage: 17kB
NOTICE:                  Buffers: local hit=6
NOTICE:                  ->  Sort (actual rows=0.33 loops=6)
NOTICE:                        Sort Key: source_row.source_row_id
NOTICE:                        Sort Method: quicksort  Memory: 25kB
NOTICE:                        Buffers: local hit=6
NOTICE:                        ->  Seq Scan on active_source_rows source_row (actual rows=0.33 loops=6)
NOTICE:                              Filter: ((daterange(existing_segments_with_target.valid_from, existing_segments_with_target.valid_until) <@ daterange(valid_from, valid_until)) AND ((existing_segments_with_target.is_new_entity AND (grouping_key = existing_segments_with_target.grouping_key)) OR ((NOT existing_segments_with_target.is_new_entity) AND ((es_id = existing_segments_with_target.es_id) OR ((es_id IS NULL) AND (existing_segments_with_target.es_id IS NULL))) AND ((lu_id = existing_segments_with_target.lu_id) OR ((lu_id IS NULL) AND (existing_segments_with_target.lu_id IS NULL))) AND ((type = existing_segments_with_target.type) OR ((type IS NULL) AND (existing_segments_with_target.type IS NULL))))))
NOTICE:                              Rows Removed by Filter: 2
NOTICE:                              Buffers: local hit=6
NOTICE:          CTE running_payload
NOTICE:            ->  Recursive Union (actual rows=0.33 loops=6)
NOTICE:                  Storage: Memory  Maximum Storage: 33kB
NOTICE:                  Buffers: local hit=6
NOTICE:                  ->  CTE Scan on ordered_sources (actual rows=0.33 loops=6)
NOTICE:                        Filter: (rn = 1)
NOTICE:                        Storage: Memory  Maximum Storage: 17kB
NOTICE:                        Buffers: local hit=6
NOTICE:                  ->  Hash Join (actual rows=0.00 loops=6)
NOTICE:                        Hash Cond: ((r.rn + 1) = s.rn)
NOTICE:                        ->  WorkTable Scan on running_payload r (actual rows=0.67 loops=3)
NOTICE:                        ->  Hash (actual rows=0.40 loops=5)
NOTICE:                              Buckets: 1024  Batches: 1  Memory Usage: 9kB
NOTICE:                              ->  CTE Scan on ordered_sources s (actual rows=0.40 loops=5)
NOTICE:                                    Storage: Memory  Maximum Storage: 17kB
NOTICE:          ->  Sort (actual rows=0.33 loops=6)
NOTICE:                Sort Key: running_payload.rn DESC
NOTICE:                Sort Method: quicksort  Memory: 25kB
NOTICE:                Buffers: local hit=6
NOTICE:                ->  CTE Scan on running_payload (actual rows=0.33 loops=6)
NOTICE:                      Storage: Memory  Maximum Storage: 17kB
NOTICE:                      Buffers: local hit=6
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=103
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments_with_propagated_ids ON COMMIT DROP AS
                SELECT
                    *,
                    FIRST_VALUE(canonical_nk_json) OVER (PARTITION BY grouping_key ORDER BY valid_from) as unified_canonical_nk_json,
                    COALESCE(
                        contributing_row_ids,
                        (array_concat_agg(contributing_row_ids) FILTER (WHERE contributing_row_ids IS NOT NULL) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (array_concat_agg(contributing_row_ids) FILTER (WHERE contributing_row_ids IS NOT NULL) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_contributing_row_ids,
                    COALESCE(
                        s_valid_from,
                        (max(s_valid_from) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (max(s_valid_from) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_s_valid_from,
                    COALESCE(
                        s_valid_until,
                        (max(s_valid_until) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (max(s_valid_until) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_s_valid_until
                FROM (
                    SELECT
                        *,
                        sum(CASE WHEN source_row_id IS NOT NULL THEN 1 ELSE 0 END) OVER (PARTITION BY grouping_key, t_valid_from ORDER BY valid_from) AS look_behind_grp,
                        sum(CASE WHEN source_row_id IS NOT NULL THEN 1 ELSE 0 END) OVER (PARTITION BY grouping_key, t_valid_from ORDER BY valid_from DESC) AS look_ahead_grp
                    FROM resolved_atomic_segments_with_payloads /* v_resolver_from */
                ) with_grp
            
NOTICE:  WindowAgg (actual rows=6.00 loops=1)
NOTICE:    Window: w3 AS (PARTITION BY with_grp.grouping_key ORDER BY with_grp.valid_from)
NOTICE:    Storage: Memory  Maximum Storage: 18kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Incremental Sort (actual rows=6.00 loops=1)
NOTICE:          Sort Key: with_grp.grouping_key, with_grp.valid_from
NOTICE:          Presorted Key: with_grp.grouping_key
NOTICE:          Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 27kB  Peak Memory: 27kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  WindowAgg (actual rows=6.00 loops=1)
NOTICE:                Window: w2 AS (PARTITION BY with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_behind_grp)
NOTICE:                Storage: Memory  Maximum Storage: 17kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Incremental Sort (actual rows=6.00 loops=1)
NOTICE:                      Sort Key: with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_behind_grp
NOTICE:                      Presorted Key: with_grp.grouping_key, with_grp.t_valid_from
NOTICE:                      Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 27kB  Peak Memory: 27kB
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  WindowAgg (actual rows=6.00 loops=1)
NOTICE:                            Window: w1 AS (PARTITION BY with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_ahead_grp)
NOTICE:                            Storage: Memory  Maximum Storage: 17kB
NOTICE:                            Buffers: local hit=1
NOTICE:                            ->  Incremental Sort (actual rows=6.00 loops=1)
NOTICE:                                  Sort Key: with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_ahead_grp
NOTICE:                                  Presorted Key: with_grp.grouping_key, with_grp.t_valid_from
NOTICE:                                  Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 27kB  Peak Memory: 27kB
NOTICE:                                  Buffers: local hit=1
NOTICE:                                  ->  Subquery Scan on with_grp (actual rows=6.00 loops=1)
NOTICE:                                        Buffers: local hit=1
NOTICE:                                        ->  WindowAgg (actual rows=6.00 loops=1)
NOTICE:                                              Window: w2 AS (PARTITION BY resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from ORDER BY resolved_atomic_segments_with_payloads.valid_from)
NOTICE:                                              Storage: Memory  Maximum Storage: 18kB
NOTICE:                                              Buffers: local hit=1
NOTICE:                                              ->  Incremental Sort (actual rows=6.00 loops=1)
NOTICE:                                                    Sort Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from, resolved_atomic_segments_with_payloads.valid_from
NOTICE:                                                    Presorted Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from
NOTICE:                                                    Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 27kB  Peak Memory: 27kB
NOTICE:                                                    Buffers: local hit=1
NOTICE:                                                    ->  WindowAgg (actual rows=6.00 loops=1)
NOTICE:                                                          Window: w1 AS (PARTITION BY resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from ORDER BY resolved_atomic_segments_with_payloads.valid_from)
NOTICE:                                                          Storage: Memory  Maximum Storage: 18kB
NOTICE:                                                          Buffers: local hit=1
NOTICE:                                                          ->  Sort (actual rows=6.00 loops=1)
NOTICE:                                                                Sort Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from, resolved_atomic_segments_with_payloads.valid_from DESC
NOTICE:                                                                Sort Method: quicksort  Memory: 27kB
NOTICE:                                                                Buffers: local hit=1
NOTICE:                                                                ->  Seq Scan on resolved_atomic_segments_with_payloads (actual rows=6.00 loops=1)
NOTICE:                                                                      Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=62
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments ON COMMIT DROP AS
                SELECT
                    grouping_key, es_id, lu_id, type,  stable_identity_columns_are_null, natural_identity_column_values_are_null, is_new_entity, is_identifiable, is_ambiguous, conflicting_ids, unified_canonical_nk_json,
                    valid_from, valid_until, t_valid_from, t_valid_until, propagated_s_valid_from, propagated_s_valid_until, propagated_contributing_row_ids, causal_id, propagated_stable_pk_payload as stable_pk_payload,
                    propagated_contributing_row_ids IS NULL AS unaffected_target_only_segment,
                    s_data_payload, t_data_payload,
                    sql_saga.get_allen_relation(propagated_s_valid_from, propagated_s_valid_until, t_valid_from, t_valid_until) AS s_t_relation,
                    COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb) as data_payload,
                    md5((jsonb_strip_nulls(COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb)))::text) as data_hash,
                    CASE 
                        WHEN s_data_payload IS NULL THEN t_ephemeral_payload
                        ELSE COALESCE(t_ephemeral_payload, '{}'::jsonb) || COALESCE(s_ephemeral_payload, '{}'::jsonb)
                    END as ephemeral_payload,
                    CASE WHEN 'f'::boolean
                        THEN trace || jsonb_build_object( 'cte', 'ras', 'propagated_stable_pk_payload', propagated_stable_pk_payload, 'final_data_payload', COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb), 'final_ephemeral_payload', CASE WHEN s_data_payload IS NULL THEN t_ephemeral_payload ELSE COALESCE(t_ephemeral_payload, '{}'::jsonb) || COALESCE(s_ephemeral_payload, '{}'::jsonb) END )
                        ELSE NULL
                    END as trace,
                    CASE WHEN s_data_payload IS NOT NULL THEN 1 ELSE 2 END as priority
                FROM resolved_atomic_segments_with_propagated_ids
            
NOTICE:  Seq Scan on resolved_atomic_segments_with_propagated_ids (actual rows=6.00 loops=1)
NOTICE:    Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=74
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE island_group ON COMMIT DROP AS
                SELECT
                    *,
                    SUM(is_island_start) OVER (PARTITION BY grouping_key ORDER BY valid_from) as island_group_id
                FROM (
                    SELECT
                        *,
                        CASE
                            WHEN prev_valid_until IS NULL
                            OR prev_valid_until <> valid_from
                            OR (prev_data_hash IS DISTINCT FROM data_hash)
                            THEN 1
                            ELSE 0
                        END as is_island_start
                    FROM (
                        SELECT
                            *,
                            LAG(valid_until) OVER w as prev_valid_until,
                            LAG(data_hash) OVER w as prev_data_hash,
                            LAG(data_payload) OVER w as prev_data_payload
                        FROM resolved_atomic_segments ras
                        WHERE ras.data_payload IS NOT NULL
                        WINDOW w AS (PARTITION BY grouping_key ORDER BY valid_from)
                    ) s1
                ) s2
            
NOTICE:  WindowAgg (actual rows=6.00 loops=1)
NOTICE:    Window: w1 AS (PARTITION BY s1.grouping_key ORDER BY s1.valid_from)
NOTICE:    Storage: Memory  Maximum Storage: 18kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Subquery Scan on s1 (actual rows=6.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:          ->  WindowAgg (actual rows=6.00 loops=1)
NOTICE:                Window: w AS (PARTITION BY ras.grouping_key ORDER BY ras.valid_from)
NOTICE:                Storage: Memory  Maximum Storage: 18kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Sort (actual rows=6.00 loops=1)
NOTICE:                      Sort Key: ras.grouping_key, ras.valid_from
NOTICE:                      Sort Method: quicksort  Memory: 26kB
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  Seq Scan on resolved_atomic_segments ras (actual rows=6.00 loops=1)
NOTICE:                            Filter: (data_payload IS NOT NULL)
NOTICE:                            Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=60
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE coalesced_final_segments ON COMMIT DROP AS
                SELECT
                    grouping_key, max(es_id) as es_id, max(lu_id) as lu_id, max(type) as type,
                    sql_saga.first(causal_id ORDER BY valid_from) as causal_id,
                    sql_saga.first(stable_identity_columns_are_null ORDER BY valid_from) as stable_identity_columns_are_null,
                    sql_saga.first(natural_identity_column_values_are_null ORDER BY valid_from) as natural_identity_column_values_are_null,
                    sql_saga.first(is_new_entity ORDER BY valid_from) as is_new_entity,
                    sql_saga.first(is_identifiable ORDER BY valid_from) as is_identifiable,
                    sql_saga.first(is_ambiguous ORDER BY valid_from) as is_ambiguous,
                    sql_saga.first(conflicting_ids ORDER BY valid_from) as conflicting_ids,
                    sql_saga.first(unified_canonical_nk_json ORDER BY valid_from) as canonical_nk_json,
                    sql_saga.first(s_t_relation ORDER BY valid_from) as s_t_relation,
                    sql_saga.first(t_valid_from ORDER BY valid_from) as ancestor_valid_from,
                    MIN(valid_from) as valid_from,
                    MAX(valid_until) as valid_until,
                    (sql_saga.first(data_payload ORDER BY valid_from DESC) || sql_saga.first(ephemeral_payload ORDER BY valid_from DESC)) as data_payload,
                    sql_saga.first(stable_pk_payload ORDER BY valid_from DESC) as stable_pk_payload,
                    bool_and(unaffected_target_only_segment) as unaffected_target_only_segment,
                    (SELECT array_agg(DISTINCT e) FROM unnest(array_concat_agg(propagated_contributing_row_ids)) e WHERE e IS NOT NULL) as row_ids,
                    CASE WHEN 'f'::boolean
                        THEN jsonb_build_object( 'cte', 'coalesced', 'island_group_id', island_group_id, 'coalesced_stable_pk_payload', sql_saga.first(stable_pk_payload ORDER BY valid_from DESC), 'final_payload', sql_saga.first(data_payload ORDER BY valid_from DESC), 'final_payload_sans_ephemeral', sql_saga.first(data_payload - '{}'::text[] ORDER BY valid_from DESC), 'atomic_traces', jsonb_agg((trace || jsonb_build_object('data_hash', data_hash, 'prev_data_hash', prev_data_hash, 'prev_data_payload', prev_data_payload)) ORDER BY valid_from) )
                        ELSE NULL
                    END as trace
                FROM island_group
                GROUP BY grouping_key, island_group_id
            
NOTICE:  GroupAggregate (actual rows=6.00 loops=1)
NOTICE:    Group Key: island_group.grouping_key, island_group.island_group_id
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=6.00 loops=1)
NOTICE:          Sort Key: island_group.grouping_key, island_group.island_group_id, island_group.valid_from
NOTICE:          Sort Method: quicksort  Memory: 26kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on island_group (actual rows=6.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:    SubPlan 1
NOTICE:      ->  Aggregate (actual rows=1.00 loops=6)
NOTICE:            ->  Sort (actual rows=1.00 loops=6)
NOTICE:                  Sort Key: e.e
NOTICE:                  Sort Method: quicksort  Memory: 25kB
NOTICE:                  ->  Function Scan on unnest e (actual rows=1.00 loops=6)
NOTICE:                        Filter: (e IS NOT NULL)
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=72
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE diff ON COMMIT DROP AS
                SELECT
                    final_seg.grouping_key, COALESCE(final_seg.es_id, target_seg.es_id) as es_id, COALESCE(final_seg.lu_id, target_seg.lu_id) as lu_id, COALESCE(final_seg.type, target_seg.type) as type, COALESCE(final_seg.causal_id, target_seg.causal_id) as causal_id,
                    COALESCE(final_seg.is_new_entity, false) as is_new_entity,
                    COALESCE(final_seg.is_identifiable, true) as is_identifiable,
                    COALESCE(final_seg.is_ambiguous, false) as is_ambiguous,
                    final_seg.conflicting_ids,
                    final_seg.canonical_nk_json,
                    COALESCE(final_seg.stable_identity_columns_are_null, false) as stable_identity_columns_are_null,
                    COALESCE(final_seg.natural_identity_column_values_are_null, false) as natural_identity_column_values_are_null,
                    final_seg.valid_from AS f_from, final_seg.valid_until AS f_until, final_seg.data_payload AS f_data, final_seg.row_ids AS f_row_ids, final_seg.stable_pk_payload, final_seg.s_t_relation,
                    CASE WHEN 'f'::boolean
                        THEN final_seg.trace || jsonb_build_object('cte', 'diff', 'diff_stable_pk_payload', final_seg.stable_pk_payload, 'final_seg_causal_id', final_seg.causal_id, 'final_payload_vs_target_payload', jsonb_build_object('f', final_seg.data_payload, 't', target_seg.data_payload))
                        ELSE NULL
                    END as trace,
                    final_seg.unaffected_target_only_segment,
                    target_seg.valid_from as t_from, target_seg.valid_until as t_until, (target_seg.data_payload || target_seg.ephemeral_payload) as t_data,
                    sql_saga.get_allen_relation(target_seg.valid_from, target_seg.valid_until, final_seg.valid_from, final_seg.valid_until) as b_a_relation
                FROM coalesced_final_segments AS final_seg
                FULL OUTER JOIN target_rows AS target_seg ON final_seg.grouping_key = ('existing_entity__' || COALESCE(target_seg.type::text, '_NULL_') || '__' || COALESCE(target_seg.lu_id::text, '_NULL_') || '__' || COALESCE(target_seg.es_id::text, '_NULL_')) AND final_seg.ancestor_valid_from = target_seg.valid_from
            
NOTICE:  Hash Full Join (actual rows=6.00 loops=1)
NOTICE:    Hash Cond: ((((((('existing_entity__'::text || COALESCE(target_seg.type, '_NULL_'::text)) || '__'::text) || COALESCE((target_seg.lu_id)::text, '_NULL_'::text)) || '__'::text) || COALESCE((target_seg.es_id)::text, '_NULL_'::text)) = final_seg.grouping_key) AND (target_seg.valid_from = final_seg.ancestor_valid_from))
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on target_rows target_seg (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Hash (actual rows=6.00 loops=1)
NOTICE:          Buckets: 1024  Batches: 1  Memory Usage: 10kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on coalesced_final_segments final_seg (actual rows=6.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=46
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE diff_ranked ON COMMIT DROP AS
                SELECT
                    d.*,
                    CASE
                        WHEN d.t_from IS NULL OR d.f_from IS NULL THEN NULL
                        WHEN d.f_from = d.t_from AND d.f_until = d.t_until AND d.f_data IS NOT DISTINCT FROM d.t_data THEN NULL
                        ELSE
                            row_number() OVER (
                                PARTITION BY d.grouping_key, d.t_from
                                ORDER BY
                                    CASE WHEN d.f_from = d.t_from THEN 1 ELSE 2 END,
                                    CASE WHEN d.f_data - '{}'::text[] IS NOT DISTINCT FROM d.t_data - '{}'::text[] THEN 1 ELSE 2 END,
                                    d.f_from,
                                    d.f_until
                            )
                    END as update_rank
                FROM diff d
            
NOTICE:  WindowAgg (actual rows=6.00 loops=1)
NOTICE:    Window: w1 AS (PARTITION BY grouping_key, t_from ORDER BY (CASE WHEN (f_from = t_from) THEN 1 ELSE 2 END), (CASE WHEN (NOT ((f_data - '{}'::text[]) IS DISTINCT FROM (t_data - '{}'::text[]))) THEN 1 ELSE 2 END), f_from, f_until ROWS UNBOUNDED PRECEDING)
NOTICE:    Storage: Memory  Maximum Storage: 17kB
NOTICE:    Buffers: local hit=1
NOTICE:    ->  Sort (actual rows=6.00 loops=1)
NOTICE:          Sort Key: grouping_key, t_from, (CASE WHEN (f_from = t_from) THEN 1 ELSE 2 END), (CASE WHEN (NOT ((f_data - '{}'::text[]) IS DISTINCT FROM (t_data - '{}'::text[]))) THEN 1 ELSE 2 END), f_from, f_until
NOTICE:          Sort Method: quicksort  Memory: 26kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on diff d (actual rows=6.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=50
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE plan_with_op ON COMMIT DROP AS
                (
                    SELECT * FROM (
                        SELECT
                            d.f_row_ids as row_ids, d.s_t_relation, d.is_new_entity,
                            CASE
                                WHEN d.is_ambiguous THEN 'ERROR'::sql_saga.temporal_merge_plan_action
                                WHEN d.is_new_entity AND NOT d.is_identifiable THEN 'ERROR'::sql_saga.temporal_merge_plan_action
                                WHEN d.t_from IS NULL THEN 'INSERT'::sql_saga.temporal_merge_plan_action
                                WHEN d.f_from IS NULL THEN 'DELETE'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank = 1 THEN 'UPDATE'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank > 1 THEN 'INSERT'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank IS NULL THEN
                                    CASE
                                        WHEN d.unaffected_target_only_segment THEN NULL
                                        ELSE 'SKIP_IDENTICAL'::sql_saga.temporal_merge_plan_action
                                    END
                                ELSE 'ERROR'::sql_saga.temporal_merge_plan_action
                            END as operation,
                            es_id, lu_id, type,
                            CASE
                                WHEN d.is_new_entity AND d.canonical_nk_json IS NOT NULL
                                THEN d.canonical_nk_json || COALESCE(d.stable_pk_payload, '{}'::jsonb)
                                ELSE jsonb_build_object('es_id', d.es_id, 'lu_id', d.lu_id, 'type', d.type) || COALESCE(d.stable_pk_payload, '{}'::jsonb)
                            END as entity_keys_json,
                            jsonb_build_object('type', d.type, 'lu_id', d.lu_id, 'es_id', d.es_id) as identity_keys,
                            jsonb_build_object() as lookup_keys,
                            d.causal_id, d.t_from as old_valid_from, d.t_until as old_valid_until, d.f_from as new_valid_from, d.f_until as new_valid_until,
                            CASE
                                WHEN d.is_ambiguous THEN NULL
                                WHEN d.is_new_entity AND NOT d.is_identifiable THEN NULL
                                ELSE d.f_data
                            END as data,
                            CASE
                                WHEN d.is_ambiguous
                                THEN jsonb_build_object('error', format('Source row is ambiguous. It matches multiple distinct target entities: %s', d.conflicting_ids))
                                WHEN d.is_new_entity AND NOT d.is_identifiable
                                THEN jsonb_build_object('error', 'Source row is unidentifiable. It has NULL for all stable identity columns ' || replace('{type,lu_id,es_id}'::text, '"', '') || ' and all natural keys ' || replace(NULL::text, '"', ''))
                                ELSE NULL
                            END as feedback,
                            d.b_a_relation, d.grouping_key,
                            CASE WHEN 'f'::boolean
                                THEN d.trace || jsonb_build_object( 'cte', 'plan_with_op', 'diff_is_new_entity', d.is_new_entity, 'diff_causal_id', d.causal_id, 'entity_keys_from_key_cols', jsonb_build_object('es_id', d.es_id, 'lu_id', d.lu_id, 'type', d.type), 'entity_keys_from_stable_pk', d.stable_pk_payload, 'final_entity_id_json', jsonb_build_object('es_id', d.es_id, 'lu_id', d.lu_id, 'type', d.type) || COALESCE(d.stable_pk_payload, '{}'::jsonb) )
                                ELSE NULL
                            END as trace
                        FROM diff_ranked d
                        WHERE d.f_row_ids IS NOT NULL OR d.t_data IS NOT NULL
                    ) with_op
                    WHERE with_op.operation IS NOT NULL
                )
                UNION ALL
                (
                    SELECT
                        ARRAY[source_row.source_row_id::BIGINT],
                        NULL::sql_saga.allen_interval_relation, source_row.is_new_entity,
                        COALESCE(
                            (source_row.early_feedback->>'operation')::sql_saga.temporal_merge_plan_action,
                            CASE
                                WHEN 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode = 'INSERT_NEW_ENTITIES' AND source_row.target_entity_exists THEN 'SKIP_FILTERED'::sql_saga.temporal_merge_plan_action
                                WHEN 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode IN ('PATCH_FOR_PORTION_OF', 'REPLACE_FOR_PORTION_OF', 'DELETE_FOR_PORTION_OF', 'UPDATE_FOR_PORTION_OF') AND NOT source_row.target_entity_exists THEN 'SKIP_NO_TARGET'::sql_saga.temporal_merge_plan_action
                                ELSE 'ERROR'::sql_saga.temporal_merge_plan_action
                            END
                        ),
                        es_id, lu_id, type,
                        CASE
                            WHEN source_row.is_new_entity AND source_row.canonical_nk_json IS NOT NULL
                            THEN source_row.canonical_nk_json || COALESCE(source_row.stable_pk_payload, '{}'::jsonb)
                            ELSE jsonb_build_object('es_id', source_row.es_id, 'lu_id', source_row.lu_id, 'type', source_row.type)
                        END as entity_keys_json,
                        jsonb_build_object('type', source_row.type, 'lu_id', source_row.lu_id, 'es_id', source_row.es_id) as identity_keys, jsonb_build_object() as lookup_keys,
                        source_row.causal_id,
                        NULL, NULL, NULL, NULL, NULL,
                        CASE
                            WHEN source_row.early_feedback IS NOT NULL THEN jsonb_build_object('error', source_row.early_feedback->>'message')
                            ELSE jsonb_build_object('info', 'Source row was correctly filtered by the mode''s logic and did not result in a DML operation.')
                        END,
                        NULL,
                        CASE
                    WHEN source_row.is_new_entity
                    THEN 'new_entity__' || source_row.causal_id::text
                    ELSE 'existing_entity__' || COALESCE(source_row.type::text, '_NULL_') || '__' || COALESCE(source_row.lu_id::text, '_NULL_') || '__' || COALESCE(source_row.es_id::text, '_NULL_')
                END AS grouping_key,
                        NULL::jsonb
                    FROM source_rows_with_early_feedback source_row
                    WHERE
                        source_row.early_feedback IS NOT NULL
                        OR NOT (
                            CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                            WHEN 'MERGE_ENTITY_PATCH' THEN true
                            WHEN 'MERGE_ENTITY_REPLACE' THEN true
                            WHEN 'MERGE_ENTITY_UPSERT' THEN true
                            WHEN 'INSERT_NEW_ENTITIES' THEN NOT source_row.target_entity_exists
                            WHEN 'PATCH_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'REPLACE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'DELETE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'UPDATE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            ELSE false
                        END
                    )
                )
            
NOTICE:  Append (actual rows=6.00 loops=1)
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on diff_ranked d (actual rows=6.00 loops=1)
NOTICE:          Filter: (((f_row_ids IS NOT NULL) OR (t_data IS NOT NULL)) AND (CASE WHEN is_ambiguous THEN 'ERROR'::sql_saga.temporal_merge_plan_action WHEN (is_new_entity AND (NOT is_identifiable)) THEN 'ERROR'::sql_saga.temporal_merge_plan_action WHEN (t_from IS NULL) THEN 'INSERT'::sql_saga.temporal_merge_plan_action WHEN (f_from IS NULL) THEN 'DELETE'::sql_saga.temporal_merge_plan_action WHEN (update_rank = 1) THEN 'UPDATE'::sql_saga.temporal_merge_plan_action WHEN (update_rank > 1) THEN 'INSERT'::sql_saga.temporal_merge_plan_action WHEN (update_rank IS NULL) THEN CASE WHEN unaffected_target_only_segment THEN NULL::sql_saga.temporal_merge_plan_action ELSE 'SKIP_IDENTICAL'::sql_saga.temporal_merge_plan_action END ELSE 'ERROR'::sql_saga.temporal_merge_plan_action END IS NOT NULL))
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Seq Scan on source_rows_with_early_feedback source_row (actual rows=0.00 loops=1)
NOTICE:          Filter: (early_feedback IS NOT NULL)
NOTICE:          Rows Removed by Filter: 2
NOTICE:          Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=54
NOTICE:  ---
NOTICE:  Explaining: 
                CREATE TEMP TABLE plan ON COMMIT DROP AS
                SELECT
                    p.row_ids, p.operation, p.causal_id, p.is_new_entity,
                    p.es_id, p.lu_id, p.type,
                    p.entity_keys_json as entity_keys,
                    p.identity_keys, p.lookup_keys,
                    p.s_t_relation, p.b_a_relation, p.old_valid_from, p.old_valid_until,
                    p.new_valid_from, p.new_valid_until, p.data, p.feedback, p.trace,
                    p.grouping_key,
                    CASE
                        WHEN p.operation <> 'UPDATE' THEN NULL::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from = p.old_valid_from AND p.new_valid_until = p.old_valid_until THEN 'NONE'::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from <= p.old_valid_from AND p.new_valid_until >= p.old_valid_until THEN 'GROW'::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from >= p.old_valid_from AND p.new_valid_until <= p.old_valid_until THEN 'SHRINK'::sql_saga.temporal_merge_update_effect
                        ELSE 'MOVE'::sql_saga.temporal_merge_update_effect
                    END AS update_effect
                FROM plan_with_op p
                LEFT JOIN source_rows_with_new_flag source_row ON source_row.source_row_id = p.row_ids[1]
            
NOTICE:  Hash Right Join (actual rows=6.00 loops=1)
NOTICE:    Hash Cond: (source_row.source_row_id = p.row_ids[1])
NOTICE:    Buffers: local hit=2
NOTICE:    ->  Seq Scan on source_rows_with_new_flag source_row (actual rows=2.00 loops=1)
NOTICE:          Buffers: local hit=1
NOTICE:    ->  Hash (actual rows=6.00 loops=1)
NOTICE:          Buckets: 1024  Batches: 1  Memory Usage: 10kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Seq Scan on plan_with_op p (actual rows=6.00 loops=1)
NOTICE:                Buffers: local hit=1
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=46
NOTICE:  ---
NOTICE:  Explaining: 
                WITH RECURSIVE ordered_plan AS (
                    SELECT
                        row_number() OVER ( ORDER BY p.grouping_key, p.es_id, p.lu_id, p.type, CASE p.operation WHEN 'DELETE' THEN 1 WHEN 'UPDATE' THEN 2 WHEN 'INSERT' THEN 3 ELSE 4 END, p.update_effect NULLS FIRST, CASE WHEN p.update_effect = 'MOVE' THEN 1 ELSE 0 END, CASE WHEN p.update_effect = 'MOVE' THEN COALESCE(p.old_valid_from, p.new_valid_from) END DESC NULLS LAST, COALESCE(p.old_valid_from, p.new_valid_from), (p.row_ids[1]) )::BIGINT as plan_op_seq,
                        p.*
                    FROM plan p
                ),
                -- For MOVE operations, group into the largest possible batches that can execute together.
                -- Within a single UPDATE statement, all rows see the pre-update state (MVCC snapshot).
                -- So if MOVE A's new_range overlaps MOVE B's old_range, they cannot be in the same statement.
                --
                -- Algorithm: Process MOVEs in plan_op_seq order (which is old_valid_from DESC).
                -- Track a multirange of old_ranges in the current batch. For each MOVE:
                -- - If new_range overlaps the batch's multirange, start a new batch
                -- - Add this MOVE's old_range to the batch's multirange
                --
                -- We use a recursive CTE to accumulate the multirange with reset-on-conflict logic.
                moves_ordered AS (
                    SELECT
                        o.plan_op_seq,
                        o.grouping_key,
                        o.old_valid_from,
                        o.old_valid_until,
                        o.new_valid_from,
                        o.new_valid_until,
                        row_number() OVER (PARTITION BY o.grouping_key ORDER BY o.plan_op_seq) AS move_seq
                    FROM ordered_plan o
                    WHERE o.operation = 'UPDATE' AND o.update_effect = 'MOVE'
                ),
                moves_with_batch AS (
                    -- Base case: first MOVE in each entity starts batch 0
                    SELECT 
                        plan_op_seq, grouping_key, old_valid_from, old_valid_until, 
                        new_valid_from, new_valid_until, move_seq,
                        0::bigint AS move_batch,
                        datemultirange(daterange(old_valid_from, old_valid_until, '[)')) AS batch_old_ranges
                    FROM moves_ordered
                    WHERE move_seq = 1
                    
                    UNION ALL
                    
                    -- Recursive case: check if new MOVE conflicts with batch's multirange
                    SELECT
                        m.plan_op_seq, m.grouping_key, m.old_valid_from, m.old_valid_until,
                        m.new_valid_from, m.new_valid_until, m.move_seq,
                        CASE 
                            WHEN daterange(m.new_valid_from, m.new_valid_until, '[)') && prev.batch_old_ranges
                            THEN prev.move_batch + 1  -- Conflict: start new batch
                            ELSE prev.move_batch      -- No conflict: same batch
                        END AS move_batch,
                        CASE 
                            WHEN daterange(m.new_valid_from, m.new_valid_until, '[)') && prev.batch_old_ranges
                            THEN datemultirange(daterange(m.old_valid_from, m.old_valid_until, '[)'))  -- Reset multirange
                            ELSE prev.batch_old_ranges + datemultirange(daterange(m.old_valid_from, m.old_valid_until, '[)'))  -- Add to multirange
                        END AS batch_old_ranges
                    FROM moves_ordered m
                    JOIN moves_with_batch prev 
                      ON prev.grouping_key = m.grouping_key 
                     AND prev.move_seq = m.move_seq - 1
                ),
                with_statement_seq AS (
                    SELECT
                        o.plan_op_seq,
                        CASE
                            WHEN o.operation = 'DELETE' THEN 1
                            WHEN o.operation = 'UPDATE' AND o.update_effect IN ('NONE', 'SHRINK') THEN 2
                            WHEN o.operation = 'UPDATE' AND o.update_effect = 'MOVE' THEN
                                -- Base of 3 for first MOVE batch, +1 for each subsequent batch
                                3 + COALESCE(mb.move_batch, 0)
                            WHEN o.operation = 'UPDATE' AND o.update_effect = 'GROW' THEN 1000000  -- Will be renumbered
                            WHEN o.operation = 'INSERT' THEN 2000000  -- Will be renumbered
                            ELSE 3000000
                        END::INT as raw_statement_seq,
                        o.row_ids, o.operation, o.update_effect, o.causal_id::TEXT, o.is_new_entity, o.entity_keys, o.identity_keys, o.lookup_keys, o.s_t_relation, o.b_a_relation, o.old_valid_from::TEXT,
                        o.old_valid_until::TEXT, o.new_valid_from::TEXT, o.new_valid_until::TEXT, o.data, o.feedback, CASE WHEN o.trace IS NOT NULL THEN o.trace || jsonb_build_object('final_grouping_key', o.grouping_key) ELSE NULL END as trace, o.grouping_key
                    FROM ordered_plan o
                    LEFT JOIN moves_with_batch mb ON mb.plan_op_seq = o.plan_op_seq
                )
                SELECT
                    plan_op_seq,
                    dense_rank() OVER (ORDER BY raw_statement_seq)::INT as statement_seq,
                    row_ids, operation, update_effect, causal_id, is_new_entity, entity_keys, identity_keys, lookup_keys, s_t_relation, b_a_relation, old_valid_from,
                    old_valid_until, new_valid_from, new_valid_until,
                    -- New range columns
                    CASE 
                        WHEN old_valid_from IS NOT NULL AND old_valid_until IS NOT NULL 
                        THEN daterange(old_valid_from::date, old_valid_until::date, '[)')::TEXT
                        ELSE NULL 
                    END as old_valid_range,
                    CASE 
                        WHEN new_valid_from IS NOT NULL AND new_valid_until IS NOT NULL 
                        THEN daterange(new_valid_from::date, new_valid_until::date, '[)')::TEXT
                        ELSE NULL 
                    END as new_valid_range,
                    data, feedback, trace, grouping_key
                FROM with_statement_seq
                ORDER BY plan_op_seq;
            
NOTICE:  Sort (actual rows=6.00 loops=1)
NOTICE:    Sort Key: o.plan_op_seq
NOTICE:    Sort Method: quicksort  Memory: 27kB
NOTICE:    Buffers: local hit=1
NOTICE:    CTE ordered_plan
NOTICE:      ->  WindowAgg (actual rows=6.00 loops=1)
NOTICE:            Window: w1 AS (ORDER BY p.grouping_key, p.es_id, p.lu_id, p.type, (CASE p.operation WHEN 'DELETE'::sql_saga.temporal_merge_plan_action THEN 1 WHEN 'UPDATE'::sql_saga.temporal_merge_plan_action THEN 2 WHEN 'INSERT'::sql_saga.temporal_merge_plan_action THEN 3 ELSE 4 END), p.update_effect, (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN 1 ELSE 0 END), (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN COALESCE(p.old_valid_from, p.new_valid_from) ELSE NULL::date END), (COALESCE(p.old_valid_from, p.new_valid_from)), (p.row_ids[1]) ROWS UNBOUNDED PRECEDING)
NOTICE:            Storage: Memory  Maximum Storage: 17kB
NOTICE:            Buffers: local hit=1
NOTICE:            ->  Sort (actual rows=6.00 loops=1)
NOTICE:                  Sort Key: p.grouping_key, p.es_id, p.lu_id, p.type, (CASE p.operation WHEN 'DELETE'::sql_saga.temporal_merge_plan_action THEN 1 WHEN 'UPDATE'::sql_saga.temporal_merge_plan_action THEN 2 WHEN 'INSERT'::sql_saga.temporal_merge_plan_action THEN 3 ELSE 4 END), p.update_effect NULLS FIRST, (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN 1 ELSE 0 END), (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN COALESCE(p.old_valid_from, p.new_valid_from) ELSE NULL::date END) DESC NULLS LAST, (COALESCE(p.old_valid_from, p.new_valid_from)), (p.row_ids[1])
NOTICE:                  Sort Method: quicksort  Memory: 26kB
NOTICE:                  Buffers: local hit=1
NOTICE:                  ->  Seq Scan on plan p (actual rows=6.00 loops=1)
NOTICE:                        Buffers: local hit=1
NOTICE:    CTE moves_ordered
NOTICE:      ->  WindowAgg (actual rows=0.00 loops=1)
NOTICE:            Window: w1 AS (PARTITION BY o_1.grouping_key ORDER BY o_1.plan_op_seq ROWS UNBOUNDED PRECEDING)
NOTICE:            ->  Incremental Sort (actual rows=0.00 loops=1)
NOTICE:                  Sort Key: o_1.grouping_key, o_1.plan_op_seq
NOTICE:                  Presorted Key: o_1.grouping_key
NOTICE:                  Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
NOTICE:                  ->  CTE Scan on ordered_plan o_1 (actual rows=0.00 loops=1)
NOTICE:                        Filter: ((operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect))
NOTICE:                        Rows Removed by Filter: 6
NOTICE:                        Storage: Memory  Maximum Storage: 18kB
NOTICE:    CTE moves_with_batch
NOTICE:      ->  Recursive Union (actual rows=0.00 loops=1)
NOTICE:            Storage: Memory  Maximum Storage: 33kB
NOTICE:            ->  CTE Scan on moves_ordered (actual rows=0.00 loops=1)
NOTICE:                  Filter: (move_seq = 1)
NOTICE:                  Storage: Memory  Maximum Storage: 17kB
NOTICE:            ->  Hash Join (actual rows=0.00 loops=1)
NOTICE:                  Hash Cond: ((prev.grouping_key = m.grouping_key) AND (prev.move_seq = (m.move_seq - 1)))
NOTICE:                  ->  WorkTable Scan on moves_with_batch prev (actual rows=0.00 loops=1)
NOTICE:                  ->  Hash (never executed)
NOTICE:                        ->  CTE Scan on moves_ordered m (never executed)
NOTICE:                              Storage: Memory  Maximum Storage: 17kB
NOTICE:    ->  WindowAgg (actual rows=6.00 loops=1)
NOTICE:          Window: w1 AS (ORDER BY ((CASE WHEN (o.operation = 'DELETE'::sql_saga.temporal_merge_plan_action) THEN '1'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = ANY ('{NONE,SHRINK}'::sql_saga.temporal_merge_update_effect[]))) THEN '2'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect)) THEN (3 + COALESCE(mb.move_batch, '0'::bigint)) WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'GROW'::sql_saga.temporal_merge_update_effect)) THEN '1000000'::bigint WHEN (o.operation = 'INSERT'::sql_saga.temporal_merge_plan_action) THEN '2000000'::bigint ELSE '3000000'::bigint END)::integer) ROWS UNBOUNDED PRECEDING)
NOTICE:          Storage: Memory  Maximum Storage: 17kB
NOTICE:          Buffers: local hit=1
NOTICE:          ->  Sort (actual rows=6.00 loops=1)
NOTICE:                Sort Key: ((CASE WHEN (o.operation = 'DELETE'::sql_saga.temporal_merge_plan_action) THEN '1'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = ANY ('{NONE,SHRINK}'::sql_saga.temporal_merge_update_effect[]))) THEN '2'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect)) THEN (3 + COALESCE(mb.move_batch, '0'::bigint)) WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'GROW'::sql_saga.temporal_merge_update_effect)) THEN '1000000'::bigint WHEN (o.operation = 'INSERT'::sql_saga.temporal_merge_plan_action) THEN '2000000'::bigint ELSE '3000000'::bigint END)::integer)
NOTICE:                Sort Method: quicksort  Memory: 26kB
NOTICE:                Buffers: local hit=1
NOTICE:                ->  Hash Left Join (actual rows=6.00 loops=1)
NOTICE:                      Hash Cond: (o.plan_op_seq = mb.plan_op_seq)
NOTICE:                      Buffers: local hit=1
NOTICE:                      ->  CTE Scan on ordered_plan o (actual rows=6.00 loops=1)
NOTICE:                            Storage: Memory  Maximum Storage: 18kB
NOTICE:                            Buffers: local hit=1
NOTICE:                      ->  Hash (actual rows=0.00 loops=1)
NOTICE:                            Buckets: 1024  Batches: 1  Memory Usage: 8kB
NOTICE:                            ->  CTE Scan on moves_with_batch mb (actual rows=0.00 loops=1)
NOTICE:                                  Storage: Memory  Maximum Storage: 17kB
NOTICE:  Planning:
NOTICE:    Buffers: shared hit=44
\echo '--- OK: Verified that EXPLAIN plan for natural key is optimal. ---'
--- OK: Verified that EXPLAIN plan for natural key is optimal. ---
ROLLBACK TO SAVEPOINT s3;
SET client_min_messages TO NOTICE;
ROLLBACK;
\i sql/include/test_teardown.sql
--
-- test_teardown.sql
--
-- Common teardown for regression tests. This script drops the unprivileged
-- user role created by test_setup.sql.
--
-- It is important to reset the role first, in case a test fails and
-- leaves the session role set to the user that is about to be dropped.
RESET ROLE;
-- Drop the extensions to ensure a clean state for the next test.
-- Use CASCADE to remove any dependent objects created by sql_saga.
DROP EXTENSION IF EXISTS sql_saga CASCADE;
DROP EXTENSION IF EXISTS btree_gist CASCADE;
-- Revoke any privileges held by the test user and drop any objects they own.
-- This is necessary before the role can be dropped.
DROP OWNED BY sql_saga_unprivileged_user;
DROP ROLE IF EXISTS sql_saga_unprivileged_user;
