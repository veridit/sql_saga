-- Scaling Analysis Benchmark
-- Tests O(n) linear scaling behavior of temporal_merge
--
-- This benchmark detects performance regressions that cause superlinear scaling.
-- Uses mathematical complexity analysis based on time ratios when doubling entities.
-- 
-- Mathematical basis (when entities double from n to 2n):
--   - O(n):      time doubles      → time_ratio ≈ 2.0
--   - O(n log n): time ratio = 2 × log(2n)/log(n) ≈ 2.0-2.3
--   - O(n²):     time quadruples   → time_ratio ≈ 4.0
--
-- Classification thresholds:
--   - O(n):      time_ratio < 2.5  (allowing for noise)
--   - O(n log n): time_ratio 2.5-3.0
--   - O(n²):     time_ratio > 3.0  (DEGRADING - regression!)
--
-- Key insight: We measure TOTAL time ratio, not per-entity efficiency.
-- Per-entity efficiency can mask O(n²) when batches are fixed size.
--
-- Prerequisites:
--   - max_locks_per_transaction >= 256 (for 128K entities / 64 batches)
--   - Default PostgreSQL is 64, which limits to ~32K entities
\i sql/include/test_setup.sql
--
-- test_setup.sql
--
-- Common setup for regression tests that need to be self-contained.
-- This script creates the extension, a user role, and grants permissions.
--
SET datestyle = 'ISO, YMD';
CREATE EXTENSION IF NOT EXISTS btree_gist;
CREATE EXTENSION IF NOT EXISTS sql_saga CASCADE;
DO $$
BEGIN
    CREATE ROLE sql_saga_unprivileged_user;
EXCEPTION WHEN duplicate_object THEN
END
$$;
GRANT USAGE ON SCHEMA sql_saga TO sql_saga_unprivileged_user;
GRANT SELECT ON ALL TABLES IN SCHEMA sql_saga TO sql_saga_unprivileged_user;
GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA sql_saga TO sql_saga_unprivileged_user;
/*
 * Allow the unprivileged user to create tables in the public schema.
 * This is required for tests that create their own tables.
 * PG 15+ restricts this by default.
 */
GRANT CREATE ON SCHEMA public TO PUBLIC;
\i sql/include/benchmark_setup.sql
\set ECHO none
--------------------------------------------------------------------------------
-- SETUP: Create schema as superuser, then switch to unprivileged user
--------------------------------------------------------------------------------
DROP SCHEMA IF EXISTS scaling_bench CASCADE;
NOTICE:  schema "scaling_bench" does not exist, skipping
CREATE SCHEMA scaling_bench;
GRANT ALL ON SCHEMA scaling_bench TO sql_saga_unprivileged_user;
SET ROLE TO sql_saga_unprivileged_user;
-- Target table: temporal legal units
CREATE TABLE scaling_bench.legal_unit (
    id serial, 
    name text, 
    comment text, 
    valid_range daterange, 
    valid_from date, 
    valid_until date
);
SELECT sql_saga.add_era('scaling_bench.legal_unit', 'valid_range',
    valid_from_column_name => 'valid_from',
    valid_until_column_name => 'valid_until');
NOTICE:  sql_saga: Created trigger "legal_unit_synchronize_temporal_columns_trigger" on table scaling_bench.legal_unit to synchronize columns: valid_from, valid_until
NOTICE:  sql_saga: Created GIST index "legal_unit_valid_range_gist_idx" on column scaling_bench.legal_unit.valid_range for temporal_merge performance
 add_era 
---------
 t
(1 row)

SELECT sql_saga.add_unique_key(
    table_oid => 'scaling_bench.legal_unit'::regclass, 
    column_names => ARRAY['id'], 
    key_type => 'primary', 
    unique_key_name => 'scaling_bench_legal_unit_id_valid'
);
NOTICE:  sql_saga: Added constraints to table scaling_bench.legal_unit: ALTER COLUMN id SET NOT NULL; ADD PRIMARY KEY (id, valid_range WITHOUT OVERLAPS)
          add_unique_key           
-----------------------------------
 scaling_bench_legal_unit_id_valid
(1 row)

CREATE INDEX ON scaling_bench.legal_unit USING GIST (valid_range) WITH (fillfactor = 90);
-- Source staging table
-- NOTE: Includes generated valid_range column with GIST index, as callers should
-- prepare their staging tables for optimal temporal_merge performance.
CREATE TABLE scaling_bench.data_table (
    row_id serial primary key,
    batch int not null,
    identity_correlation int not null,
    legal_unit_id int,
    merge_statuses jsonb,
    merge_errors jsonb,
    comment text,
    lu_name text,
    valid_from date not null,
    valid_until date not null,
    valid_range daterange GENERATED ALWAYS AS (daterange(valid_from, valid_until)) STORED
);
CREATE INDEX ON scaling_bench.data_table (batch, identity_correlation);
CREATE INDEX ON scaling_bench.data_table (identity_correlation, batch, row_id);
CREATE INDEX ON scaling_bench.data_table USING GIST (valid_range);
-- Identity resolution tracking
CREATE TABLE scaling_bench.identity_resolution (
    identity_correlation int PRIMARY KEY,
    legal_unit_id int,
    resolved_batch int NOT NULL
);
--------------------------------------------------------------------------------
-- HELPER: Data generator with configurable batch size
--------------------------------------------------------------------------------
CREATE OR REPLACE FUNCTION scaling_bench.generate_test_data(
    p_total_entities int,
    p_entities_per_batch int
) RETURNS int LANGUAGE plpgsql AS $function$
DECLARE
    v_num_batches int;
BEGIN
    v_num_batches := CEIL(p_total_entities::numeric / p_entities_per_batch);
    
    TRUNCATE scaling_bench.data_table, scaling_bench.identity_resolution;
    
    INSERT INTO scaling_bench.data_table (
        row_id, batch, identity_correlation, comment, lu_name, valid_from, valid_until
    )
    SELECT 
        row_number() OVER () as row_id,
        CEIL(entity_num::numeric / p_entities_per_batch) as batch,
        entity_num as identity_correlation,
        format('Entity %s', entity_num) as comment,
        format('Company-%s', entity_num) as lu_name,
        ('2024-01-01'::date + ((CEIL(entity_num::numeric / p_entities_per_batch) - 1) * 30)::int) as valid_from,
        'infinity'::date as valid_until
    FROM generate_series(1, p_total_entities) as entity_num;
    
    ANALYZE scaling_bench.data_table;
    RETURN v_num_batches;
END;
$function$;
--------------------------------------------------------------------------------
-- HELPER: Process a single batch through temporal_merge
--------------------------------------------------------------------------------
CREATE OR REPLACE PROCEDURE scaling_bench.process_batch(p_batch_id int)
LANGUAGE plpgsql AS $procedure$
BEGIN
    EXECUTE format($$
        CREATE OR REPLACE TEMP VIEW source_view_lu AS
        SELECT
            row_id,
            identity_correlation as founding_id,
            legal_unit_id AS id,
            lu_name AS name,
            comment,
            merge_statuses,
            merge_errors,
            valid_from,
            valid_until,
            valid_range
        FROM scaling_bench.data_table WHERE batch = %1$L AND lu_name IS NOT NULL;
    $$, p_batch_id);

    CALL sql_saga.temporal_merge(
        target_table => 'scaling_bench.legal_unit',
        source_table => 'source_view_lu',
        primary_identity_columns => ARRAY['id'],
        ephemeral_columns => ARRAY['comment'],
        mode => 'MERGE_ENTITY_PATCH',
        founding_id_column => 'founding_id',
        update_source_with_identity => true,
        update_source_with_feedback => false
    );

    -- Track identity resolution across batches
    INSERT INTO scaling_bench.identity_resolution (identity_correlation, legal_unit_id, resolved_batch)
    SELECT DISTINCT identity_correlation, legal_unit_id, p_batch_id
    FROM scaling_bench.data_table
    WHERE batch = p_batch_id AND legal_unit_id IS NOT NULL
    ON CONFLICT (identity_correlation) DO UPDATE SET
        legal_unit_id = EXCLUDED.legal_unit_id,
        resolved_batch = EXCLUDED.resolved_batch;

    -- Propagate resolved IDs to future batches
    UPDATE scaling_bench.data_table AS dt
    SET legal_unit_id = ir.legal_unit_id
    FROM scaling_bench.identity_resolution AS ir
    WHERE dt.identity_correlation = ir.identity_correlation
      AND dt.legal_unit_id IS NULL;
END;
$procedure$;
--------------------------------------------------------------------------------
\echo ''

\echo '================================================================================'
================================================================================
\echo 'SCALING BENCHMARK: O(n) Linear Scaling Analysis'
SCALING BENCHMARK: O(n) Linear Scaling Analysis
\echo '================================================================================'
================================================================================
\echo ''

\echo 'Testing temporal_merge with fixed batch size (2000 entities/batch).'
Testing temporal_merge with fixed batch size (2000 entities/batch).
\echo 'Doubling total entities doubles batch count - time should scale linearly.'
Doubling total entities doubles batch count - time should scale linearly.
\echo ''

--------------------------------------------------------------------------------
-- Results collection table
CREATE TEMP TABLE scaling_results (
    entities int,
    batch_size int,
    num_batches int,
    batch1_ms numeric,
    avg_batch_ms numeric,
    total_ms numeric,
    ms_per_entity numeric
);
-- Run scaling tests using a procedure with COMMIT between batches.
-- This simulates production usage where each batch runs in a separate transaction,
-- avoiding the artificial O(n²) overhead that occurs when running many batches
-- in a single DO block (due to PL/pgSQL context accumulation).
--
-- Scale sizes chosen to double each step for clear ratio analysis.
-- 64K requires max_locks_per_transaction >= 256.
CREATE OR REPLACE PROCEDURE scaling_bench.run_all_scales()
LANGUAGE plpgsql AS $procedure$
DECLARE
    v_entities int;
    v_batch_size int := 2000;  -- Fixed batch size for consistent comparison
    v_num_batches int;
    v_batch_id int;
    v_start timestamptz;
    v_batch_start timestamptz;
    v_batch1_ms numeric;
    v_batch_ms numeric;
    v_total_batch_ms numeric;
    v_total_ms numeric;
    v_scale_sizes int[] := ARRAY[2000, 4000, 8000, 16000, 32000, 64000, 128000];
    v_max_locks int;
BEGIN
    -- Check max_locks_per_transaction for guidance
    SELECT setting::int INTO v_max_locks 
    FROM pg_settings WHERE name = 'max_locks_per_transaction';
    
    IF v_max_locks < 256 THEN
        RAISE NOTICE 'NOTE: max_locks_per_transaction = %. For 128K entities, increase to 512.', v_max_locks;
    END IF;

    FOREACH v_entities IN ARRAY v_scale_sizes LOOP
        -- Reset tables
        TRUNCATE scaling_bench.data_table, scaling_bench.identity_resolution RESTART IDENTITY CASCADE;
        DELETE FROM scaling_bench.legal_unit;
        ALTER SEQUENCE scaling_bench.legal_unit_id_seq RESTART WITH 1;
        
        -- Generate data
        v_num_batches := scaling_bench.generate_test_data(v_entities, v_batch_size);
        
        ANALYZE scaling_bench.data_table;
        ANALYZE scaling_bench.legal_unit;
        
        -- COMMIT to start fresh for timing (critical for accurate measurement)
        COMMIT;
        
        v_start := clock_timestamp();
        v_total_batch_ms := 0;
        
        -- Process all batches with COMMIT between each (simulates production usage)
        FOR v_batch_id IN 1..v_num_batches LOOP
            v_batch_start := clock_timestamp();
            CALL scaling_bench.process_batch(v_batch_id);
            
            -- COMMIT after each batch - this is critical!
            -- Without this, PL/pgSQL context accumulates causing artificial O(n²) behavior.
            COMMIT;
            
            v_batch_ms := EXTRACT(EPOCH FROM clock_timestamp() - v_batch_start) * 1000;
            v_total_batch_ms := v_total_batch_ms + v_batch_ms;
            
            IF v_batch_id = 1 THEN
                v_batch1_ms := v_batch_ms;
            END IF;
        END LOOP;
        
        v_total_ms := EXTRACT(EPOCH FROM clock_timestamp() - v_start) * 1000;
        
        INSERT INTO scaling_results VALUES (
            v_entities, 
            v_batch_size, 
            v_num_batches, 
            v_batch1_ms, 
            v_total_batch_ms / v_num_batches,
            v_total_ms,
            v_total_ms / v_entities
        );
        COMMIT;
    END LOOP;
    
    RAISE NOTICE 'Scaling benchmark completed. See results tables below.';
END;
$procedure$;
CALL scaling_bench.run_all_scales();
NOTICE:  Scaling benchmark completed. See results tables below.
--------------------------------------------------------------------------------
\echo ''

\echo '================================================================================'
================================================================================
\echo 'SCALING ANALYSIS: Complexity Detection (Deterministic Output)'
SCALING ANALYSIS: Complexity Detection (Deterministic Output)
\echo '================================================================================'
================================================================================
\echo ''

\echo 'Mathematical complexity analysis based on time ratios when doubling entities.'
Mathematical complexity analysis based on time ratios when doubling entities.
\echo 'When entities double (n -> 2n):'
When entities double (n -> 2n):
\echo '  O(n):      time_ratio ~ 2.0  (time doubles)'
  O(n):      time_ratio ~ 2.0  (time doubles)
\echo '  O(n log n): time_ratio ~ 2.0-2.3'
  O(n log n): time_ratio ~ 2.0-2.3
\echo '  O(n^2):    time_ratio ~ 4.0  (time quadruples)'
  O(n^2):    time_ratio ~ 4.0  (time quadruples)
\echo ''

\echo 'Classifications:'
Classifications:
\echo '  LINEAR     = O(n) scaling (time_ratio < 2.6)'
  LINEAR     = O(n) scaling (time_ratio < 2.6)
\echo '  LOGLINEAR  = O(n log n) scaling (time_ratio 2.6-3.5)'
  LOGLINEAR  = O(n log n) scaling (time_ratio 2.6-3.5)
\echo '  QUADRATIC  = O(n^2) scaling detected - REGRESSION! (time_ratio > 3.5)'
  QUADRATIC  = O(n^2) scaling detected - REGRESSION! (time_ratio > 3.5)
\echo ''

\echo 'See expected/performance/111_benchmark_scaling_analysis.perf for timing details.'
See expected/performance/111_benchmark_scaling_analysis.perf for timing details.
\echo ''

--------------------------------------------------------------------------------
-- Create a view for complexity classification based on time ratios
-- NOTE: At very large scales (64K+), some superlinear behavior is expected due to:
--   - Memory pressure and cache effects
--   - PostgreSQL planner estimates becoming less accurate  
--   - Lock contention with many batches
-- The key metric is that scaling should not reach O(n^2) (ratio ~4.0) at any scale.
CREATE TEMP VIEW scaling_analysis AS
WITH analysis AS (
    SELECT 
        entities,
        num_batches,
        total_ms,
        ms_per_entity,
        LAG(entities) OVER (ORDER BY entities) as prev_entities,
        LAG(total_ms) OVER (ORDER BY entities) as prev_total_ms
    FROM scaling_results
)
SELECT 
    prev_entities,
    entities,
    total_ms,
    total_ms / NULLIF(prev_total_ms, 0) as time_ratio,
    CASE 
        -- O(n): time_ratio close to 2.0 (allow up to 2.6 for noise)
        WHEN total_ms / NULLIF(prev_total_ms, 0) < 2.6 THEN 'LINEAR'
        -- O(n log n): time_ratio between 2.6 and 3.5
        WHEN total_ms / NULLIF(prev_total_ms, 0) < 3.5 THEN 'LOGLINEAR'
        -- O(n^2): time_ratio approaching 4.0 or higher
        ELSE 'QUADRATIC'
    END as complexity,
    CASE 
        WHEN total_ms / NULLIF(prev_total_ms, 0) >= 3.5 THEN 'REGRESSION!'
        ELSE 'OK'
    END as status
FROM analysis
WHERE prev_entities IS NOT NULL;
-- Output ONLY the deterministic complexity classification
-- (timing values vary by machine/run and go to .perf file)
SELECT 
    prev_entities || ' -> ' || entities as scale,
    complexity,
    status
FROM scaling_analysis
ORDER BY prev_entities;
      scale      | complexity | status 
-----------------+------------+--------
 2000 -> 4000    | LINEAR     | OK
 4000 -> 8000    | LINEAR     | OK
 8000 -> 16000   | LINEAR     | OK
 16000 -> 32000  | LINEAR     | OK
 32000 -> 64000  | LINEAR     | OK
 64000 -> 128000 | LINEAR     | OK
(6 rows)

\echo ''

\echo 'If any row shows QUADRATIC/REGRESSION!, investigate immediately!'
If any row shows QUADRATIC/REGRESSION!, investigate immediately!
\echo 'Common causes: LATERAL joins, = ANY(array), OR IS NULL patterns'
Common causes: LATERAL joins, = ANY(array), OR IS NULL patterns
\echo ''

--------------------------------------------------------------------------------
-- Write timing details to performance file (variable between runs)
--------------------------------------------------------------------------------
\set perf_file expected/performance/111_benchmark_scaling_analysis.perf
\pset tuples_only on
\pset footer off
\o :perf_file
SELECT '# Performance Baseline: sql_saga scaling analysis';
SELECT '# These numbers are reference baselines, not test assertions.';
SELECT '# They are updated after significant performance changes.';
SELECT '#';
SELECT '';
\pset tuples_only off
SELECT '# Raw timing data:' as "header";
SELECT 
    entities,
    num_batches,
    ROUND(batch1_ms)::int as batch1_ms,
    ROUND(avg_batch_ms)::int as avg_batch_ms,
    ROUND(total_ms)::int as total_ms,
    ROUND(ms_per_entity, 3) as ms_per_entity,
    ROUND(1000.0 / ms_per_entity)::int as entities_per_sec
FROM scaling_results
ORDER BY entities;
\pset tuples_only on
SELECT '';
\pset tuples_only off
SELECT '# Scaling analysis (time_ratio when entities double):' as "header";
SELECT 
    prev_entities || ' -> ' || entities as scale,
    ROUND(time_ratio, 2) as time_ratio,
    complexity,
    status
FROM scaling_analysis
ORDER BY prev_entities;
\o
\pset footer on
--------------------------------------------------------------------------------
-- CLEANUP
--------------------------------------------------------------------------------
RESET ROLE;
DROP SCHEMA scaling_bench CASCADE;
NOTICE:  drop cascades to 7 other objects
DETAIL:  drop cascades to table scaling_bench.legal_unit
drop cascades to table scaling_bench.data_table
drop cascades to view source_view_lu
drop cascades to table scaling_bench.identity_resolution
drop cascades to function scaling_bench.generate_test_data(integer,integer)
drop cascades to function scaling_bench.process_batch(integer)
drop cascades to function scaling_bench.run_all_scales()
\i sql/include/benchmark_teardown.sql
\set ECHO none
\i sql/include/test_teardown.sql
--
-- test_teardown.sql
--
-- Common teardown for regression tests. This script drops the unprivileged
-- user role created by test_setup.sql.
--
-- It is important to reset the role first, in case a test fails and
-- leaves the session role set to the user that is about to be dropped.
RESET ROLE;
-- Drop the extensions to ensure a clean state for the next test.
-- Use CASCADE to remove any dependent objects created by sql_saga.
DROP EXTENSION IF EXISTS sql_saga CASCADE;
DROP EXTENSION IF EXISTS btree_gist CASCADE;
-- Revoke any privileges held by the test user and drop any objects they own.
-- This is necessary before the role can be dropped.
DROP OWNED BY sql_saga_unprivileged_user;
DROP ROLE IF EXISTS sql_saga_unprivileged_user;
