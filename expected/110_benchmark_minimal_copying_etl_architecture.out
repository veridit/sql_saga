\i sql/include/test_setup.sql
--
-- test_setup.sql
--
-- Common setup for regression tests that need to be self-contained.
-- This script creates the extension, a user role, and grants permissions.
--
SET datestyle = 'ISO, YMD';
CREATE EXTENSION IF NOT EXISTS btree_gist;
CREATE EXTENSION IF NOT EXISTS sql_saga CASCADE;
DO $$
BEGIN
    CREATE ROLE sql_saga_unprivileged_user;
EXCEPTION WHEN duplicate_object THEN
END
$$;
GRANT USAGE ON SCHEMA sql_saga TO sql_saga_unprivileged_user;
GRANT SELECT ON ALL TABLES IN SCHEMA sql_saga TO sql_saga_unprivileged_user;
GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA sql_saga TO sql_saga_unprivileged_user;
/*
 * Allow the unprivileged user to create tables in the public schema.
 * This is required for tests that create their own tables.
 * PG 15+ restricts this by default.
 */
GRANT CREATE ON SCHEMA public TO PUBLIC;
\i sql/include/benchmark_setup.sql
\set ECHO none
-- Create schema as superuser before switching roles
CREATE SCHEMA etl_bench;
GRANT ALL ON SCHEMA etl_bench TO sql_saga_unprivileged_user;
SET ROLE TO sql_saga_unprivileged_user;
SELECT $$
--------------------------------------------------------------------------------
BENCHMARK: Minimal-Copying ETL Architecture - Production Scale Performance
--------------------------------------------------------------------------------
This benchmark measures the performance of the advanced "merge -> back-propagate -> merge" 
ETL pattern using updatable views and optimized back-propagation.

Focus: Production-scale volumes with optimized O(n*log(n)) algorithms
Pattern tested: legal_unit -> location -> stat_for_unit -> activity
Key metric: rows/second throughput for each operation

Architecture: Forward-propagation with materialized identity resolution
Purpose: Measure ETL performance at production scale with optimizations
--------------------------------------------------------------------------------
$$ as doc;
                                             doc                                             
---------------------------------------------------------------------------------------------
                                                                                            +
 --------------------------------------------------------------------------------           +
 BENCHMARK: Minimal-Copying ETL Architecture - Production Scale Performance                 +
 --------------------------------------------------------------------------------           +
 This benchmark measures the performance of the advanced "merge -> back-propagate -> merge" +
 ETL pattern using updatable views and optimized back-propagation.                          +
                                                                                            +
 Focus: Production-scale volumes with optimized O(n*log(n)) algorithms                      +
 Pattern tested: legal_unit -> location -> stat_for_unit -> activity                        +
 Key metric: rows/second throughput for each operation                                      +
                                                                                            +
 Architecture: Forward-propagation with materialized identity resolution                    +
 Purpose: Measure ETL performance at production scale with optimizations                    +
 --------------------------------------------------------------------------------           +
 
(1 row)

-- Setup ETL schema
CREATE TABLE etl_bench.stat_definition (id int primary key, code text unique);
INSERT INTO etl_bench.stat_definition VALUES (1, 'employees'), (2, 'turnover');
CREATE TABLE etl_bench.ident_type (id int primary key, code text unique);
INSERT INTO etl_bench.ident_type VALUES (1, 'tax'), (2, 'ssn');
CREATE TABLE etl_bench.activity_type (id int primary key, code text unique);
INSERT INTO etl_bench.activity_type VALUES (10, 'manufacturing'), (20, 'retail');
-- Legal Unit table (parent entity)
CREATE TABLE etl_bench.legal_unit (id serial, name text, comment text, valid_range daterange, valid_from date, valid_until date);
SELECT sql_saga.add_era('etl_bench.legal_unit', 'valid_range',
    valid_from_column_name => 'valid_from',
    valid_until_column_name => 'valid_until');
NOTICE:  sql_saga: Created trigger "legal_unit_synchronize_temporal_columns_trigger" on table etl_bench.legal_unit to synchronize columns: valid_from, valid_until
 add_era 
---------
 t
(1 row)

SELECT sql_saga.add_unique_key(table_oid => 'etl_bench.legal_unit'::regclass, column_names => ARRAY['id'], key_type => 'primary', unique_key_name => 'legal_unit_id_valid');
NOTICE:  sql_saga: Added constraints to table etl_bench.legal_unit: ALTER COLUMN id SET NOT NULL; ADD PRIMARY KEY (id, valid_range WITHOUT OVERLAPS)
   add_unique_key    
---------------------
 legal_unit_id_valid
(1 row)

-- Location table (dependent entity)
CREATE TABLE etl_bench.location (id serial, legal_unit_id int, type text, address text, comment text, valid_range daterange, valid_from date, valid_until date);
SELECT sql_saga.add_era('etl_bench.location', 'valid_range',
    valid_from_column_name => 'valid_from',
    valid_until_column_name => 'valid_until');
NOTICE:  sql_saga: Created trigger "location_synchronize_temporal_columns_trigger" on table etl_bench.location to synchronize columns: valid_from, valid_until
 add_era 
---------
 t
(1 row)

SELECT sql_saga.add_unique_key(table_oid => 'etl_bench.location'::regclass, column_names => ARRAY['id'], key_type => 'primary', unique_key_name => 'location_id_valid');
NOTICE:  sql_saga: Added constraints to table etl_bench.location: ALTER COLUMN id SET NOT NULL; ADD PRIMARY KEY (id, valid_range WITHOUT OVERLAPS)
  add_unique_key   
-------------------
 location_id_valid
(1 row)

SELECT sql_saga.add_unique_key(table_oid => 'etl_bench.location'::regclass, column_names => ARRAY['legal_unit_id', 'type'], key_type => 'natural', unique_key_name => 'location_natural_valid');
NOTICE:  sql_saga: Added constraints to table etl_bench.location: ADD CONSTRAINT location_natural_valid_uniq UNIQUE (legal_unit_id, type, valid_range WITHOUT OVERLAPS); ADD CONSTRAINT location_natural_valid_pk_consistency_excl EXCLUDE USING gist (legal_unit_id WITH =, type WITH =, id WITH <>)
     add_unique_key     
------------------------
 location_natural_valid
(1 row)

SELECT sql_saga.add_foreign_key(
    fk_table_oid => 'etl_bench.location'::regclass,
    fk_column_names => ARRAY['legal_unit_id'],
    pk_table_oid => 'etl_bench.legal_unit'::regclass,
    pk_column_names => ARRAY['id']
);
       add_foreign_key        
------------------------------
 location_legal_unit_id_valid
(1 row)

-- Statistics table (dependent entity)
CREATE TABLE etl_bench.stat_for_unit (legal_unit_id int, stat_definition_id int references etl_bench.stat_definition(id), value numeric, comment text, valid_range daterange, valid_from date, valid_until date);
SELECT sql_saga.add_era('etl_bench.stat_for_unit', 'valid_range',
    valid_from_column_name => 'valid_from',
    valid_until_column_name => 'valid_until');
NOTICE:  sql_saga: Created trigger "stat_for_unit_synchronize_temporal_columns_trigger" on table etl_bench.stat_for_unit to synchronize columns: valid_from, valid_until
 add_era 
---------
 t
(1 row)

SELECT sql_saga.add_unique_key(table_oid => 'etl_bench.stat_for_unit'::regclass, column_names => ARRAY['legal_unit_id', 'stat_definition_id'], key_type => 'natural', unique_key_name => 'stat_for_unit_natural_valid');
NOTICE:  sql_saga: Added constraints to table etl_bench.stat_for_unit: ADD CONSTRAINT stat_for_unit_natural_valid_uniq UNIQUE (legal_unit_id, stat_definition_id, valid_range WITHOUT OVERLAPS)
       add_unique_key        
-----------------------------
 stat_for_unit_natural_valid
(1 row)

SELECT sql_saga.add_foreign_key(
    fk_table_oid => 'etl_bench.stat_for_unit'::regclass,
    fk_column_names => ARRAY['legal_unit_id'],
    pk_table_oid => 'etl_bench.legal_unit'::regclass,
    pk_column_names => ARRAY['id']
);
          add_foreign_key          
-----------------------------------
 stat_for_unit_legal_unit_id_valid
(1 row)

-- Activity table (temporal with natural keys)
CREATE TABLE etl_bench.activity (legal_unit_id int, activity_type_id int references etl_bench.activity_type(id), comment text, valid_range daterange, valid_from date, valid_until date);
SELECT sql_saga.add_era('etl_bench.activity', 'valid_range',
    valid_from_column_name => 'valid_from',
    valid_until_column_name => 'valid_until');
NOTICE:  sql_saga: Created trigger "activity_synchronize_temporal_columns_trigger" on table etl_bench.activity to synchronize columns: valid_from, valid_until
 add_era 
---------
 t
(1 row)

SELECT sql_saga.add_unique_key(table_oid => 'etl_bench.activity'::regclass, column_names => ARRAY['legal_unit_id', 'activity_type_id'], key_type => 'natural', unique_key_name => 'activity_natural_valid');
NOTICE:  sql_saga: Added constraints to table etl_bench.activity: ADD CONSTRAINT activity_natural_valid_uniq UNIQUE (legal_unit_id, activity_type_id, valid_range WITHOUT OVERLAPS)
     add_unique_key     
------------------------
 activity_natural_valid
(1 row)

SELECT sql_saga.add_foreign_key(
    fk_table_oid => 'etl_bench.activity'::regclass,
    fk_column_names => ARRAY['legal_unit_id'],
    pk_table_oid => 'etl_bench.legal_unit'::regclass,
    pk_column_names => ARRAY['id']
);
       add_foreign_key        
------------------------------
 activity_legal_unit_id_valid
(1 row)

-- Performance-critical indexes (Agent 2B strategy)
CREATE INDEX ON etl_bench.legal_unit USING GIST (valid_range) WITH (fillfactor = 90);
CREATE INDEX ON etl_bench.location USING GIST (valid_range) WITH (fillfactor = 90);
CREATE INDEX ON etl_bench.stat_for_unit USING GIST (valid_range) WITH (fillfactor = 90);
CREATE INDEX ON etl_bench.activity USING GIST (valid_range) WITH (fillfactor = 90);
-- Master ETL data table
CREATE TABLE etl_bench.data_table (
    row_id serial primary key,
    batch int not null,
    identity_correlation int not null,
    legal_unit_id int,
    location_id int,
    merge_statuses jsonb,
    merge_errors jsonb,
    comment text,
    tax_ident text,
    lu_name text,
    physical_address text,
    postal_address text,
    activity_code text,
    employees numeric,
    turnover numeric,
    valid_from date not null,
    valid_until date not null
);
-- Performance indexes for data table
CREATE INDEX ON etl_bench.data_table (batch, identity_correlation);
CREATE INDEX ON etl_bench.data_table (identity_correlation, batch, row_id);
-- Identity resolution table for optimized back-propagation (Agent 2A algorithm)
CREATE TABLE etl_bench.identity_resolution (
    identity_correlation int PRIMARY KEY,
    legal_unit_id int,
    location_id int, 
    resolved_batch int NOT NULL
);
--------------------------------------------------------------------------------
\echo '--- Optimized ETL Procedures (Agent 2A Forward-Propagation Algorithm) ---'
--- Optimized ETL Procedures (Agent 2A Forward-Propagation Algorithm) ---
--------------------------------------------------------------------------------
-- Fast data generator using bulk operations
CREATE FUNCTION etl_bench.generate_test_data(
    p_total_entities int,
    p_batches_per_entity int DEFAULT 3,
    p_rows_per_entity_per_batch int DEFAULT 5
) RETURNS void LANGUAGE plpgsql AS $function$
DECLARE
    v_start_time timestamptz;
    v_end_time timestamptz;
    v_total_rows bigint;
    v_rows_per_second numeric;
BEGIN
    RAISE NOTICE 'Generating test data: % entities, % total rows', 
        p_total_entities, (p_total_entities * p_batches_per_entity * p_rows_per_entity_per_batch);
    
    v_start_time := clock_timestamp();
    
    -- Clear existing data
    TRUNCATE etl_bench.data_table, etl_bench.identity_resolution;
    
    -- Bulk generate data using set-based operations (O(n) complexity)
    INSERT INTO etl_bench.data_table (
        row_id, batch, identity_correlation, comment,
        tax_ident, lu_name, physical_address, postal_address,
        activity_code, employees, turnover, valid_from, valid_until
    )
    SELECT 
        row_number() OVER () as row_id,
        batch_num,
        entity_num as identity_correlation,
        format('Entity %s batch %s row %s', entity_num, batch_num, row_in_batch_num) as comment,
        CASE WHEN row_in_batch_num = 1 THEN format('TAX%s', entity_num) END as tax_ident,
        CASE WHEN row_in_batch_num = 1 THEN format('Company-%s', entity_num) END as lu_name,
        CASE WHEN row_in_batch_num <= 2 THEN format('%s Main St, City %s', entity_num, entity_num) END as physical_address,
        CASE WHEN row_in_batch_num <= 3 THEN format('PO Box %s', entity_num) END as postal_address,
        CASE WHEN row_in_batch_num <= 4 THEN (CASE WHEN entity_num % 2 = 0 THEN 'manufacturing' ELSE 'retail' END) END as activity_code,
        CASE WHEN row_in_batch_num <= 4 THEN (entity_num % 100) + 10 END as employees,
        CASE WHEN row_in_batch_num <= 5 THEN (entity_num % 1000) * 1000 + 50000 END as turnover,
        ('2024-01-01'::date + ((batch_num - 1) * 90 + (row_in_batch_num - 1) * 30)) as valid_from,
        'infinity'::date as valid_until
    FROM 
        generate_series(1, p_total_entities) as entity_num,
        generate_series(1, p_batches_per_entity) as batch_num,
        generate_series(1, p_rows_per_entity_per_batch) as row_in_batch_num;
    
    GET DIAGNOSTICS v_total_rows = ROW_COUNT;
    
    v_end_time := clock_timestamp();
    v_rows_per_second := v_total_rows / EXTRACT(EPOCH FROM v_end_time - v_start_time);
    
    RAISE NOTICE 'Generated % rows in % (% rows/sec)', 
        v_total_rows, (v_end_time - v_start_time), ROUND(v_rows_per_second);
END;
$function$;
-- Optimized legal units processor with forward-propagation
CREATE PROCEDURE etl_bench.process_legal_units(p_batch_id int)
LANGUAGE plpgsql AS $procedure$
BEGIN
    -- Create updatable view for the batch
    EXECUTE format($$
        CREATE OR REPLACE TEMP VIEW source_view_lu AS
        SELECT
            row_id,
            identity_correlation as founding_id,
            legal_unit_id AS id,
            lu_name AS name,
            comment,
            merge_statuses,
            merge_errors,
            valid_from,
            valid_until
        FROM etl_bench.data_table WHERE batch = %1$L AND lu_name IS NOT NULL;
    $$, p_batch_id);

    -- Temporal merge with back-propagation
    CALL sql_saga.temporal_merge(
        target_table => 'etl_bench.legal_unit',
        source_table => 'source_view_lu',
        primary_identity_columns => ARRAY['id'],
        ephemeral_columns => ARRAY['comment'],
        mode => 'MERGE_ENTITY_PATCH',
        founding_id_column => 'founding_id',
        update_source_with_identity => true,
        update_source_with_feedback => true,
        feedback_status_column => 'merge_statuses',
        feedback_status_key => 'legal_unit',
        feedback_error_column => 'merge_errors',
        feedback_error_key => 'legal_unit'
    );

    -- OPTIMIZED: Forward-propagation using materialized identity resolution
    INSERT INTO etl_bench.identity_resolution (identity_correlation, legal_unit_id, resolved_batch)
    SELECT DISTINCT 
        identity_correlation,
        legal_unit_id,
        p_batch_id
    FROM etl_bench.data_table
    WHERE batch = p_batch_id AND legal_unit_id IS NOT NULL
    ON CONFLICT (identity_correlation) DO UPDATE SET
        legal_unit_id = EXCLUDED.legal_unit_id,
        resolved_batch = EXCLUDED.resolved_batch;

    -- Apply forward-propagation to all relevant rows (O(n*log(n)) complexity)
    UPDATE etl_bench.data_table dt
    SET legal_unit_id = ir.legal_unit_id
    FROM etl_bench.identity_resolution ir
    WHERE dt.identity_correlation = ir.identity_correlation
      AND dt.legal_unit_id IS NULL;
END;
$procedure$;
-- Other ETL procedures (simplified for now - location, stats, activity)
CREATE PROCEDURE etl_bench.process_locations(p_batch_id int)
LANGUAGE plpgsql AS $procedure$
BEGIN
    -- Simplified location processing (can be expanded later)
    NULL;
END;
$procedure$;
CREATE PROCEDURE etl_bench.process_statistics(p_batch_id int)
LANGUAGE plpgsql AS $procedure$
BEGIN
    -- Simplified stats processing (can be expanded later)
    NULL;
END;
$procedure$;
CREATE PROCEDURE etl_bench.process_activities(p_batch_id int)
LANGUAGE plpgsql AS $procedure$
BEGIN
    -- Simplified activity processing (can be expanded later)
    NULL;
END;
$procedure$;
--------------------------------------------------------------------------------
\echo '--- Production Scale Performance Benchmark ---'
--- Production Scale Performance Benchmark ---
--------------------------------------------------------------------------------
DO $$
DECLARE
    v_dataset_size int := 10000; -- Start with 10K for fast iteration
    v_total_batches int;
    v_batch_id int;
    v_start_time timestamptz;
    v_end_time timestamptz;
    v_duration interval;
    v_rows_processed int;
    v_rows_per_second numeric;
    v_total_rows int;
    v_total_start_time timestamptz;
    v_total_duration interval;
BEGIN
    RAISE NOTICE 'STARTING BENCHMARK: % entities (optimized ETL)', v_dataset_size;
    
    v_total_start_time := clock_timestamp();
    
    -- Generate test data
    INSERT INTO benchmark (event, row_count, is_performance_benchmark) 
    VALUES (format('Generate Test Data - %s entities', v_dataset_size), v_dataset_size, true);
    
    v_start_time := clock_timestamp();
    CALL sql_saga.benchmark_reset();
    
    PERFORM etl_bench.generate_test_data(v_dataset_size, 3, 5);
    SELECT COUNT(*) INTO v_total_rows FROM etl_bench.data_table;
    
    v_end_time := clock_timestamp();
    v_duration := v_end_time - v_start_time;
    v_rows_per_second := v_total_rows / EXTRACT(EPOCH FROM v_duration);
    
    INSERT INTO benchmark (event, row_count, is_performance_benchmark) 
    VALUES (format('Data Generation Complete - %s rows/sec', ROUND(v_rows_per_second)), v_total_rows, true);
    CALL sql_saga.benchmark_log_and_reset(format('Generate Test Data - %s entities', v_dataset_size));
    
    -- Calculate total batches to process
    SELECT MAX(batch) INTO v_total_batches FROM etl_bench.data_table;
    
    -- Process each batch and measure performance
    FOR v_batch_id IN 1..v_total_batches LOOP
        
        -- Count rows in this batch
        SELECT COUNT(*) INTO v_rows_processed 
        FROM etl_bench.data_table 
        WHERE batch = v_batch_id;
        
        -- Process Legal Units (main optimization target)
        INSERT INTO benchmark (event, row_count, is_performance_benchmark) 
        VALUES (format('Process Legal Units - Batch %s/%s', v_batch_id, v_total_batches), v_rows_processed, true);
        
        v_start_time := clock_timestamp();
        CALL sql_saga.benchmark_reset();
        
        CALL etl_bench.process_legal_units(v_batch_id);
        
        v_end_time := clock_timestamp();
        v_duration := v_end_time - v_start_time;
        v_rows_per_second := CASE WHEN EXTRACT(EPOCH FROM v_duration) > 0 
                            THEN v_rows_processed / EXTRACT(EPOCH FROM v_duration) 
                            ELSE 0 END;
        
        INSERT INTO benchmark (event, row_count, is_performance_benchmark) 
        VALUES (format('Legal Units Complete - %s rows/sec', ROUND(v_rows_per_second)), v_rows_processed, true);
        CALL sql_saga.benchmark_log_and_reset(format('Process Legal Units - Batch %s', v_batch_id));
        
    END LOOP;
    
    -- Overall performance summary
    v_total_duration := clock_timestamp() - v_total_start_time;
    v_rows_per_second := v_total_rows / EXTRACT(EPOCH FROM v_total_duration);
    
    INSERT INTO benchmark (event, row_count, is_performance_benchmark) 
    VALUES (format('COMPLETE DATASET %s entities - %s rows/sec overall', v_dataset_size, ROUND(v_rows_per_second)), v_total_rows, true);
    
    RAISE NOTICE 'COMPLETED: % entities in % (% rows/sec overall)', v_dataset_size, v_total_duration, ROUND(v_rows_per_second);
    
END;
$$;
NOTICE:  STARTING BENCHMARK: 10000 entities (optimized ETL)
NOTICE:  Generating test data: 10000 entities, 150000 total rows
NOTICE:  Generated 150000 rows in @ 1.010031 secs (148510 rows/sec)
NOTICE:  COMPLETED: 10000 entities in @ 36.189876 secs (4145 rows/sec overall)
--------------------------------------------------------------------------------
\echo '--- Benchmark Results Summary ---'
--- Benchmark Results Summary ---
--------------------------------------------------------------------------------
SELECT 
    event,
    row_count,
    format_duration(timestamp - LAG(timestamp) OVER (ORDER BY seq_id)) as duration,
    CASE 
        WHEN event ~ 'rows/sec' THEN 'THROUGHPUT'
        WHEN event ~ 'Complete' THEN 'RESULT'  
        WHEN event ~ 'Process' THEN 'OPERATION'
        ELSE 'SETUP'
    END as event_type
FROM benchmark 
WHERE is_performance_benchmark = true
ORDER BY seq_id;
                          event                          | row_count | duration | event_type 
---------------------------------------------------------+-----------+----------+------------
 Generate Test Data - 10000 entities                     |     10000 |          | SETUP
 Data Generation Complete - 147564 rows/sec              |    150000 | 1 secs   | THROUGHPUT
 Process Legal Units - Batch 1/3                         |     50000 | 5 ms     | OPERATION
 Legal Units Complete - 1896 rows/sec                    |     50000 | 26 secs  | THROUGHPUT
 Process Legal Units - Batch 2/3                         |     50000 | 10 ms    | OPERATION
 Legal Units Complete - 11376 rows/sec                   |     50000 | 4 secs   | THROUGHPUT
 Process Legal Units - Batch 3/3                         |     50000 | 11 ms    | OPERATION
 Legal Units Complete - 11420 rows/sec                   |     50000 | 4 secs   | THROUGHPUT
 COMPLETE DATASET 10000 entities - 4145 rows/sec overall |    150000 | 2 ms     | THROUGHPUT
(9 rows)

-- Performance monitor data (if available)
\echo '--- Performance Monitor Data (Top Queries by Execution Time) ---'
--- Performance Monitor Data (Top Queries by Execution Time) ---
SELECT 
    event,
    label,
    calls,
    ROUND(total_exec_time::numeric, 2) as exec_time_ms,
    rows,
    CASE WHEN rows > 0 THEN ROUND((rows::numeric / total_exec_time::numeric * 1000), 1) END as rows_per_sec,
    LEFT(query, 100) || '...' as query_preview
FROM benchmark_monitor_log_filtered
WHERE event ~ 'Process|Generate'
ORDER BY total_exec_time DESC
LIMIT 20;
             event             | label | calls | exec_time_ms |  rows  | rows_per_sec |                                              query_preview                                              
-------------------------------+-------+-------+--------------+--------+--------------+---------------------------------------------------------------------------------------------------------
 Process Legal Units - Batch 1 |       |     1 |     26370.63 |      0 |              | CALL etl_bench.process_legal_units(v_batch_id)...
 Process Legal Units - Batch 1 |       |     1 |     24785.46 |      0 |              | CALL sql_saga.temporal_merge(         target_table => 'etl_bench.legal_unit',         source_table =...
 Process Legal Units - Batch 1 |       |     1 |     19712.44 |  10000 |        507.3 | INSERT INTO temporal_merge_plan     SELECT * FROM sql_saga.temporal_merge_plan(         target_table...
 Process Legal Units - Batch 1 |       |     1 |     16646.66 |  10000 |        600.7 | CREATE TEMP TABLE resolved_atomic_segments_with_payloads ON COMMIT DROP AS                 WITH all_...
 Process Legal Units - Batch 1 |       |     1 |     16645.92 |  10000 |        600.7 | CREATE TEMP TABLE resolved_atomic_segments_with_payloads ON COMMIT DROP AS                 WITH all_...
 Process Legal Units - Batch 1 |       |     1 |      5069.57 |      0 |              | CALL sql_saga.temporal_merge_execute(         target_table => temporal_merge.target_table,         s...
 Process Legal Units - Batch 1 |       |     1 |      4550.51 |  10000 |       2197.6 | WITH map_row_to_entity AS (                                     SELECT DISTINCT ON (s.source_row_id)...
 Process Legal Units - Batch 2 |       |     1 |      4395.24 |      0 |              | CALL etl_bench.process_legal_units(v_batch_id)...
 Process Legal Units - Batch 3 |       |     1 |      4378.24 |      0 |              | CALL etl_bench.process_legal_units(v_batch_id)...
 Process Legal Units - Batch 3 |       |     1 |      4292.83 |      0 |              | CALL sql_saga.temporal_merge(         target_table => 'etl_bench.legal_unit',         source_table =...
 Process Legal Units - Batch 2 |       |     1 |      4282.73 |      0 |              | CALL sql_saga.temporal_merge(         target_table => 'etl_bench.legal_unit',         source_table =...
 Process Legal Units - Batch 2 |       |     1 |      3694.56 |  10000 |       2706.7 | INSERT INTO temporal_merge_plan     SELECT * FROM sql_saga.temporal_merge_plan(         target_table...
 Process Legal Units - Batch 3 |       |     1 |      3680.58 |  10000 |       2717.0 | INSERT INTO temporal_merge_plan     SELECT * FROM sql_saga.temporal_merge_plan(         target_table...
 Process Legal Units - Batch 3 |       |     1 |      2491.51 |  10000 |       4013.6 | CREATE TEMP TABLE source_with_eclipsed_flag ON COMMIT DROP AS                 SELECT                ...
 Process Legal Units - Batch 3 |       |     1 |      2490.85 |  10000 |       4014.7 | CREATE TEMP TABLE source_with_eclipsed_flag ON COMMIT DROP AS                 SELECT                ...
 Process Legal Units - Batch 2 |       |     1 |      2488.96 |  10000 |       4017.7 | CREATE TEMP TABLE source_with_eclipsed_flag ON COMMIT DROP AS                 SELECT                ...
 Process Legal Units - Batch 2 |       |     1 |      2488.41 |  10000 |       4018.6 | CREATE TEMP TABLE source_with_eclipsed_flag ON COMMIT DROP AS                 SELECT                ...
 Process Legal Units - Batch 1 |       |     1 |      2462.72 |  10000 |       4060.6 | CREATE TEMP TABLE source_with_eclipsed_flag ON COMMIT DROP AS                 SELECT                ...
 Process Legal Units - Batch 1 |       |     1 |      2462.08 |  10000 |       4061.6 | CREATE TEMP TABLE source_with_eclipsed_flag ON COMMIT DROP AS                 SELECT                ...
 Process Legal Units - Batch 1 |       |     1 |      1554.50 | 140000 |      90060.8 | UPDATE etl_bench.data_table dt     SET legal_unit_id = ir.legal_unit_id     FROM etl_bench.identity_...
(20 rows)

-- Generate performance monitoring files
\echo '-- Monitor log from pg_stat_monitor --'
-- Monitor log from pg_stat_monitor --
\set monitor_log_filename expected/performance/110_benchmark_minimal_copying_etl_architecture_benchmark_monitor.csv
\i sql/include/benchmark_monitor_csv.sql
\set ECHO none
 monitor_log_has_rows 
----------------------
 t
(1 row)

-- Capture performance metrics to a separate file for manual review
\set benchmark_log_filename expected/performance/110_benchmark_minimal_copying_etl_architecture_benchmark_report.log
\i sql/include/benchmark_report_log.sql
\set ECHO none
\i sql/include/test_teardown.sql
--
-- test_teardown.sql
--
-- Common teardown for regression tests. This script drops the unprivileged
-- user role created by test_setup.sql.
--
-- It is important to reset the role first, in case a test fails and
-- leaves the session role set to the user that is about to be dropped.
RESET ROLE;
-- Drop the extensions to ensure a clean state for the next test.
-- Use CASCADE to remove any dependent objects created by sql_saga.
DROP EXTENSION IF EXISTS sql_saga CASCADE;
psql:sql/include/test_teardown.sql:13: NOTICE:  drop cascades to 13 other objects
DETAIL:  drop cascades to function sql_saga.benchmark_reset_pg_stat_monitor()
drop cascades to function sql_saga.benchmark_reset()
drop cascades to function sql_saga.__internal_get_pg_stat_monitor_data()
drop cascades to function sql_saga.benchmark_log_and_reset(text)
drop cascades to function sql_saga.benchmark_teardown()
drop cascades to function sql_saga.etl_bench_legal_unit_valid_template_sync()
drop cascades to trigger legal_unit_valid_sync_temporal_trg on table etl_bench.legal_unit
drop cascades to function sql_saga.etl_bench_location_valid_template_sync()
drop cascades to trigger location_valid_sync_temporal_trg on table etl_bench.location
drop cascades to function sql_saga.etl_bench_stat_for_unit_valid_template_sync()
drop cascades to trigger stat_for_unit_valid_sync_temporal_trg on table etl_bench.stat_for_unit
drop cascades to function sql_saga.etl_bench_activity_valid_template_sync()
drop cascades to trigger activity_valid_sync_temporal_trg on table etl_bench.activity
DROP EXTENSION IF EXISTS btree_gist CASCADE;
psql:sql/include/test_teardown.sql:14: NOTICE:  drop cascades to 9 other objects
DETAIL:  drop cascades to constraint legal_unit_pkey on table etl_bench.legal_unit
drop cascades to constraint location_legal_unit_id_valid on table etl_bench.location
drop cascades to constraint stat_for_unit_legal_unit_id_valid on table etl_bench.stat_for_unit
drop cascades to constraint activity_legal_unit_id_valid on table etl_bench.activity
drop cascades to constraint location_pkey on table etl_bench.location
drop cascades to constraint stat_for_unit_natural_valid_uniq on table etl_bench.stat_for_unit
drop cascades to constraint activity_natural_valid_uniq on table etl_bench.activity
drop cascades to constraint location_natural_valid_uniq on table etl_bench.location
drop cascades to constraint location_natural_valid_pk_consistency_excl on table etl_bench.location
-- Revoke any privileges held by the test user and drop any objects they own.
-- This is necessary before the role can be dropped.
DROP OWNED BY sql_saga_unprivileged_user;
DROP ROLE IF EXISTS sql_saga_unprivileged_user;
