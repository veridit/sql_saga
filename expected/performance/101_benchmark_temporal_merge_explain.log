
--- Performance Monitoring: EXPLAIN the cached planner query from previous temporal_merge call ---
--- Explaining 50 steps from cache for source legal_unit_source
---
Explaining: 
                CREATE TEMP TABLE source_initial ON COMMIT DROP AS
                WITH source_data AS (
                    SELECT
                        source_table.row_id /* row_id_column */ as source_row_id,
                        source_table.row_id /* v_causal_select_expr */ as causal_id,
                        source_table.id,  -- v_non_temporal_lookup_cols_select_list_prefix
                        COALESCE(lower(source_table.valid_range), NULL) as valid_from, COALESCE(upper(source_table.valid_range), NULL) as valid_until, /* v_source_temporal_cols_expr */
                        jsonb_strip_nulls(jsonb_build_object('name', source_table.name)) /* v_source_data_payload_expr */ AS data_payload,
                        jsonb_strip_nulls(jsonb_build_object()) /* v_source_ephemeral_payload_expr */ AS ephemeral_payload,
                        jsonb_build_object('id', source_table.id) /* v_stable_pk_cols_jsonb_build_source */ as stable_pk_payload,
                        source_table.id IS NULL /* v_entity_id_check_is_null_expr */ as stable_identity_columns_are_null,
                        (source_table.id IS NULL) /* v_lookup_cols_are_null_expr */ as lookup_columns_unavailable,
                        NOT ((source_table.id IS NULL) AND ((source_table.id IS NULL))) /* v_is_identifiable_expr */ as is_identifiable,
                        true /* v_consistency_check_expr */ as temporal_columns_are_consistent
                    FROM legal_unit_source /* v_source_table_ident */ source_table
                )
                SELECT 
                    *,
                    -- OPTIMIZATION: Pre-compute range column to avoid repeated daterange() calls
                    -- This provides ~10-15% performance improvement in eclipse detection
                    daterange(valid_from, valid_until) as valid_range
                FROM source_data;
            
Seq Scan on legal_unit_source source_table  (cost=0.00..110.50 rows=2500 width=152) (actual time=0.018..2.888 rows=2500.00 loops=1)
  Buffers: local hit=23
Planning Time: 0.040 ms
Execution Time: 5.857 ms
---
Executing setup: CREATE INDEX ON source_initial (source_row_id);
---
Executing setup: CREATE INDEX ON source_initial (id);
---
Executing setup: CREATE INDEX ON source_initial (id, source_row_id);
---
Executing setup: CREATE INDEX ON source_initial (causal_id);
---
Executing setup: CREATE INDEX ON source_initial (causal_id, source_row_id);
---
Executing setup: ANALYZE source_initial;
---
Explaining: 
                CREATE TEMP TABLE source_with_eclipsed_flag ON COMMIT DROP AS
                SELECT * FROM (
                    -- Path 1: Existing Entities (Window Function on Lookup Keys)
                    SELECT
                        s1.*,
                        COALESCE(
                            range_agg(valid_range) OVER (
                                PARTITION BY id
                                ORDER BY source_row_id DESC
                                ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING
                            ) @> valid_range,
                            false
                        ) as is_eclipsed,
                        array_agg(source_row_id) OVER (
                            PARTITION BY id
                            ORDER BY source_row_id DESC
                            ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING
                        ) as eclipsed_by
                    FROM source_initial s1
                    WHERE NOT s1.lookup_columns_unavailable

                    UNION ALL

                    -- Path 2: New Entities (Window Function on Causal ID)
                    SELECT
                        s1.*,
                        COALESCE(
                            range_agg(valid_range) OVER (
                                PARTITION BY causal_id
                                ORDER BY source_row_id DESC
                                ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING
                            ) @> valid_range,
                            false
                        ) as is_eclipsed,
                        array_agg(source_row_id) OVER (
                            PARTITION BY causal_id
                            ORDER BY source_row_id DESC
                            ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING
                        ) as eclipsed_by
                    FROM source_initial s1
                    WHERE s1.lookup_columns_unavailable
                ) t;
            
Append  (cost=230.10..394.14 rows=2501 width=135) (actual time=0.666..1.726 rows=2500.00 loops=1)
  Buffers: local hit=128
  ->  WindowAgg  (cost=230.10..292.60 rows=2500 width=135) (actual time=0.665..1.454 rows=2500.00 loops=1)
        Window: w1 AS (PARTITION BY s1.id ORDER BY s1.source_row_id ROWS BETWEEN UNBOUNDED PRECEDING AND '1'::bigint PRECEDING)
        Storage: Memory  Maximum Storage: 17kB
        Buffers: local hit=64
        ->  Sort  (cost=230.10..236.35 rows=2500 width=102) (actual time=0.660..0.726 rows=2500.00 loops=1)
              Sort Key: s1.id, s1.source_row_id DESC
              Sort Method: quicksort  Memory: 397kB
              Buffers: local hit=64
              ->  Seq Scan on source_initial s1  (cost=0.00..89.00 rows=2500 width=102) (actual time=0.005..0.284 rows=2500.00 loops=1)
                    Filter: (NOT lookup_columns_unavailable)
                    Buffers: local hit=64
  ->  WindowAgg  (cost=89.01..89.04 rows=1 width=135) (actual time=0.153..0.153 rows=0.00 loops=1)
        Window: w1 AS (PARTITION BY s1_1.causal_id ORDER BY s1_1.source_row_id ROWS BETWEEN UNBOUNDED PRECEDING AND '1'::bigint PRECEDING)
        Buffers: local hit=64
        ->  Sort  (cost=89.01..89.02 rows=1 width=102) (actual time=0.152..0.152 rows=0.00 loops=1)
              Sort Key: s1_1.causal_id, s1_1.source_row_id DESC
              Sort Method: quicksort  Memory: 25kB
              Buffers: local hit=64
              ->  Seq Scan on source_initial s1_1  (cost=0.00..89.00 rows=1 width=102) (actual time=0.147..0.147 rows=0.00 loops=1)
                    Filter: lookup_columns_unavailable
                    Rows Removed by Filter: 2500
                    Buffers: local hit=64
Planning:
  Buffers: shared hit=41, local read=5
Planning Time: 0.160 ms
Execution Time: 3.874 ms
---
Explaining: 
                CREATE TEMP TABLE target_rows ON COMMIT DROP AS
                SELECT
                    id,  /* v_non_temporal_lookup_cols_select_list_no_alias_prefix */ -- (non-temporal identity columns)
                    lower(t.valid_range) as valid_from, -- The temporal identity column (e.g., valid_from)
                    NULL::integer /* v_causal_column_type */ as causal_id, -- Target rows do not originate from a source row, so causal_id is NULL. Type is introspected.
                    jsonb_build_object('id', id) /* v_stable_pk_cols_jsonb_build_bare */ as stable_pk_payload,
                    upper(t.valid_range) as valid_until,
                    jsonb_build_object('name', name) /* v_target_data_cols_jsonb_build_bare */ AS data_payload,
                    jsonb_build_object() /* v_target_ephemeral_cols_jsonb_build_bare */ AS ephemeral_payload,
                    jsonb_strip_nulls(jsonb_build_object('id', t.id)) /* v_target_nk_json_expr */ AS canonical_nk_json
                FROM (
                        SELECT * FROM (
                            SELECT DISTINCT ON (u.id, u.valid_range) * FROM (
                                (
                                        SELECT DISTINCT ON (partitioned.id, partitioned.valid_range) * FROM (
                                            (
                                            SELECT inner_t.*
                                            FROM legal_unit_tm inner_t
                                            WHERE inner_t.id IS NOT NULL
                                              AND (inner_t.id) IN (
                                                SELECT si.id FROM source_initial si WHERE si.id IS NOT NULL
                                              )
                                        )
                                        ) partitioned
                                    )
                            ) u
                        )
                    ) AS t  /* v_target_rows_filter */
            
Subquery Scan on unnamed_subquery  (cost=292.23..308.67 rows=200 width=144) (actual time=0.937..4.983 rows=2500.00 loops=1)
  Buffers: shared hit=36, local hit=64
  ->  Unique  (cost=292.23..305.17 rows=200 width=68) (actual time=0.926..1.712 rows=2500.00 loops=1)
        Buffers: shared hit=36, local hit=64
        ->  Unique  (cost=292.23..303.64 rows=306 width=68) (actual time=0.926..1.371 rows=2500.00 loops=1)
              Buffers: shared hit=36, local hit=64
              ->  Sort  (cost=292.23..296.03 rows=1522 width=68) (actual time=0.925..0.996 rows=2500.00 loops=1)
                    Sort Key: inner_t.id, inner_t.valid_range
                    Sort Method: quicksort  Memory: 229kB
                    Buffers: shared hit=36, local hit=64
                    ->  Hash Semi Join  (cost=120.25..211.78 rows=1522 width=68) (actual time=0.347..0.743 rows=2500.00 loops=1)
                          Hash Cond: (inner_t.id = si.id)
                          Buffers: shared hit=36, local hit=64
                          ->  Seq Scan on legal_unit_tm inner_t  (cost=0.00..66.60 rows=3045 width=68) (actual time=0.035..0.172 rows=2500.00 loops=1)
                                Filter: (id IS NOT NULL)
                                Buffers: shared hit=36
                          ->  Hash  (cost=89.00..89.00 rows=2500 width=4) (actual time=0.307..0.308 rows=2500.00 loops=1)
                                Buckets: 4096  Batches: 1  Memory Usage: 120kB
                                Buffers: local hit=64
                                ->  Seq Scan on source_initial si  (cost=0.00..89.00 rows=2500 width=4) (actual time=0.004..0.164 rows=2500.00 loops=1)
                                      Filter: (id IS NOT NULL)
                                      Buffers: local hit=64
Planning:
  Buffers: shared hit=2
Planning Time: 0.142 ms
Execution Time: 7.066 ms
---
Executing setup: CREATE INDEX ON target_rows (id);
---
Executing setup: CREATE INDEX ON target_rows USING gist (daterange(valid_from, valid_until));
---
Executing setup: ANALYZE target_rows;
---
Explaining: 
                CREATE TEMP TABLE source_rows_with_matches ON COMMIT DROP AS
                SELECT
                    source_row.*,
                    target_row.stable_pk_payload as discovered_stable_pk_payload,
                    target_row.id AS discovered_id_1 /* v_propagated_id_cols_list */
                FROM source_with_eclipsed_flag source_row
                LEFT JOIN target_rows target_row ON ((((source_row.id = target_row.id OR (source_row.id IS NULL AND target_row.id IS NULL)))) /* v_source_rows_exists_join_expr */)
            
Nested Loop Left Join  (cost=5.22..22554.05 rows=2432 width=214) (actual time=0.029..4.368 rows=2500.00 loops=1)
  Buffers: local hit=12556 read=8 written=1
  ->  Seq Scan on source_with_eclipsed_flag source_row  (cost=0.00..88.32 rows=2432 width=185) (actual time=0.004..0.115 rows=2500.00 loops=1)
        Buffers: local hit=64
  ->  Bitmap Heap Scan on target_rows target_row  (cost=5.22..9.23 rows=1 width=29) (actual time=0.001..0.001 rows=1.00 loops=2500)
        Recheck Cond: ((source_row.id = id) OR (id IS NULL))
        Filter: ((source_row.id = id) OR ((source_row.id IS NULL) AND (id IS NULL)))
        Heap Blocks: exact=2500
        Buffers: local hit=12492 read=8 written=1
        ->  BitmapOr  (cost=5.22..5.22 rows=1 width=0) (actual time=0.001..0.001 rows=0.00 loops=2500)
              Buffers: local hit=9992 read=8 written=1
              ->  Bitmap Index Scan on target_rows_id_idx  (cost=0.00..0.30 rows=1 width=0) (actual time=0.000..0.000 rows=1.00 loops=2500)
                    Index Cond: (id = source_row.id)
                    Index Searches: 2500
                    Buffers: local hit=4993 read=7 written=1
              ->  Bitmap Index Scan on target_rows_id_idx  (cost=0.00..4.29 rows=1 width=0) (actual time=0.000..0.000 rows=0.00 loops=2500)
                    Index Cond: (id IS NULL)
                    Index Searches: 2500
                    Buffers: local hit=4999 read=1
Planning:
  Buffers: shared hit=44, local read=1
Planning Time: 0.107 ms
Execution Time: 6.764 ms
---
Explaining: 
                CREATE TEMP TABLE source_rows_with_aggregates ON COMMIT DROP AS
                SELECT
                    source_row_id,
                    count(DISTINCT discovered_stable_pk_payload) as match_count,
                    -- Filter out NULL to avoid [null] result for new entities with no matches
                    COALESCE(jsonb_agg(DISTINCT discovered_stable_pk_payload) FILTER (WHERE discovered_stable_pk_payload IS NOT NULL), '[]'::jsonb) as conflicting_ids
                FROM source_rows_with_matches
                GROUP BY source_row_id
            
GroupAggregate  (cost=197.12..220.10 rows=200 width=44) (actual time=0.399..1.974 rows=2500.00 loops=1)
  Group Key: source_row_id
  Buffers: local hit=64
  ->  Sort  (cost=197.12..202.24 rows=2048 width=36) (actual time=0.391..0.460 rows=2500.00 loops=1)
        Sort Key: source_row_id, discovered_stable_pk_payload
        Sort Method: quicksort  Memory: 214kB
        Buffers: local hit=64
        ->  Seq Scan on source_rows_with_matches  (cost=0.00..84.48 rows=2048 width=36) (actual time=0.004..0.243 rows=2500.00 loops=1)
              Buffers: local hit=64
Planning:
  Buffers: shared hit=38
Planning Time: 0.057 ms
Execution Time: 3.366 ms
---
Explaining: 
                CREATE TEMP TABLE source_rows_with_discovery ON COMMIT DROP AS
                SELECT
                    m.*,
                    a.match_count,
                    a.conflicting_ids,
                    (a.match_count > 1) as is_ambiguous
                FROM source_rows_with_matches m
                JOIN source_rows_with_aggregates a ON m.source_row_id = a.source_row_id
            
Merge Join  (cost=478.99..1137.22 rows=37028 width=262) (actual time=0.538..1.221 rows=2500.00 loops=1)
  Merge Cond: (m.source_row_id = a.source_row_id)
  Buffers: local hit=96
  ->  Sort  (cost=197.12..202.24 rows=2048 width=221) (actual time=0.215..0.281 rows=2500.00 loops=1)
        Sort Key: m.source_row_id
        Sort Method: quicksort  Memory: 483kB
        Buffers: local hit=64
        ->  Seq Scan on source_rows_with_matches m  (cost=0.00..84.48 rows=2048 width=221) (actual time=0.004..0.122 rows=2500.00 loops=1)
              Buffers: local hit=64
  ->  Sort  (cost=281.87..290.91 rows=3616 width=44) (actual time=0.319..0.384 rows=2500.00 loops=1)
        Sort Key: a.source_row_id
        Sort Method: quicksort  Memory: 253kB
        Buffers: local hit=32
        ->  Seq Scan on source_rows_with_aggregates a  (cost=0.00..68.16 rows=3616 width=44) (actual time=0.003..0.143 rows=2500.00 loops=1)
              Buffers: local hit=32
Planning:
  Buffers: shared hit=10
Planning Time: 0.051 ms
Execution Time: 4.492 ms
---
Explaining: 
                CREATE TEMP TABLE source_rows ON COMMIT DROP AS
                SELECT DISTINCT ON (p.source_row_id)
                    p.source_row_id, p.causal_id, p.valid_from, p.valid_until, p.data_payload, p.ephemeral_payload,
                    (p.stable_identity_columns_are_null AND p.discovered_stable_pk_payload IS NULL) as stable_identity_columns_are_null,
                    p.lookup_columns_unavailable, p.is_identifiable,
                    p.is_ambiguous, p.conflicting_ids, p.is_eclipsed, p.eclipsed_by,
                    p.temporal_columns_are_consistent,
                    COALESCE(p.stable_pk_payload, p.discovered_stable_pk_payload) as stable_pk_payload,
                    (p.discovered_stable_pk_payload IS NOT NULL) /* v_target_entity_exists_expr */ as target_entity_exists,
                    COALESCE(p.id, p.discovered_id_1) AS id /* v_coalesced_id_cols_list */
                FROM source_rows_with_discovery p
                ORDER BY p.source_row_id, p.discovered_stable_pk_payload
            
Unique  (cost=375.43..393.35 rows=200 width=219) (actual time=0.841..1.089 rows=2500.00 loops=1)
  Buffers: local hit=128
  ->  Sort  (cost=375.43..384.39 rows=3584 width=219) (actual time=0.841..0.906 rows=2500.00 loops=1)
        Sort Key: source_row_id, discovered_stable_pk_payload
        Sort Method: quicksort  Memory: 542kB
        Buffers: local hit=128
        ->  Seq Scan on source_rows_with_discovery p  (cost=0.00..163.84 rows=3584 width=219) (actual time=0.005..0.368 rows=2500.00 loops=1)
              Buffers: local hit=128
Planning:
  Buffers: shared hit=42
Planning Time: 0.061 ms
Execution Time: 3.441 ms
---
Explaining: 
                CREATE TEMP TABLE source_rows_with_new_flag ON COMMIT DROP AS
                SELECT *, NOT target_entity_exists as is_new_entity
                FROM source_rows
            
Seq Scan on source_rows  (cost=0.00..87.68 rows=2368 width=188) (actual time=0.004..0.306 rows=2500.00 loops=1)
  Buffers: local hit=64
Planning:
  Buffers: shared hit=36
Planning Time: 0.038 ms
Execution Time: 2.744 ms
---
Explaining: 
                CREATE TEMP TABLE source_rows_with_nk_json ON COMMIT DROP AS
                SELECT source_row.*, jsonb_strip_nulls(jsonb_build_object('id', source_row.id)) /* v_lookup_keys_as_jsonb_expr */ as nk_json, ARRAY(SELECT e FROM unnest(ARRAY[CASE WHEN source_row.id IS NOT NULL THEN 'id' END]::TEXT[]) e WHERE e IS NOT NULL ORDER BY e) /* v_lookup_keys_as_array_expr */ as nk_non_null_keys_array
                FROM source_rows_with_new_flag source_row
            
Seq Scan on source_rows_with_new_flag source_row  (cost=0.00..164.64 rows=2368 width=252) (actual time=0.023..4.846 rows=2500.00 loops=1)
  Buffers: local hit=64
  SubPlan 1
    ->  Sort  (cost=0.02..0.03 rows=1 width=32) (actual time=0.001..0.001 rows=1.00 loops=2500)
          Sort Key: e.e
          Sort Method: quicksort  Memory: 25kB
          ->  Function Scan on unnest e  (cost=0.00..0.01 rows=1 width=32) (actual time=0.000..0.000 rows=1.00 loops=2500)
                Filter: (e IS NOT NULL)
Planning:
  Buffers: shared hit=38
Planning Time: 0.056 ms
Execution Time: 8.566 ms
---
Executing setup: CREATE INDEX ON source_rows_with_nk_json USING GIN (nk_json);
---
Executing setup: ANALYZE source_rows_with_nk_json;
---
Explaining: 
                CREATE TEMP TABLE source_rows_with_canonical_key ON COMMIT DROP AS
                WITH nk_specificity_check AS (
                    SELECT count(DISTINCT nk_non_null_keys_array::text) = 1 AS is_uniform
                    FROM source_rows_with_nk_json
                    WHERE is_new_entity
                )
                SELECT s.*, (CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN canonical_nk_json IS NOT NULL AND canonical_nk_json <> '{}'::jsonb THEN COALESCE(canonical_nk_json->>'id', '_NULL_') ELSE COALESCE(id::text, '_NULL_') END
                ELSE 'existing_entity__' || COALESCE(id::text, '_NULL_')
            END /* v_grouping_key_expr */) as grouping_key
                FROM (
                    SELECT
                        s1.*,
                        CASE 
                            -- Fast path: uniform specificity means each row is its own canonical
                            WHEN (SELECT is_uniform FROM nk_specificity_check) 
                            THEN s1.nk_json
                            -- Slow path: varied specificity requires finding most specific match
                            ELSE (
                                SELECT s2_inner.nk_json
                                FROM source_rows_with_nk_json s2_inner
                                WHERE s1.is_new_entity AND s2_inner.is_new_entity AND s2_inner.nk_json @> s1.nk_json
                                ORDER BY array_length(s2_inner.nk_non_null_keys_array, 1) DESC, s2_inner.nk_non_null_keys_array::text DESC
                                LIMIT 1
                            )
                        END as canonical_nk_json
                    FROM source_rows_with_nk_json s1
                ) s
            
Seq Scan on source_rows_with_nk_json s1  (cost=612.14..761054.66 rows=2500 width=275) (actual time=0.254..1.974 rows=2500.00 loops=1)
  Buffers: local hit=256
  InitPlan 1
    ->  Aggregate  (cost=153.02..153.03 rows=1 width=1) (actual time=0.243..0.244 rows=1.00 loops=1)
          Buffers: local hit=128
          ->  Sort  (cost=153.01..153.01 rows=1 width=29) (actual time=0.242..0.242 rows=0.00 loops=1)
                Sort Key: ((source_rows_with_nk_json.nk_non_null_keys_array)::text)
                Sort Method: quicksort  Memory: 25kB
                Buffers: local hit=128
                ->  Seq Scan on source_rows_with_nk_json  (cost=0.00..153.00 rows=1 width=29) (actual time=0.240..0.240 rows=0.00 loops=1)
                      Filter: is_new_entity
                      Rows Removed by Filter: 2500
                      Buffers: local hit=128
  InitPlan 3
    ->  Aggregate  (cost=153.02..153.03 rows=1 width=1) (never executed)
          ->  Sort  (cost=153.01..153.01 rows=1 width=29) (never executed)
                Sort Key: ((source_rows_with_nk_json_1.nk_non_null_keys_array)::text)
                ->  Seq Scan on source_rows_with_nk_json source_rows_with_nk_json_1  (cost=0.00..153.00 rows=1 width=29) (never executed)
                      Filter: is_new_entity
  InitPlan 5
    ->  Aggregate  (cost=153.02..153.03 rows=1 width=1) (never executed)
          ->  Sort  (cost=153.01..153.01 rows=1 width=29) (never executed)
                Sort Key: ((source_rows_with_nk_json_2.nk_non_null_keys_array)::text)
                ->  Seq Scan on source_rows_with_nk_json source_rows_with_nk_json_2  (cost=0.00..153.00 rows=1 width=29) (never executed)
                      Filter: is_new_entity
  InitPlan 7
    ->  Aggregate  (cost=153.02..153.03 rows=1 width=1) (never executed)
          ->  Sort  (cost=153.01..153.01 rows=1 width=29) (never executed)
                Sort Key: ((source_rows_with_nk_json_3.nk_non_null_keys_array)::text)
                ->  Seq Scan on source_rows_with_nk_json source_rows_with_nk_json_3  (cost=0.00..153.00 rows=1 width=29) (never executed)
                      Filter: is_new_entity
  SubPlan 2
    ->  Limit  (cost=76.02..76.02 rows=1 width=61) (actual time=0.000..0.000 rows=0.00 loops=2500)
          ->  Sort  (cost=76.02..76.02 rows=1 width=61) (actual time=0.000..0.000 rows=0.00 loops=2500)
                Sort Key: (array_length(s2_inner.nk_non_null_keys_array, 1)) DESC, ((s2_inner.nk_non_null_keys_array)::text) DESC
                Sort Method: quicksort  Memory: 25kB
                ->  Result  (cost=12.94..76.01 rows=1 width=61) (actual time=0.000..0.000 rows=0.00 loops=2500)
                      One-Time Filter: s1.is_new_entity
                      ->  Bitmap Heap Scan on source_rows_with_nk_json s2_inner  (cost=12.94..76.00 rows=1 width=54) (never executed)
                            Recheck Cond: (nk_json @> s1.nk_json)
                            Filter: is_new_entity
                            ->  Bitmap Index Scan on source_rows_with_nk_json_nk_json_idx  (cost=0.00..12.94 rows=25 width=0) (never executed)
                                  Index Cond: (nk_json @> s1.nk_json)
                                  Index Searches: 0
  SubPlan 4
    ->  Limit  (cost=76.02..76.02 rows=1 width=61) (never executed)
          ->  Sort  (cost=76.02..76.02 rows=1 width=61) (never executed)
                Sort Key: (array_length(s2_inner_1.nk_non_null_keys_array, 1)) DESC, ((s2_inner_1.nk_non_null_keys_array)::text) DESC
                ->  Result  (cost=12.94..76.01 rows=1 width=61) (never executed)
                      One-Time Filter: s1.is_new_entity
                      ->  Bitmap Heap Scan on source_rows_with_nk_json s2_inner_1  (cost=12.94..76.00 rows=1 width=54) (never executed)
                            Recheck Cond: (nk_json @> s1.nk_json)
                            Filter: is_new_entity
                            ->  Bitmap Index Scan on source_rows_with_nk_json_nk_json_idx  (cost=0.00..12.94 rows=25 width=0) (never executed)
                                  Index Cond: (nk_json @> s1.nk_json)
                                  Index Searches: 0
  SubPlan 6
    ->  Limit  (cost=76.02..76.02 rows=1 width=61) (never executed)
          ->  Sort  (cost=76.02..76.02 rows=1 width=61) (never executed)
                Sort Key: (array_length(s2_inner_2.nk_non_null_keys_array, 1)) DESC, ((s2_inner_2.nk_non_null_keys_array)::text) DESC
                ->  Result  (cost=12.94..76.01 rows=1 width=61) (never executed)
                      One-Time Filter: s1.is_new_entity
                      ->  Bitmap Heap Scan on source_rows_with_nk_json s2_inner_2  (cost=12.94..76.00 rows=1 width=54) (never executed)
                            Recheck Cond: (nk_json @> s1.nk_json)
                            Filter: is_new_entity
                            ->  Bitmap Index Scan on source_rows_with_nk_json_nk_json_idx  (cost=0.00..12.94 rows=25 width=0) (never executed)
                                  Index Cond: (nk_json @> s1.nk_json)
                                  Index Searches: 0
  SubPlan 8
    ->  Limit  (cost=76.02..76.02 rows=1 width=61) (never executed)
          ->  Sort  (cost=76.02..76.02 rows=1 width=61) (never executed)
                Sort Key: (array_length(s2_inner_3.nk_non_null_keys_array, 1)) DESC, ((s2_inner_3.nk_non_null_keys_array)::text) DESC
                ->  Result  (cost=12.94..76.01 rows=1 width=61) (never executed)
                      One-Time Filter: s1.is_new_entity
                      ->  Bitmap Heap Scan on source_rows_with_nk_json s2_inner_3  (cost=12.94..76.00 rows=1 width=54) (never executed)
                            Recheck Cond: (nk_json @> s1.nk_json)
                            Filter: is_new_entity
                            ->  Bitmap Index Scan on source_rows_with_nk_json_nk_json_idx  (cost=0.00..12.94 rows=25 width=0) (never executed)
                                  Index Cond: (nk_json @> s1.nk_json)
                                  Index Searches: 0
Planning:
  Buffers: shared hit=64, local hit=4
Planning Time: 0.281 ms
Execution Time: 5.435 ms
---
Explaining: 
                CREATE TEMP TABLE source_rows_with_early_feedback ON COMMIT DROP AS
                SELECT
                    s.*,
                    CASE
                        WHEN s.is_ambiguous
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row is ambiguous. It matches multiple distinct target entities: ' || s.conflicting_ids::text )
                        WHEN NOT s.is_identifiable AND s.is_new_entity
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row is unidentifiable. It has NULL for all stable identity columns ' || replace('{id}'::text, '"', '') || ' and all natural keys ' || replace('[["id"]]'::text, '"', '') )
                        WHEN NOT s.temporal_columns_are_consistent
                        THEN jsonb_build_object( 'operation', 'ERROR'::text, 'message', 'Source row has inconsistent temporal columns. Column "' || NULL || '" must be equal to column "' || NULL || '" + ' || NULL || '.' )
                        WHEN s.is_eclipsed
                        THEN jsonb_build_object( 'operation', 'SKIP_ECLIPSED'::text, 'message', 'Source row was eclipsed by row_ids=' || s.eclipsed_by::text || ' in the same batch.' )
                        ELSE NULL
                    END as early_feedback
                FROM source_rows_with_canonical_key s
            
Seq Scan on source_rows_with_canonical_key s  (cost=0.00..238.40 rows=2944 width=348) (actual time=0.005..0.446 rows=2500.00 loops=1)
  Buffers: local hit=128
Planning:
  Buffers: shared hit=46
Planning Time: 0.057 ms
Execution Time: 4.029 ms
---
Explaining: 
                CREATE TEMP TABLE active_source_rows ON COMMIT DROP AS
                SELECT source_row.*
                FROM source_rows_with_early_feedback source_row
                WHERE source_row.early_feedback IS NULL
                AND CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                    WHEN 'MERGE_ENTITY_PATCH' THEN true
                    WHEN 'MERGE_ENTITY_REPLACE' THEN true
                    WHEN 'MERGE_ENTITY_UPSERT' THEN true
                    WHEN 'INSERT_NEW_ENTITIES' THEN NOT source_row.target_entity_exists
                    WHEN 'PATCH_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'REPLACE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'DELETE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    WHEN 'UPDATE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                    ELSE false
                END
            
Seq Scan on source_rows_with_early_feedback source_row  (cost=0.00..154.88 rows=13 width=348) (actual time=0.004..0.339 rows=2500.00 loops=1)
  Filter: (early_feedback IS NULL)
  Buffers: local hit=128
Planning:
  Buffers: shared hit=48
Planning Time: 0.055 ms
Execution Time: 2.900 ms
---
Executing setup: CREATE INDEX ON active_source_rows (id);
---
Executing setup: CREATE INDEX ON active_source_rows (grouping_key);
---
Executing setup: CREATE INDEX ON active_source_rows USING gist (daterange(valid_from, valid_until));
---
Executing setup: ANALYZE active_source_rows;
---
Explaining: 
                CREATE TEMP TABLE all_rows ON COMMIT DROP AS
                SELECT id,  /* v_non_temporal_lookup_cols_select_list_no_alias_prefix */ causal_id, valid_from, valid_until, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, lookup_columns_unavailable, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json FROM active_source_rows
                UNION ALL
                SELECT
                    target_row.id,  /* v_non_temporal_tr_qualified_lookup_cols_prefix */
                    target_row.causal_id,
                    target_row.valid_from,
                    target_row.valid_until,
                    false as is_new_entity,
                    target_row.stable_pk_payload,
                    false as stable_identity_columns_are_null,
                    false as lookup_columns_unavailable,
                    true as is_identifiable,
                    false as is_ambiguous,
                    NULL::jsonb as conflicting_ids,
                    target_row.canonical_nk_json
                FROM target_rows target_row
            
Append  (cost=0.00..267.00 rows=5000 width=107) (actual time=0.007..0.842 rows=5000.00 loops=1)
  Buffers: local hit=192
  ->  Seq Scan on active_source_rows  (cost=0.00..153.00 rows=2500 width=111) (actual time=0.007..0.386 rows=2500.00 loops=1)
        Buffers: local hit=128
  ->  Seq Scan on target_rows target_row  (cost=0.00..89.00 rows=2500 width=103) (actual time=0.009..0.224 rows=2500.00 loops=1)
        Buffers: local hit=64
Planning:
  Buffers: shared hit=50, local read=2 written=1
Planning Time: 0.106 ms
Execution Time: 4.366 ms
---
Explaining: 
                CREATE TEMP TABLE time_points_raw ON COMMIT DROP AS
                SELECT id, causal_id, valid_from AS point, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, lookup_columns_unavailable, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json FROM all_rows
                UNION ALL
                SELECT id, causal_id, valid_until AS point, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, lookup_columns_unavailable, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json FROM all_rows
            
Append  (cost=0.00..471.04 rows=14336 width=113) (actual time=0.008..1.360 rows=10000.00 loops=1)
  Buffers: local hit=256
  ->  Seq Scan on all_rows  (cost=0.00..199.68 rows=7168 width=113) (actual time=0.008..0.456 rows=5000.00 loops=1)
        Buffers: local hit=128
  ->  Seq Scan on all_rows all_rows_1  (cost=0.00..199.68 rows=7168 width=113) (actual time=0.004..0.440 rows=5000.00 loops=1)
        Buffers: local hit=128
Planning:
  Buffers: shared hit=26
Planning Time: 0.057 ms
Execution Time: 7.280 ms
---
  ->  WindowAgg  (cost=565.82..686.12 rows=6016 width=149) (actual time=1.550..3.268 rows=5000.00 loops=1)
        Window: w1 AS (PARTITION BY time_points.grouping_key ORDER BY time_points.point)
        Storage: Memory  Maximum Storage: 17kB
Explaining: 
                CREATE TEMP TABLE time_points_unified ON COMMIT DROP AS
                SELECT
                    *,
                    CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN canonical_nk_json IS NOT NULL AND canonical_nk_json <> '{}'::jsonb THEN COALESCE(canonical_nk_json->>'id', '_NULL_') ELSE COALESCE(id::text, '_NULL_') END
                ELSE 'existing_entity__' || COALESCE(id::text, '_NULL_')
            END AS grouping_key,
                    CASE
                        WHEN is_new_entity THEN causal_id
                        ELSE FIRST_VALUE(causal_id) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN canonical_nk_json IS NOT NULL AND canonical_nk_json <> '{}'::jsonb THEN COALESCE(canonical_nk_json->>'id', '_NULL_') ELSE COALESCE(id::text, '_NULL_') END
                ELSE 'existing_entity__' || COALESCE(id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS LAST)
                    END as unified_causal_id,
                    FIRST_VALUE(stable_pk_payload) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN canonical_nk_json IS NOT NULL AND canonical_nk_json <> '{}'::jsonb THEN COALESCE(canonical_nk_json->>'id', '_NULL_') ELSE COALESCE(id::text, '_NULL_') END
                ELSE 'existing_entity__' || COALESCE(id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS FIRST) as unified_stable_pk_payload,
                    FIRST_VALUE(canonical_nk_json) OVER (PARTITION BY CASE
                WHEN is_new_entity
                THEN 'new_entity__' || CASE WHEN canonical_nk_json IS NOT NULL AND canonical_nk_json <> '{}'::jsonb THEN COALESCE(canonical_nk_json->>'id', '_NULL_') ELSE COALESCE(id::text, '_NULL_') END
                ELSE 'existing_entity__' || COALESCE(id::text, '_NULL_')
            END ORDER BY causal_id ASC NULLS FIRST) as unified_canonical_nk_json
                FROM time_points_raw
            
WindowAgg  (cost=1255.25..2470.55 rows=10944 width=213) (actual time=10.847..23.909 rows=10000.00 loops=1)
  Window: w2 AS (PARTITION BY (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((canonical_nk_json IS NOT NULL) AND (canonical_nk_json <> '{}'::jsonb)) THEN COALESCE((canonical_nk_json ->> 'id'::text), '_NULL_'::text) ELSE COALESCE((id)::text, '_NULL_'::text) END) ELSE ('existing_entity__'::text || COALESCE((id)::text, '_NULL_'::text)) END) ORDER BY causal_id)
  Storage: Memory  Maximum Storage: 17kB
  Buffers: local hit=192
  ->  Incremental Sort  (cost=1255.16..2060.15 rows=10944 width=209) (actual time=10.843..20.494 rows=10000.00 loops=1)
        Sort Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((canonical_nk_json IS NOT NULL) AND (canonical_nk_json <> '{}'::jsonb)) THEN COALESCE((canonical_nk_json ->> 'id'::text), '_NULL_'::text) ELSE COALESCE((id)::text, '_NULL_'::text) END) ELSE ('existing_entity__'::text || COALESCE((id)::text, '_NULL_'::text)) END), causal_id
        Presorted Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((canonical_nk_json IS NOT NULL) AND (canonical_nk_json <> '{}'::jsonb)) THEN COALESCE((canonical_nk_json ->> 'id'::text), '_NULL_'::text) ELSE COALESCE((id)::text, '_NULL_'::text) END) ELSE ('existing_entity__'::text || COALESCE((id)::text, '_NULL_'::text)) END)
        Full-sort Groups: 313  Sort Method: quicksort  Average Memory: 30kB  Peak Memory: 30kB
        Buffers: local hit=192
        ->  WindowAgg  (cost=1254.57..1719.66 rows=10944 width=209) (actual time=10.799..15.685 rows=10000.00 loops=1)
              Window: w1 AS (PARTITION BY (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((canonical_nk_json IS NOT NULL) AND (canonical_nk_json <> '{}'::jsonb)) THEN COALESCE((canonical_nk_json ->> 'id'::text), '_NULL_'::text) ELSE COALESCE((id)::text, '_NULL_'::text) END) ELSE ('existing_entity__'::text || COALESCE((id)::text, '_NULL_'::text)) END) ORDER BY causal_id)
              Storage: Memory  Maximum Storage: 17kB
              Buffers: local hit=192
              ->  Sort  (cost=1254.54..1281.90 rows=10944 width=145) (actual time=10.785..11.068 rows=10000.00 loops=1)
                    Sort Key: (CASE WHEN is_new_entity THEN ('new_entity__'::text || CASE WHEN ((canonical_nk_json IS NOT NULL) AND (canonical_nk_json <> '{}'::jsonb)) THEN COALESCE((canonical_nk_json ->> 'id'::text), '_NULL_'::text) ELSE COALESCE((id)::text, '_NULL_'::text) END) ELSE ('existing_entity__'::text || COALESCE((id)::text, '_NULL_'::text)) END), causal_id NULLS FIRST
                    Sort Method: quicksort  Memory: 1555kB
                    Buffers: local hit=192
                    ->  Seq Scan on time_points_raw  (cost=0.00..520.32 rows=10944 width=145) (actual time=0.006..1.354 rows=10000.00 loops=1)
                          Buffers: local hit=192
Planning:
  Buffers: shared hit=24
Planning Time: 0.072 ms
Execution Time: 30.473 ms
---
Explaining: 
                CREATE TEMP TABLE time_points_with_unified_ids ON COMMIT DROP AS
                SELECT
                    tpu.grouping_key,
                    (tpu.unified_canonical_nk_json->>'id')::integer AS id,
                    tpu.unified_causal_id as causal_id,
                    tpu.point,
                    tpu.is_new_entity,
                    tpu.unified_stable_pk_payload as stable_pk_payload,
                    tpu.stable_identity_columns_are_null,
                    tpu.lookup_columns_unavailable,
                    tpu.is_identifiable,
                    tpu.is_ambiguous,
                    tpu.conflicting_ids,
                    tpu.unified_canonical_nk_json as canonical_nk_json
                FROM time_points_unified tpu
            
Seq Scan on time_points_unified tpu  (cost=0.00..403.84 rows=8448 width=145) (actual time=0.008..2.015 rows=10000.00 loops=1)
  Buffers: local hit=256
Planning:
  Buffers: shared hit=32
Planning Time: 0.057 ms
Execution Time: 8.071 ms
---
Explaining: 
                CREATE TEMP TABLE time_points ON COMMIT DROP AS
                SELECT DISTINCT ON (grouping_key, point) *
                FROM time_points_with_unified_ids
                ORDER BY grouping_key, point, causal_id DESC NULLS LAST
            
Unique  (cost=875.10..942.78 rows=902 width=145) (actual time=5.724..6.806 rows=5000.00 loops=1)
  Buffers: local hit=192
  ->  Sort  (cost=875.10..897.66 rows=9024 width=145) (actual time=5.723..5.987 rows=10000.00 loops=1)
        Sort Key: grouping_key, point, causal_id DESC NULLS LAST
        Sort Method: quicksort  Memory: 1670kB
        Buffers: local hit=192
        ->  Seq Scan on time_points_with_unified_ids  (cost=0.00..282.24 rows=9024 width=145) (actual time=0.003..0.380 rows=10000.00 loops=1)
              Buffers: local hit=192
Planning:
  Buffers: shared hit=26
Planning Time: 0.051 ms
Execution Time: 9.420 ms
---
Explaining: 
                CREATE TEMP TABLE atomic_segments ON COMMIT DROP AS
                SELECT grouping_key, id, causal_id, point as valid_from, next_point as valid_until, is_new_entity, stable_pk_payload, stable_identity_columns_are_null, lookup_columns_unavailable, is_identifiable, is_ambiguous, conflicting_ids, canonical_nk_json
                FROM (
                    SELECT *, LEAD(point) OVER (PARTITION BY grouping_key ORDER BY point) as next_point
                    FROM time_points
                ) with_lead
                WHERE point IS NOT NULL AND next_point IS NOT NULL AND point < next_point
            
Subquery Scan on with_lead  (cost=565.82..761.32 rows=1985 width=149) (actual time=1.552..3.574 rows=2500.00 loops=1)
  Filter: ((with_lead.point IS NOT NULL) AND (with_lead.next_point IS NOT NULL) AND (with_lead.point < with_lead.next_point))
  Rows Removed by Filter: 2500
  Buffers: local hit=128
        Buffers: local hit=128
        ->  Sort  (cost=565.80..580.84 rows=6016 width=145) (actual time=1.545..1.675 rows=5000.00 loops=1)
              Sort Key: time_points.grouping_key, time_points.point
              Sort Method: quicksort  Memory: 847kB
              Buffers: local hit=128
              ->  Seq Scan on time_points  (cost=0.00..188.16 rows=6016 width=145) (actual time=0.005..0.542 rows=5000.00 loops=1)
                    Buffers: local hit=128
Planning:
  Buffers: shared hit=26
Planning Time: 0.073 ms
Execution Time: 5.750 ms
---
Executing setup: CREATE INDEX ON atomic_segments (grouping_key);
---
Executing setup: ANALYZE atomic_segments;
---
Explaining: 
                CREATE TEMP TABLE existing_segments_with_target ON COMMIT DROP AS
                SELECT
                    seg.grouping_key, seg.canonical_nk_json, seg.id, seg.causal_id, seg.is_new_entity, seg.is_identifiable, seg.is_ambiguous, seg.conflicting_ids, seg.stable_identity_columns_are_null, seg.lookup_columns_unavailable, seg.valid_from, seg.valid_until,
                    target_row.valid_from as t_valid_from, target_row.valid_until as t_valid_until,
                    target_row.data_payload as t_data_payload, target_row.ephemeral_payload as t_ephemeral_payload, seg.stable_pk_payload, 
                    target_row.stable_pk_payload as target_stable_pk_payload,
                    source_payloads.source_row_id, source_payloads.contributing_row_ids,
                    source_payloads.data_payload as s_data_payload, source_payloads.ephemeral_payload as s_ephemeral_payload,
                    source_payloads.valid_from AS s_valid_from, source_payloads.valid_until AS s_valid_until,
                    NULL::jsonb as trace
                FROM atomic_segments seg
                LEFT JOIN target_rows target_row 
                    ON (target_row.id = seg.id OR (target_row.id IS NULL AND seg.id IS NULL)) 
                    AND daterange(seg.valid_from, seg.valid_until) <@ daterange(target_row.valid_from, target_row.valid_until)
                
                        LEFT JOIN LATERAL (
                            WITH RECURSIVE ordered_sources AS (
                                SELECT
                                    source_row.source_row_id, source_row.data_payload, source_row.ephemeral_payload,
                                    source_row.valid_from, source_row.valid_until, source_row.causal_id,
                                    row_number() OVER (ORDER BY source_row.source_row_id) as rn
                                FROM active_source_rows source_row
                                WHERE (source_row.id = seg.id OR (source_row.id IS NULL AND seg.id IS NULL))
                                AND daterange(seg.valid_from, seg.valid_until) <@ daterange(source_row.valid_from, source_row.valid_until)
                            ),
                            running_payload AS (
                                SELECT rn, source_row_id, data_payload, ephemeral_payload, valid_from, valid_until, causal_id, ARRAY[source_row_id::BIGINT] as contributing_row_ids
                                FROM ordered_sources WHERE rn = 1
                                UNION ALL
                                SELECT
                                    s.rn, s.source_row_id,
                                    r.data_payload || jsonb_strip_nulls(s.data_payload),
                                    r.ephemeral_payload || jsonb_strip_nulls(s.ephemeral_payload),
                                    s.valid_from, s.valid_until, s.causal_id,
                                    r.contributing_row_ids || s.source_row_id::BIGINT
                                FROM running_payload r JOIN ordered_sources s ON s.rn = r.rn + 1
                            )
                            SELECT source_row_id, data_payload, ephemeral_payload, valid_from, valid_until, causal_id, contributing_row_ids
                            FROM running_payload
                            ORDER BY rn DESC
                            LIMIT 1
                        ) source_payloads ON true
                    
                WHERE NOT seg.is_new_entity
            
Nested Loop Left Join  (cost=16.26..41719.25 rows=2500 width=337) (actual time=0.103..703.665 rows=2500.00 loops=1)
  Buffers: local hit=172501 read=63 written=15
  ->  Nested Loop Left Join  (cost=0.15..1386.75 rows=2500 width=197) (actual time=0.055..680.267 rows=2500.00 loops=1)
        Buffers: local hit=160009 read=55 written=9
        ->  Seq Scan on atomic_segments seg  (cost=0.00..89.00 rows=2500 width=125) (actual time=0.004..0.517 rows=2500.00 loops=1)
              Filter: (NOT is_new_entity)
              Buffers: local hit=64
        ->  Index Scan using target_rows_daterange_idx on target_rows target_row  (cost=0.15..0.51 rows=1 width=76) (actual time=0.142..0.271 rows=1.00 loops=2500)
              Index Cond: (daterange(valid_from, valid_until) @> daterange(seg.valid_from, seg.valid_until))
              Filter: ((id = seg.id) OR ((id IS NULL) AND (seg.id IS NULL)))
              Rows Removed by Filter: 2499
              Index Searches: 2500
              Buffers: local hit=159945 read=55 written=9
  ->  Limit  (cost=16.11..16.11 rows=1 width=120) (actual time=0.009..0.009 rows=1.00 loops=2500)
        Buffers: local hit=12492 read=8 written=6
        CTE ordered_sources
          ->  WindowAgg  (cost=12.61..12.63 rows=1 width=63) (actual time=0.004..0.004 rows=1.00 loops=2500)
                Window: w1 AS (ORDER BY source_row.source_row_id ROWS UNBOUNDED PRECEDING)
                Storage: Memory  Maximum Storage: 17kB
                Buffers: local hit=12492 read=8 written=6
                ->  Sort  (cost=12.61..12.61 rows=1 width=55) (actual time=0.004..0.004 rows=1.00 loops=2500)
                      Sort Key: source_row.source_row_id
                      Sort Method: quicksort  Memory: 25kB
                      Buffers: local hit=12492 read=8 written=6
                      ->  Bitmap Heap Scan on active_source_rows source_row  (cost=8.58..12.60 rows=1 width=55) (actual time=0.002..0.002 rows=1.00 loops=2500)
                            Recheck Cond: ((id = seg.id) OR (id IS NULL))
                            Filter: (((id = seg.id) OR ((id IS NULL) AND (seg.id IS NULL))) AND (daterange(seg.valid_from, seg.valid_until) <@ daterange(valid_from, valid_until)))
                            Heap Blocks: exact=2500
                            Buffers: local hit=12492 read=8 written=6
                            ->  BitmapOr  (cost=8.58..8.58 rows=1 width=0) (actual time=0.001..0.001 rows=0.00 loops=2500)
                                  Buffers: local hit=9992 read=8 written=6
                                  ->  Bitmap Index Scan on active_source_rows_id_idx  (cost=0.00..4.29 rows=1 width=0) (actual time=0.001..0.001 rows=1.00 loops=2500)
                                        Index Cond: (id = seg.id)
                                        Index Searches: 2500
                                        Buffers: local hit=4993 read=7 written=5
                                  ->  Bitmap Index Scan on active_source_rows_id_idx  (cost=0.00..4.29 rows=1 width=0) (actual time=0.000..0.000 rows=0.00 loops=2500)
                                        Index Cond: (id IS NULL)
                                        Index Searches: 2500
                                        Buffers: local hit=4999 read=1 written=1
        CTE running_payload
          ->  Recursive Union  (cost=0.00..3.21 rows=11 width=120) (actual time=0.005..0.007 rows=1.00 loops=2500)
                Storage: Memory  Maximum Storage: 33kB
                Buffers: local hit=12492 read=8 written=6
                ->  CTE Scan on ordered_sources  (cost=0.00..0.02 rows=1 width=120) (actual time=0.005..0.005 rows=1.00 loops=2500)
                      Filter: (rn = 1)
                      Storage: Memory  Maximum Storage: 17kB
                      Buffers: local hit=12492 read=8 written=6
                ->  Hash Join  (cost=0.03..0.31 rows=1 width=120) (actual time=0.001..0.001 rows=0.00 loops=2500)
                      Hash Cond: ((r.rn + 1) = s.rn)
                      ->  WorkTable Scan on running_payload r  (cost=0.00..0.20 rows=10 width=104) (actual time=0.000..0.000 rows=1.00 loops=2500)
                      ->  Hash  (cost=0.02..0.02 rows=1 width=88) (actual time=0.001..0.001 rows=1.00 loops=2500)
                            Buckets: 1024  Batches: 1  Memory Usage: 9kB
                            ->  CTE Scan on ordered_sources s  (cost=0.00..0.02 rows=1 width=88) (actual time=0.000..0.000 rows=1.00 loops=2500)
                                  Storage: Memory  Maximum Storage: 17kB
        ->  Sort  (cost=0.28..0.30 rows=11 width=120) (actual time=0.009..0.009 rows=1.00 loops=2500)
              Sort Key: running_payload.rn DESC
              Sort Method: quicksort  Memory: 25kB
              Buffers: local hit=12492 read=8 written=6
              ->  CTE Scan on running_payload  (cost=0.00..0.22 rows=11 width=120) (actual time=0.005..0.008 rows=1.00 loops=2500)
                    Storage: Memory  Maximum Storage: 17kB
                    Buffers: local hit=12492 read=8 written=6
Planning:
  Buffers: shared hit=64, local read=1 written=1
Planning Time: 0.261 ms
Execution Time: 708.149 ms
---
Executing setup: ANALYZE existing_segments_with_target;
---
Explaining: 
                CREATE TEMP TABLE new_segments_no_target ON COMMIT DROP AS
                SELECT
                    seg.grouping_key, seg.canonical_nk_json, seg.id, seg.causal_id, seg.is_new_entity, seg.is_identifiable, seg.is_ambiguous, seg.conflicting_ids, seg.stable_identity_columns_are_null, seg.lookup_columns_unavailable, seg.valid_from, seg.valid_until,
                    NULL::date as t_valid_from, NULL::date as t_valid_until,
                    NULL::jsonb as t_data_payload, NULL::jsonb as t_ephemeral_payload, seg.stable_pk_payload,
                    NULL::jsonb as target_stable_pk_payload,
                    source_payloads.source_row_id, source_payloads.contributing_row_ids,
                    source_payloads.data_payload as s_data_payload, source_payloads.ephemeral_payload as s_ephemeral_payload,
                    source_payloads.valid_from AS s_valid_from, source_payloads.valid_until AS s_valid_until,
                    NULL::jsonb as trace
                FROM atomic_segments seg
                
                        LEFT JOIN LATERAL (
                            WITH RECURSIVE ordered_sources AS (
                                SELECT
                                    source_row.source_row_id, source_row.data_payload, source_row.ephemeral_payload,
                                    source_row.valid_from, source_row.valid_until, source_row.causal_id,
                                    row_number() OVER (ORDER BY source_row.source_row_id) as rn
                                FROM active_source_rows source_row
                                WHERE source_row.grouping_key = seg.grouping_key
                                AND daterange(seg.valid_from, seg.valid_until) <@ daterange(source_row.valid_from, source_row.valid_until)
                            ),
                            running_payload AS (
                                SELECT rn, source_row_id, data_payload, ephemeral_payload, valid_from, valid_until, causal_id, ARRAY[source_row_id::BIGINT] as contributing_row_ids
                                FROM ordered_sources WHERE rn = 1
                                UNION ALL
                                SELECT
                                    s.rn, s.source_row_id,
                                    r.data_payload || jsonb_strip_nulls(s.data_payload),
                                    r.ephemeral_payload || jsonb_strip_nulls(s.ephemeral_payload),
                                    s.valid_from, s.valid_until, s.causal_id,
                                    r.contributing_row_ids || s.source_row_id::BIGINT
                                FROM running_payload r JOIN ordered_sources s ON s.rn = r.rn + 1
                            )
                            SELECT source_row_id, data_payload, ephemeral_payload, valid_from, valid_until, causal_id, contributing_row_ids
                            FROM running_payload
                            ORDER BY rn DESC
                            LIMIT 1
                        ) source_payloads ON true
                    
                WHERE seg.is_new_entity
            
Nested Loop Left Join  (cost=11.82..100.84 rows=1 width=369) (actual time=0.767..0.768 rows=0.00 loops=1)
  Buffers: local hit=64
  ->  Seq Scan on atomic_segments seg  (cost=0.00..89.00 rows=1 width=125) (actual time=0.766..0.766 rows=0.00 loops=1)
        Filter: is_new_entity
        Rows Removed by Filter: 2500
        Buffers: local hit=64
  ->  Limit  (cost=11.82..11.82 rows=1 width=120) (never executed)
        CTE ordered_sources
          ->  WindowAgg  (cost=8.31..8.33 rows=1 width=63) (never executed)
                Window: w1 AS (ORDER BY source_row.source_row_id ROWS UNBOUNDED PRECEDING)
                ->  Sort  (cost=8.31..8.32 rows=1 width=55) (never executed)
                      Sort Key: source_row.source_row_id
                      ->  Index Scan using active_source_rows_grouping_key_idx on active_source_rows source_row  (cost=0.28..8.30 rows=1 width=55) (never executed)
                            Index Cond: (grouping_key = seg.grouping_key)
                            Filter: (daterange(seg.valid_from, seg.valid_until) <@ daterange(valid_from, valid_until))
                            Index Searches: 0
        CTE running_payload
          ->  Recursive Union  (cost=0.00..3.21 rows=11 width=120) (never executed)
                Storage: Memory  Maximum Storage: 33kB
                ->  CTE Scan on ordered_sources  (cost=0.00..0.02 rows=1 width=120) (never executed)
                      Filter: (rn = 1)
                      Storage: Memory  Maximum Storage: 17kB
                ->  Hash Join  (cost=0.03..0.31 rows=1 width=120) (never executed)
                      Hash Cond: ((r.rn + 1) = s.rn)
                      ->  WorkTable Scan on running_payload r  (cost=0.00..0.20 rows=10 width=104) (never executed)
                      ->  Hash  (cost=0.02..0.02 rows=1 width=88) (never executed)
                            ->  CTE Scan on ordered_sources s  (cost=0.00..0.02 rows=1 width=88) (never executed)
                                  Storage: Memory  Maximum Storage: 17kB
        ->  Sort  (cost=0.28..0.30 rows=11 width=120) (never executed)
              Sort Key: running_payload.rn DESC
              ->  CTE Scan on running_payload  (cost=0.00..0.22 rows=11 width=120) (never executed)
                    Storage: Memory  Maximum Storage: 17kB
Planning:
  Buffers: shared hit=3
Planning Time: 0.255 ms
Execution Time: 1.992 ms
---
Executing setup: ANALYZE new_segments_no_target;
---
Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments_with_payloads ON COMMIT DROP AS
                WITH all_segments AS (
                    SELECT * FROM existing_segments_with_target
                    UNION ALL
                    SELECT * FROM new_segments_no_target
                )
                SELECT
                    seg.*,
                    seg.stable_pk_payload as propagated_stable_pk_payload
                FROM all_segments seg
                WHERE (seg.s_data_payload IS NOT NULL OR seg.t_data_payload IS NOT NULL)
                AND CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                    WHEN 'PATCH_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                    WHEN 'REPLACE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                    WHEN 'DELETE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                    WHEN 'UPDATE_FOR_PORTION_OF' THEN seg.t_data_payload IS NOT NULL
                    ELSE true
                END
            
Result  (cost=0.00..190.51 rows=2501 width=334) (actual time=0.008..0.861 rows=2500.00 loops=1)
  Buffers: local hit=128
  ->  Append  (cost=0.00..165.50 rows=2501 width=309) (actual time=0.007..0.615 rows=2500.00 loops=1)
        Buffers: local hit=128
        ->  Seq Scan on existing_segments_with_target  (cost=0.00..153.00 rows=2500 width=309) (actual time=0.006..0.488 rows=2500.00 loops=1)
              Filter: ((s_data_payload IS NOT NULL) OR (t_data_payload IS NOT NULL))
              Buffers: local hit=128
        ->  Seq Scan on new_segments_no_target  (cost=0.00..0.00 rows=1 width=393) (actual time=0.008..0.008 rows=0.00 loops=1)
              Filter: ((s_data_payload IS NOT NULL) OR (t_data_payload IS NOT NULL))
Planning:
  Buffers: shared hit=131
Planning Time: 0.282 ms
Execution Time: 8.190 ms
---
Executing setup: ANALYZE resolved_atomic_segments_with_payloads;
---
Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments_with_propagated_ids ON COMMIT DROP AS
                SELECT
                    *,
                    FIRST_VALUE(canonical_nk_json) OVER (PARTITION BY grouping_key ORDER BY valid_from) as unified_canonical_nk_json,
                    COALESCE(
                        contributing_row_ids,
                        (array_concat_agg(contributing_row_ids) FILTER (WHERE contributing_row_ids IS NOT NULL) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (array_concat_agg(contributing_row_ids) FILTER (WHERE contributing_row_ids IS NOT NULL) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as causal_source_row_ids,
                    COALESCE(
                        s_valid_from,
                        (max(s_valid_from) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (max(s_valid_from) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_s_valid_from,
                    COALESCE(
                        s_valid_until,
                        (max(s_valid_until) OVER (PARTITION BY grouping_key, t_valid_from, look_behind_grp)),
                        (max(s_valid_until) OVER (PARTITION BY grouping_key, t_valid_from, look_ahead_grp))
                    ) as propagated_s_valid_until
                FROM (
                    SELECT
                        *,
                        sum(CASE WHEN source_row_id IS NOT NULL THEN 1 ELSE 0 END) OVER (PARTITION BY grouping_key, t_valid_from ORDER BY valid_from) AS look_behind_grp,
                        sum(CASE WHEN source_row_id IS NOT NULL THEN 1 ELSE 0 END) OVER (PARTITION BY grouping_key, t_valid_from ORDER BY valid_from DESC) AS look_ahead_grp
                    FROM resolved_atomic_segments_with_payloads /* v_resolver_from */
                ) with_grp
            
WindowAgg  (cost=294.63..1019.10 rows=2500 width=422) (actual time=2.109..16.447 rows=2500.00 loops=1)
  Window: w3 AS (PARTITION BY with_grp.grouping_key ORDER BY with_grp.valid_from)
  Storage: Memory  Maximum Storage: 17kB
  Buffers: local hit=128
  ->  Incremental Sort  (cost=294.63..975.35 rows=2500 width=430) (actual time=2.104..15.198 rows=2500.00 loops=1)
        Sort Key: with_grp.grouping_key, with_grp.valid_from
        Presorted Key: with_grp.grouping_key
        Full-sort Groups: 79  Sort Method: quicksort  Average Memory: 38kB  Peak Memory: 38kB
        Buffers: local hit=128
        ->  WindowAgg  (cost=294.39..862.85 rows=2500 width=430) (actual time=1.942..13.394 rows=2500.00 loops=1)
              Window: w2 AS (PARTITION BY with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_behind_grp)
              Storage: Memory  Maximum Storage: 17kB
              Buffers: local hit=128
              ->  Incremental Sort  (cost=294.39..800.35 rows=2500 width=390) (actual time=1.939..11.738 rows=2500.00 loops=1)
                    Sort Key: with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_behind_grp
                    Presorted Key: with_grp.grouping_key, with_grp.t_valid_from
                    Full-sort Groups: 79  Sort Method: quicksort  Average Memory: 36kB  Peak Memory: 36kB
                    Buffers: local hit=128
                    ->  WindowAgg  (cost=294.23..687.85 rows=2500 width=390) (actual time=1.820..10.145 rows=2500.00 loops=1)
                          Window: w1 AS (PARTITION BY with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_ahead_grp)
                          Storage: Memory  Maximum Storage: 17kB
                          Buffers: local hit=128
                          ->  Incremental Sort  (cost=294.23..625.35 rows=2500 width=350) (actual time=1.814..8.456 rows=2500.00 loops=1)
                                Sort Key: with_grp.grouping_key, with_grp.t_valid_from, with_grp.look_ahead_grp
                                Presorted Key: with_grp.grouping_key, with_grp.t_valid_from
                                Full-sort Groups: 79  Sort Method: quicksort  Average Memory: 35kB  Peak Memory: 35kB
                                Buffers: local hit=128
                                ->  Subquery Scan on with_grp  (cost=294.13..512.85 rows=2500 width=350) (actual time=1.732..6.939 rows=2500.00 loops=1)
                                      Buffers: local hit=128
                                      ->  WindowAgg  (cost=294.13..512.85 rows=2500 width=350) (actual time=1.731..6.717 rows=2500.00 loops=1)
                                            Window: w2 AS (PARTITION BY resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from ORDER BY resolved_atomic_segments_with_payloads.valid_from)
                                            Storage: Memory  Maximum Storage: 17kB
                                            Buffers: local hit=128
                                            ->  Incremental Sort  (cost=294.13..462.85 rows=2500 width=342) (actual time=1.727..4.733 rows=2500.00 loops=1)
                                                  Sort Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from, resolved_atomic_segments_with_payloads.valid_from
                                                  Presorted Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from
                                                  Full-sort Groups: 79  Sort Method: quicksort  Average Memory: 35kB  Peak Memory: 35kB
                                                  Buffers: local hit=128
                                                  ->  WindowAgg  (cost=294.10..350.35 rows=2500 width=342) (actual time=1.679..3.203 rows=2500.00 loops=1)
                                                        Window: w1 AS (PARTITION BY resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from ORDER BY resolved_atomic_segments_with_payloads.valid_from)
                                                        Storage: Memory  Maximum Storage: 17kB
                                                        Buffers: local hit=128
                                                        ->  Sort  (cost=294.10..300.35 rows=2500 width=334) (actual time=1.669..1.743 rows=2500.00 loops=1)
                                                              Sort Key: resolved_atomic_segments_with_payloads.grouping_key, resolved_atomic_segments_with_payloads.t_valid_from, resolved_atomic_segments_with_payloads.valid_from DESC
                                                              Sort Method: quicksort  Memory: 900kB
                                                              Buffers: local hit=128
                                                              ->  Seq Scan on resolved_atomic_segments_with_payloads  (cost=0.00..153.00 rows=2500 width=334) (actual time=0.013..0.625 rows=2500.00 loops=1)
                                                                    Buffers: local hit=128
Planning:
  Buffers: shared hit=82
Planning Time: 0.448 ms
Execution Time: 21.956 ms
---
Explaining: 
                CREATE TEMP TABLE resolved_atomic_segments ON COMMIT DROP AS
                SELECT
                    grouping_key, id,  stable_identity_columns_are_null, lookup_columns_unavailable, is_new_entity, is_identifiable, is_ambiguous, conflicting_ids, unified_canonical_nk_json,
                    valid_from, valid_until, t_valid_from, t_valid_until, propagated_s_valid_from, propagated_s_valid_until, causal_source_row_ids, causal_id, propagated_stable_pk_payload as stable_pk_payload,
                    causal_source_row_ids IS NULL AS unaffected_target_only_segment,
                    s_data_payload, t_data_payload,
                    sql_saga.get_allen_relation(propagated_s_valid_from, propagated_s_valid_until, t_valid_from, t_valid_until) AS orig_source_target_relation,
                    COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb) as data_payload,
                    md5((jsonb_strip_nulls(COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb)))::text) as data_hash,
                    CASE 
                        WHEN s_data_payload IS NULL THEN t_ephemeral_payload
                        ELSE COALESCE(t_ephemeral_payload, '{}'::jsonb) || COALESCE(s_ephemeral_payload, '{}'::jsonb)
                    END as ephemeral_payload,
                    CASE WHEN 'f'::boolean
                        THEN trace || jsonb_build_object( 'cte', 'ras', 'segment_stable_pk', propagated_stable_pk_payload, 'final_data_payload', COALESCE(t_data_payload, '{}'::jsonb) || COALESCE(s_data_payload, '{}'::jsonb), 'final_ephemeral_payload', CASE WHEN s_data_payload IS NULL THEN t_ephemeral_payload ELSE COALESCE(t_ephemeral_payload, '{}'::jsonb) || COALESCE(s_ephemeral_payload, '{}'::jsonb) END )
                        ELSE NULL
                    END as trace
                FROM resolved_atomic_segments_with_propagated_ids
            
Seq Scan on resolved_atomic_segments_with_propagated_ids  (cost=0.00..444.00 rows=2880 width=394) (actual time=0.039..4.593 rows=2500.00 loops=1)
  Buffers: local hit=192
Planning:
  Buffers: shared hit=66
Planning Time: 0.152 ms
Execution Time: 8.824 ms
---
Explaining: 
                CREATE TEMP TABLE island_group ON COMMIT DROP AS
                SELECT
                    *,
                    SUM(is_island_start) OVER (PARTITION BY grouping_key ORDER BY valid_from) as island_group_id
                FROM (
                    SELECT
                        *,
                        CASE
                            WHEN prev_valid_until IS NULL
                            OR prev_valid_until <> valid_from
                            OR (prev_data_hash IS DISTINCT FROM data_hash)
                            THEN 1
                            ELSE 0
                        END as is_island_start
                    FROM (
                        SELECT
                            *,
                            LAG(valid_until) OVER w as prev_valid_until,
                            LAG(data_hash) OVER w as prev_data_hash,
                            LAG(data_payload) OVER w as prev_data_payload
                        FROM resolved_atomic_segments ras
                        WHERE ras.data_payload IS NOT NULL
                        WINDOW w AS (PARTITION BY grouping_key ORDER BY valid_from)
                    ) s1
                ) s2
            
WindowAgg  (cost=288.41..415.38 rows=2420 width=474) (actual time=1.433..5.420 rows=2500.00 loops=1)
  Window: w1 AS (PARTITION BY s1.grouping_key ORDER BY s1.valid_from)
  Storage: Memory  Maximum Storage: 17kB
  Buffers: local hit=128
  ->  Subquery Scan on s1  (cost=288.36..348.83 rows=2420 width=462) (actual time=1.425..2.966 rows=2500.00 loops=1)
        Buffers: local hit=128
        ->  WindowAgg  (cost=288.36..348.83 rows=2420 width=462) (actual time=1.424..2.673 rows=2500.00 loops=1)
              Window: w AS (PARTITION BY ras.grouping_key ORDER BY ras.valid_from)
              Storage: Memory  Maximum Storage: 17kB
              Buffers: local hit=128
              ->  Sort  (cost=288.33..294.38 rows=2420 width=394) (actual time=1.418..1.518 rows=2500.00 loops=1)
                    Sort Key: ras.grouping_key, ras.valid_from
                    Sort Method: quicksort  Memory: 934kB
                    Buffers: local hit=128
                    ->  Seq Scan on resolved_atomic_segments ras  (cost=0.00..152.32 rows=2420 width=394) (actual time=0.006..0.509 rows=2500.00 loops=1)
                          Filter: (data_payload IS NOT NULL)
                          Buffers: local hit=128
Planning:
  Buffers: shared hit=54
Planning Time: 0.138 ms
Execution Time: 9.915 ms
---
Explaining: 
                CREATE TEMP TABLE coalesced_final_segments ON COMMIT DROP AS
                SELECT
                    grouping_key, max(id) as id,
                    sql_saga.first(causal_id ORDER BY valid_from) as causal_id,
                    sql_saga.first(stable_identity_columns_are_null ORDER BY valid_from) as stable_identity_columns_are_null,
                    sql_saga.first(lookup_columns_unavailable ORDER BY valid_from) as lookup_columns_unavailable,
                    sql_saga.first(is_new_entity ORDER BY valid_from) as is_new_entity,
                    sql_saga.first(is_identifiable ORDER BY valid_from) as is_identifiable,
                    sql_saga.first(is_ambiguous ORDER BY valid_from) as is_ambiguous,
                    sql_saga.first(conflicting_ids ORDER BY valid_from) as conflicting_ids,
                    sql_saga.first(unified_canonical_nk_json ORDER BY valid_from) as canonical_nk_json,
                    sql_saga.first(orig_source_target_relation ORDER BY valid_from) as orig_source_target_relation,
                    sql_saga.first(t_valid_from ORDER BY valid_from) as ancestor_valid_from,
                    MIN(valid_from) as valid_from,
                    MAX(valid_until) as valid_until,
                    (sql_saga.first(data_payload ORDER BY valid_from DESC) || sql_saga.first(ephemeral_payload ORDER BY valid_from DESC)) as final_payload,
                    sql_saga.first(stable_pk_payload ORDER BY valid_from DESC) as stable_pk_payload,
                    bool_and(unaffected_target_only_segment) as unaffected_target_only_segment,
                    (SELECT array_agg(DISTINCT e) FROM unnest(array_concat_agg(causal_source_row_ids)) e WHERE e IS NOT NULL) as row_ids,
                    CASE WHEN 'f'::boolean
                        THEN jsonb_build_object( 'cte', 'coalesced', 'island_group_id', island_group_id, 'coalesced_stable_pk', sql_saga.first(stable_pk_payload ORDER BY valid_from DESC), 'final_payload', sql_saga.first(data_payload ORDER BY valid_from DESC), 'final_payload_sans_ephemeral', sql_saga.first(data_payload - '{}'::text[] ORDER BY valid_from DESC), 'atomic_traces', jsonb_agg((trace || jsonb_build_object('data_hash', data_hash, 'prev_data_hash', prev_data_hash, 'prev_data_payload', prev_data_payload)) ORDER BY valid_from) )
                        ELSE NULL
                    END as trace
                FROM island_group
                GROUP BY grouping_key, island_group_id
            
GroupAggregate  (cost=261.12..7028.52 rows=205 width=262) (actual time=1.273..17.704 rows=2500.00 loops=1)
  Group Key: island_group.grouping_key, island_group.island_group_id
  Buffers: local hit=128
  ->  Sort  (cost=261.12..266.24 rows=2048 width=262) (actual time=1.221..1.304 rows=2500.00 loops=1)
        Sort Key: island_group.grouping_key, island_group.island_group_id, island_group.valid_from
        Sort Method: quicksort  Memory: 649kB
        Buffers: local hit=128
        ->  Seq Scan on island_group  (cost=0.00..148.48 rows=2048 width=262) (actual time=0.009..0.482 rows=2500.00 loops=1)
              Buffers: local hit=128
  SubPlan 1
    ->  Aggregate  (cost=0.32..0.33 rows=1 width=32) (actual time=0.001..0.001 rows=1.00 loops=2500)
          ->  Sort  (cost=0.27..0.29 rows=10 width=8) (actual time=0.001..0.001 rows=1.00 loops=2500)
                Sort Key: e.e
                Sort Method: quicksort  Memory: 25kB
                ->  Function Scan on unnest e  (cost=0.00..0.10 rows=10 width=8) (actual time=0.000..0.000 rows=1.00 loops=2500)
                      Filter: (e IS NOT NULL)
Planning:
  Buffers: shared hit=66
Planning Time: 0.133 ms
Execution Time: 24.228 ms
---
Explaining: 
                CREATE TEMP TABLE diff ON COMMIT DROP AS
                SELECT
                    final_seg.grouping_key, COALESCE(final_seg.id, target_seg.id) as id, COALESCE(final_seg.causal_id, target_seg.causal_id) as causal_id,
                    COALESCE(final_seg.is_new_entity, false) as is_new_entity,
                    COALESCE(final_seg.is_identifiable, true) as is_identifiable,
                    COALESCE(final_seg.is_ambiguous, false) as is_ambiguous,
                    final_seg.conflicting_ids,
                    final_seg.canonical_nk_json,
                    COALESCE(final_seg.stable_identity_columns_are_null, false) as stable_identity_columns_are_null,
                    COALESCE(final_seg.lookup_columns_unavailable, false) as lookup_columns_unavailable,
                    final_seg.valid_from AS f_from, final_seg.valid_until AS f_until, final_seg.final_payload AS f_payload, final_seg.row_ids AS f_row_ids, final_seg.stable_pk_payload, final_seg.orig_source_target_relation,
                    CASE WHEN 'f'::boolean
                        THEN final_seg.trace || jsonb_build_object('cte', 'diff', 'diff_stable_pk', final_seg.stable_pk_payload, 'final_seg_causal_id', final_seg.causal_id, 'final_payload_vs_target_payload', jsonb_build_object('f', final_seg.final_payload, 't', target_seg.data_payload))
                        ELSE NULL
                    END as trace,
                    final_seg.unaffected_target_only_segment,
                    target_seg.valid_from as t_from, target_seg.valid_until as t_until, (target_seg.data_payload || target_seg.ephemeral_payload) as t_payload,
                    sql_saga.get_allen_relation(target_seg.valid_from, target_seg.valid_until, final_seg.valid_from, final_seg.valid_until) as b_a_relation
                FROM coalesced_final_segments AS final_seg
                FULL OUTER JOIN target_rows AS target_seg ON final_seg.grouping_key = ('existing_entity__' || COALESCE(target_seg.id::text, '_NULL_')) AND final_seg.ancestor_valid_from = target_seg.valid_from
            
Merge Full Join  (cost=605.52..923.02 rows=3584 width=294) (actual time=3.939..5.925 rows=2500.00 loops=1)
  Merge Cond: ((final_seg.grouping_key = (('existing_entity__'::text || COALESCE((target_seg.id)::text, '_NULL_'::text)))) AND (final_seg.ancestor_valid_from = target_seg.valid_from))
  Buffers: local hit=171 read=21
  ->  Sort  (cost=375.43..384.39 rows=3584 width=222) (actual time=1.151..1.223 rows=2500.00 loops=1)
        Sort Key: final_seg.grouping_key, final_seg.ancestor_valid_from
        Sort Method: quicksort  Memory: 618kB
        Buffers: local hit=128
        ->  Seq Scan on coalesced_final_segments final_seg  (cost=0.00..163.84 rows=3584 width=222) (actual time=0.007..0.403 rows=2500.00 loops=1)
              Buffers: local hit=128
  ->  Sort  (cost=230.10..236.35 rows=2500 width=55) (actual time=2.777..2.852 rows=2500.00 loops=1)
        Sort Key: (('existing_entity__'::text || COALESCE((target_seg.id)::text, '_NULL_'::text))), target_seg.valid_from
        Sort Method: quicksort  Memory: 326kB
        Buffers: local hit=43 read=21
        ->  Seq Scan on target_rows target_seg  (cost=0.00..89.00 rows=2500 width=55) (actual time=0.007..0.461 rows=2500.00 loops=1)
              Buffers: local hit=43 read=21
Planning:
  Buffers: shared hit=42
Planning Time: 0.187 ms
Execution Time: 11.706 ms
---
Explaining: 
                CREATE TEMP TABLE diff_ranked ON COMMIT DROP AS
                SELECT
                    d.*,
                    CASE
                        WHEN d.t_from IS NULL OR d.f_from IS NULL THEN NULL
                        WHEN d.f_from = d.t_from AND d.f_until = d.t_until AND d.f_payload IS NOT DISTINCT FROM d.t_payload THEN NULL
                        ELSE
                            row_number() OVER (
                                PARTITION BY d.grouping_key, d.t_from
                                ORDER BY
                                    CASE WHEN d.f_from = d.t_from THEN 1 ELSE 2 END,
                                    CASE WHEN d.f_payload - '{}'::text[] IS NOT DISTINCT FROM d.t_payload - '{}'::text[] THEN 1 ELSE 2 END,
                                    d.f_from,
                                    d.f_until
                            )
                    END as update_rank
                FROM diff d
            
WindowAgg  (cost=378.33..530.30 rows=3200 width=310) (actual time=1.681..3.327 rows=2500.00 loops=1)
  Window: w1 AS (PARTITION BY grouping_key, t_from ORDER BY (CASE WHEN (f_from = t_from) THEN 1 ELSE 2 END), (CASE WHEN (NOT ((f_payload - '{}'::text[]) IS DISTINCT FROM (t_payload - '{}'::text[]))) THEN 1 ELSE 2 END), f_from, f_until ROWS UNBOUNDED PRECEDING)
  Storage: Memory  Maximum Storage: 17kB
  Buffers: local hit=128
  ->  Sort  (cost=378.30..386.30 rows=3200 width=302) (actual time=1.674..1.754 rows=2500.00 loops=1)
        Sort Key: grouping_key, t_from, (CASE WHEN (f_from = t_from) THEN 1 ELSE 2 END), (CASE WHEN (NOT ((f_payload - '{}'::text[]) IS DISTINCT FROM (t_payload - '{}'::text[]))) THEN 1 ELSE 2 END), f_from, f_until
        Sort Method: quicksort  Memory: 760kB
        Buffers: local hit=128
        ->  Seq Scan on diff d  (cost=0.00..192.00 rows=3200 width=302) (actual time=0.009..0.861 rows=2500.00 loops=1)
              Buffers: local hit=128
Planning:
  Buffers: shared hit=46
Planning Time: 0.096 ms
Execution Time: 9.056 ms
---
        Storage: Memory  Maximum Storage: 17kB
        Buffers: local hit=128
        ->  Sort  (cost=344.42..352.42 rows=3200 width=305) (actual time=6.295..6.364 rows=2500.00 loops=1)
                    ->  CTE Scan on ordered_plan o  (cost=0.00..64.00 rows=3200 width=301) (actual time=3.041..3.364 rows=2500.00 loops=1)
                          Storage: Memory  Maximum Storage: 692kB
                          Buffers: local hit=128
                    ->  Hash  (cost=0.22..0.22 rows=11 width=16) (actual time=2.062..2.063 rows=0.00 loops=1)
                          Buckets: 1024  Batches: 1  Memory Usage: 8kB
Explaining: 
                CREATE TEMP TABLE plan_with_op ON COMMIT DROP AS
                (
                    SELECT * FROM (
                        SELECT
                            d.f_row_ids as row_ids, d.orig_source_target_relation, d.is_new_entity,
                            CASE
                                WHEN d.is_ambiguous THEN 'ERROR'::sql_saga.temporal_merge_plan_action
                                WHEN d.is_new_entity AND NOT d.is_identifiable THEN 'ERROR'::sql_saga.temporal_merge_plan_action
                                WHEN d.t_from IS NULL THEN 'INSERT'::sql_saga.temporal_merge_plan_action
                                WHEN d.f_from IS NULL THEN 'DELETE'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank = 1 THEN 'UPDATE'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank > 1 THEN 'INSERT'::sql_saga.temporal_merge_plan_action
                                WHEN d.update_rank IS NULL THEN
                                    CASE
                                        WHEN d.unaffected_target_only_segment THEN NULL
                                        ELSE 'SKIP_IDENTICAL'::sql_saga.temporal_merge_plan_action
                                    END
                                ELSE 'ERROR'::sql_saga.temporal_merge_plan_action
                            END as operation,
                            id,
                            CASE
                                WHEN d.is_new_entity AND d.canonical_nk_json IS NOT NULL
                                THEN d.canonical_nk_json || COALESCE(d.stable_pk_payload, '{}'::jsonb)
                                ELSE jsonb_build_object('id', d.id) || COALESCE(d.stable_pk_payload, '{}'::jsonb)
                            END as entity_keys_json,
                            jsonb_build_object('id', d.id) as identity_keys,
                            jsonb_build_object('id', d.id) as lookup_keys,
                            d.causal_id, d.t_from as old_valid_from, d.t_until as old_valid_until, d.f_from as new_valid_from, d.f_until as new_valid_until,
                            CASE
                                WHEN d.is_ambiguous THEN NULL
                                WHEN d.is_new_entity AND NOT d.is_identifiable THEN NULL
                                ELSE d.f_payload
                            END as data,
                            CASE
                                WHEN d.is_ambiguous
                                THEN jsonb_build_object('error', format('Source row is ambiguous. It matches multiple distinct target entities: %s', d.conflicting_ids))
                                WHEN d.is_new_entity AND NOT d.is_identifiable
                                THEN jsonb_build_object('error', 'Source row is unidentifiable. It has NULL for all stable identity columns ' || replace('{id}'::text, '"', '') || ' and all natural keys ' || replace('[["id"]]'::text, '"', ''))
                                ELSE NULL
                            END as feedback,
                            d.b_a_relation, d.grouping_key,
                            CASE WHEN 'f'::boolean
                                THEN d.trace || jsonb_build_object( 'cte', 'plan_with_op', 'diff_is_new_entity', d.is_new_entity, 'diff_causal_id', d.causal_id, 'entity_keys_from_key_cols', jsonb_build_object('id', d.id), 'entity_keys_from_stable_pk', d.stable_pk_payload, 'final_entity_id_json', jsonb_build_object('id', d.id) || COALESCE(d.stable_pk_payload, '{}'::jsonb) )
                                ELSE NULL
                            END as trace
                        FROM diff_ranked d
                        WHERE d.f_row_ids IS NOT NULL OR d.t_payload IS NOT NULL
                    ) with_op
                    WHERE with_op.operation IS NOT NULL
                )
                UNION ALL
                (
                    SELECT
                        ARRAY[source_row.source_row_id::BIGINT],
                        NULL::sql_saga.allen_interval_relation, source_row.is_new_entity,
                        COALESCE(
                            (source_row.early_feedback->>'operation')::sql_saga.temporal_merge_plan_action,
                            CASE
                                WHEN 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode = 'INSERT_NEW_ENTITIES' AND source_row.target_entity_exists THEN 'SKIP_FILTERED'::sql_saga.temporal_merge_plan_action
                                WHEN 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode IN ('PATCH_FOR_PORTION_OF', 'REPLACE_FOR_PORTION_OF', 'DELETE_FOR_PORTION_OF', 'UPDATE_FOR_PORTION_OF') AND NOT source_row.target_entity_exists THEN 'SKIP_NO_TARGET'::sql_saga.temporal_merge_plan_action
                                ELSE 'ERROR'::sql_saga.temporal_merge_plan_action
                            END
                        ),
                        id,
                        CASE
                            WHEN source_row.is_new_entity AND source_row.canonical_nk_json IS NOT NULL
                            THEN source_row.canonical_nk_json || COALESCE(source_row.stable_pk_payload, '{}'::jsonb)
                            ELSE jsonb_build_object('id', source_row.id)
                        END as entity_keys_json,
                        jsonb_build_object('id', source_row.id) as identity_keys, jsonb_build_object('id', source_row.id) as lookup_keys,
                        source_row.causal_id,
                        NULL, NULL, NULL, NULL, NULL,
                        CASE
                            WHEN source_row.early_feedback IS NOT NULL THEN jsonb_build_object('error', source_row.early_feedback->>'message')
                            ELSE jsonb_build_object('info', 'Source row was correctly filtered by the mode''s logic and did not result in a DML operation.')
                        END,
                        NULL,
                        CASE
                    WHEN source_row.is_new_entity
                    THEN 'new_entity__' || COALESCE(source_row.id::text, '_NULL_')
                    ELSE 'existing_entity__' || COALESCE(source_row.id::text, '_NULL_')
                END AS grouping_key,
                        NULL::jsonb
                    FROM source_rows_with_early_feedback source_row
                    WHERE
                        source_row.early_feedback IS NOT NULL
                        OR NOT (
                            CASE 'MERGE_ENTITY_PATCH'::sql_saga.temporal_merge_mode
                            WHEN 'MERGE_ENTITY_PATCH' THEN true
                            WHEN 'MERGE_ENTITY_REPLACE' THEN true
                            WHEN 'MERGE_ENTITY_UPSERT' THEN true
                            WHEN 'INSERT_NEW_ENTITIES' THEN NOT source_row.target_entity_exists
                            WHEN 'PATCH_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'REPLACE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'DELETE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            WHEN 'UPDATE_FOR_PORTION_OF' THEN source_row.target_entity_exists
                            ELSE false
                        END
                    )
                )
            
Append  (cost=0.00..547.73 rows=5732 width=293) (actual time=0.017..6.169 rows=2500.00 loops=1)
  Buffers: local hit=128 read=128 written=108
  ->  Seq Scan on diff_ranked d  (cost=0.00..250.50 rows=3057 width=293) (actual time=0.017..3.924 rows=2500.00 loops=1)
        Filter: (((f_row_ids IS NOT NULL) OR (t_payload IS NOT NULL)) AND (CASE WHEN is_ambiguous THEN 'ERROR'::sql_saga.temporal_merge_plan_action WHEN (is_new_entity AND (NOT is_identifiable)) THEN 'ERROR'::sql_saga.temporal_merge_plan_action WHEN (t_from IS NULL) THEN 'INSERT'::sql_saga.temporal_merge_plan_action WHEN (f_from IS NULL) THEN 'DELETE'::sql_saga.temporal_merge_plan_action WHEN (update_rank = 1) THEN 'UPDATE'::sql_saga.temporal_merge_plan_action WHEN (update_rank > 1) THEN 'INSERT'::sql_saga.temporal_merge_plan_action WHEN (update_rank IS NULL) THEN CASE WHEN unaffected_target_only_segment THEN NULL::sql_saga.temporal_merge_plan_action ELSE 'SKIP_IDENTICAL'::sql_saga.temporal_merge_plan_action END ELSE 'ERROR'::sql_saga.temporal_merge_plan_action END IS NOT NULL))
        Buffers: local hit=128
  ->  Seq Scan on source_rows_with_early_feedback source_row  (cost=0.00..268.57 rows=2675 width=293) (actual time=2.121..2.121 rows=0.00 loops=1)
        Filter: (early_feedback IS NOT NULL)
        Rows Removed by Filter: 2500
        Buffers: local read=128 written=108
Planning:
  Buffers: shared hit=50
Planning Time: 0.193 ms
Execution Time: 11.690 ms
---
Explaining: 
                CREATE TEMP TABLE plan ON COMMIT DROP AS
                SELECT
                    p.row_ids, p.operation, p.causal_id, p.is_new_entity,
                    p.id,
                    p.entity_keys_json as entity_keys,
                    p.identity_keys, p.lookup_keys,
                    p.orig_source_target_relation, p.b_a_relation, p.old_valid_from, p.old_valid_until,
                    p.new_valid_from, p.new_valid_until, p.data, p.feedback, p.trace,
                    p.grouping_key,
                    CASE
                        WHEN p.operation <> 'UPDATE' THEN NULL::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from = p.old_valid_from AND p.new_valid_until = p.old_valid_until THEN 'NONE'::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from <= p.old_valid_from AND p.new_valid_until >= p.old_valid_until THEN 'GROW'::sql_saga.temporal_merge_update_effect
                        WHEN p.new_valid_from >= p.old_valid_from AND p.new_valid_until <= p.old_valid_until THEN 'SHRINK'::sql_saga.temporal_merge_update_effect
                        ELSE 'MOVE'::sql_saga.temporal_merge_update_effect
                    END AS update_effect
                FROM plan_with_op p
                LEFT JOIN source_rows_with_new_flag source_row ON source_row.source_row_id = p.row_ids[1]
            
Merge Right Join  (cost=566.70..1809.90 rows=37888 width=297) (actual time=2.059..2.887 rows=2500.00 loops=1)
  Merge Cond: (source_row.source_row_id = (p.row_ids[1]))
  Buffers: local hit=128 read=64 written=53
  ->  Sort  (cost=220.40..226.32 rows=2368 width=4) (actual time=0.870..0.935 rows=2500.00 loops=1)
        Sort Key: source_row.source_row_id
        Sort Method: quicksort  Memory: 97kB
        Buffers: local read=64 written=53
        ->  Seq Scan on source_rows_with_new_flag source_row  (cost=0.00..87.68 rows=2368 width=4) (actual time=0.039..0.782 rows=2500.00 loops=1)
              Buffers: local read=64 written=53
  ->  Sort  (cost=346.30..354.30 rows=3200 width=293) (actual time=1.184..1.252 rows=2500.00 loops=1)
        Sort Key: (p.row_ids[1])
        Sort Method: quicksort  Memory: 679kB
        Buffers: local hit=128
        ->  Seq Scan on plan_with_op p  (cost=0.00..160.00 rows=3200 width=293) (actual time=0.008..0.442 rows=2500.00 loops=1)
              Buffers: local hit=128
Planning:
  Buffers: shared hit=42
Planning Time: 0.111 ms
Execution Time: 6.165 ms
---
              Sort Key: ((CASE WHEN (o.operation = 'DELETE'::sql_saga.temporal_merge_plan_action) THEN '1'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = ANY ('{NONE,SHRINK}'::sql_saga.temporal_merge_update_effect[]))) THEN '2'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect)) THEN (3 + COALESCE(mb.move_batch, '0'::bigint)) WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'GROW'::sql_saga.temporal_merge_update_effect)) THEN '1000000'::bigint WHEN (o.operation = 'INSERT'::sql_saga.temporal_merge_plan_action) THEN '2000000'::bigint ELSE '3000000'::bigint END)::integer)
              Sort Method: quicksort  Memory: 679kB
              Buffers: local hit=128
              ->  Hash Left Join  (cost=0.36..158.12 rows=3200 width=305) (actual time=5.108..5.716 rows=2500.00 loops=1)
                    Hash Cond: (o.plan_op_seq = mb.plan_op_seq)
                    Buffers: local hit=128
Explaining: 
                WITH RECURSIVE ordered_plan AS (
                    SELECT
                        row_number() OVER ( ORDER BY p.grouping_key, p.id, CASE p.operation WHEN 'DELETE' THEN 1 WHEN 'UPDATE' THEN 2 WHEN 'INSERT' THEN 3 ELSE 4 END, p.update_effect NULLS FIRST, CASE WHEN p.update_effect = 'MOVE' THEN 1 ELSE 0 END, CASE WHEN p.update_effect = 'MOVE' THEN COALESCE(p.old_valid_from, p.new_valid_from) END DESC NULLS LAST, COALESCE(p.old_valid_from, p.new_valid_from), (p.row_ids[1]) )::BIGINT as plan_op_seq,
                        p.*
                    FROM plan p
                ),
                -- For MOVE operations, group into the largest possible batches that can execute together.
                -- Within a single UPDATE statement, all rows see the pre-update state (MVCC snapshot).
                -- So if MOVE A's new_range overlaps MOVE B's old_range, they cannot be in the same statement.
                --
                -- Algorithm: Process MOVEs in plan_op_seq order (which is old_valid_from DESC).
                -- Track a multirange of old_ranges in the current batch. For each MOVE:
                -- - If new_range overlaps the batch's multirange, start a new batch
                -- - Add this MOVE's old_range to the batch's multirange
                --
                -- We use a recursive CTE to accumulate the multirange with reset-on-conflict logic.
                moves_ordered AS (
                    SELECT
                        o.plan_op_seq,
                        o.grouping_key,
                        o.old_valid_from,
                        o.old_valid_until,
                        o.new_valid_from,
                        o.new_valid_until,
                        row_number() OVER (PARTITION BY o.grouping_key ORDER BY o.plan_op_seq) AS move_seq
                    FROM ordered_plan o
                    WHERE o.operation = 'UPDATE' AND o.update_effect = 'MOVE'
                ),
                moves_with_batch AS (
                    -- Base case: first MOVE in each entity starts batch 0
                    SELECT 
                        plan_op_seq, grouping_key, old_valid_from, old_valid_until, 
                        new_valid_from, new_valid_until, move_seq,
                        0::bigint AS move_batch,
                        datemultirange(daterange(old_valid_from, old_valid_until, '[)')) AS batch_old_ranges
                    FROM moves_ordered
                    WHERE move_seq = 1
                    
                    UNION ALL
                    
                    -- Recursive case: check if new MOVE conflicts with batch's multirange
                    SELECT
                        m.plan_op_seq, m.grouping_key, m.old_valid_from, m.old_valid_until,
                        m.new_valid_from, m.new_valid_until, m.move_seq,
                        CASE 
                            WHEN daterange(m.new_valid_from, m.new_valid_until, '[)') && prev.batch_old_ranges
                            THEN prev.move_batch + 1  -- Conflict: start new batch
                            ELSE prev.move_batch      -- No conflict: same batch
                        END AS move_batch,
                        CASE 
                            WHEN daterange(m.new_valid_from, m.new_valid_until, '[)') && prev.batch_old_ranges
                            THEN datemultirange(daterange(m.old_valid_from, m.old_valid_until, '[)'))  -- Reset multirange
                            ELSE prev.batch_old_ranges + datemultirange(daterange(m.old_valid_from, m.old_valid_until, '[)'))  -- Add to multirange
                        END AS batch_old_ranges
                    FROM moves_ordered m
                    JOIN moves_with_batch prev 
                      ON prev.grouping_key = m.grouping_key 
                     AND prev.move_seq = m.move_seq - 1
                ),
                with_statement_seq AS (
                    SELECT
                        o.plan_op_seq,
                        CASE
                            WHEN o.operation = 'DELETE' THEN 1
                            WHEN o.operation = 'UPDATE' AND o.update_effect IN ('NONE', 'SHRINK') THEN 2
                            WHEN o.operation = 'UPDATE' AND o.update_effect = 'MOVE' THEN
                                -- Base of 3 for first MOVE batch, +1 for each subsequent batch
                                3 + COALESCE(mb.move_batch, 0)
                            WHEN o.operation = 'UPDATE' AND o.update_effect = 'GROW' THEN 1000000  -- Will be renumbered
                            WHEN o.operation = 'INSERT' THEN 2000000  -- Will be renumbered
                            ELSE 3000000
                        END::INT as raw_statement_seq,
                        o.row_ids, o.operation, o.update_effect, o.causal_id::TEXT, o.is_new_entity, o.entity_keys, o.identity_keys, o.lookup_keys, o.orig_source_target_relation, o.b_a_relation, o.old_valid_from::TEXT,
                        o.old_valid_until::TEXT, o.new_valid_from::TEXT, o.new_valid_until::TEXT, o.data, o.feedback, CASE WHEN o.trace IS NOT NULL THEN o.trace || jsonb_build_object('final_grouping_key', o.grouping_key) ELSE NULL END as trace, o.grouping_key
                    FROM ordered_plan o
                    LEFT JOIN moves_with_batch mb ON mb.plan_op_seq = o.plan_op_seq
                )
                SELECT
                    plan_op_seq,
                    dense_rank() OVER (ORDER BY raw_statement_seq)::INT as statement_seq,
                    row_ids, operation, update_effect, causal_id, is_new_entity, entity_keys, identity_keys, lookup_keys, orig_source_target_relation, b_a_relation, old_valid_from,
                    old_valid_until, new_valid_from, new_valid_until,
                    -- New range columns
                    CASE 
                        WHEN old_valid_from IS NOT NULL AND old_valid_until IS NOT NULL 
                        THEN daterange(old_valid_from::date, old_valid_until::date, '[)')::TEXT
                        ELSE NULL 
                    END as old_valid_range,
                    CASE 
                        WHEN new_valid_from IS NOT NULL AND new_valid_until IS NOT NULL 
                        THEN daterange(new_valid_from::date, new_valid_until::date, '[)')::TEXT
                        ELSE NULL 
                    END as new_valid_range,
                    data, feedback, trace, grouping_key
                FROM with_statement_seq
                ORDER BY plan_op_seq;
            
Sort  (cost=1633.20..1641.20 rows=3200 width=513) (actual time=11.656..11.724 rows=2500.00 loops=1)
  Sort Key: o.plan_op_seq
  Sort Method: quicksort  Memory: 854kB
  Buffers: local hit=128
  CTE ordered_plan
    ->  WindowAgg  (cost=386.34..538.30 rows=3200 width=329) (actual time=3.039..3.671 rows=2500.00 loops=1)
          Window: w1 AS (ORDER BY p.grouping_key, p.id, (CASE p.operation WHEN 'DELETE'::sql_saga.temporal_merge_plan_action THEN 1 WHEN 'UPDATE'::sql_saga.temporal_merge_plan_action THEN 2 WHEN 'INSERT'::sql_saga.temporal_merge_plan_action THEN 3 ELSE 4 END), p.update_effect, (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN 1 ELSE 0 END), (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN COALESCE(p.old_valid_from, p.new_valid_from) ELSE NULL::date END), (COALESCE(p.old_valid_from, p.new_valid_from)), (p.row_ids[1]) ROWS UNBOUNDED PRECEDING)
          Storage: Memory  Maximum Storage: 17kB
          Buffers: local hit=128
          ->  Sort  (cost=386.30..394.30 rows=3200 width=321) (actual time=3.034..3.098 rows=2500.00 loops=1)
                Sort Key: p.grouping_key, p.id, (CASE p.operation WHEN 'DELETE'::sql_saga.temporal_merge_plan_action THEN 1 WHEN 'UPDATE'::sql_saga.temporal_merge_plan_action THEN 2 WHEN 'INSERT'::sql_saga.temporal_merge_plan_action THEN 3 ELSE 4 END), p.update_effect NULLS FIRST, (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN 1 ELSE 0 END), (CASE WHEN (p.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect) THEN COALESCE(p.old_valid_from, p.new_valid_from) ELSE NULL::date END) DESC NULLS LAST, (COALESCE(p.old_valid_from, p.new_valid_from)), (p.row_ids[1])
                Sort Method: quicksort  Memory: 710kB
                Buffers: local hit=128
                ->  Seq Scan on plan p  (cost=0.00..200.00 rows=3200 width=321) (actual time=0.007..0.490 rows=2500.00 loops=1)
                      Buffers: local hit=128
  CTE moves_ordered
    ->  WindowAgg  (cost=80.05..80.09 rows=2 width=64) (actual time=2.060..2.060 rows=0.00 loops=1)
          Window: w1 AS (PARTITION BY o_1.grouping_key ORDER BY o_1.plan_op_seq ROWS UNBOUNDED PRECEDING)
          ->  Incremental Sort  (cost=80.01..80.06 rows=2 width=56) (actual time=2.060..2.060 rows=0.00 loops=1)
                Sort Key: o_1.grouping_key, o_1.plan_op_seq
                Presorted Key: o_1.grouping_key
                Full-sort Groups: 1  Sort Method: quicksort  Average Memory: 25kB  Peak Memory: 25kB
                ->  CTE Scan on ordered_plan o_1  (cost=0.00..80.00 rows=1 width=56) (actual time=2.054..2.054 rows=0.00 loops=1)
                      Filter: ((operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect))
                      Rows Removed by Filter: 2500
                      Storage: Memory  Maximum Storage: 692kB
  CTE moves_with_batch
    ->  Recursive Union  (cost=0.00..4.08 rows=11 width=104) (actual time=2.062..2.062 rows=0.00 loops=1)
          Storage: Memory  Maximum Storage: 33kB
          ->  CTE Scan on moves_ordered  (cost=0.00..0.05 rows=1 width=104) (actual time=2.060..2.060 rows=0.00 loops=1)
                Filter: (move_seq = 1)
                Storage: Memory  Maximum Storage: 17kB
          ->  Hash Join  (cost=0.07..0.39 rows=1 width=104) (actual time=0.001..0.002 rows=0.00 loops=1)
                Hash Cond: ((prev.grouping_key = m.grouping_key) AND (prev.move_seq = (m.move_seq - 1)))
                ->  WorkTable Scan on moves_with_batch prev  (cost=0.00..0.20 rows=10 width=80) (actual time=0.001..0.001 rows=0.00 loops=1)
                ->  Hash  (cost=0.04..0.04 rows=2 width=64) (never executed)
                      ->  CTE Scan on moves_ordered m  (cost=0.00..0.04 rows=2 width=64) (never executed)
                            Storage: Memory  Maximum Storage: 17kB
  ->  WindowAgg  (cost=344.44..824.42 rows=3200 width=513) (actual time=6.318..10.942 rows=2500.00 loops=1)
        Window: w1 AS (ORDER BY ((CASE WHEN (o.operation = 'DELETE'::sql_saga.temporal_merge_plan_action) THEN '1'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = ANY ('{NONE,SHRINK}'::sql_saga.temporal_merge_update_effect[]))) THEN '2'::bigint WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'MOVE'::sql_saga.temporal_merge_update_effect)) THEN (3 + COALESCE(mb.move_batch, '0'::bigint)) WHEN ((o.operation = 'UPDATE'::sql_saga.temporal_merge_plan_action) AND (o.update_effect = 'GROW'::sql_saga.temporal_merge_update_effect)) THEN '1000000'::bigint WHEN (o.operation = 'INSERT'::sql_saga.temporal_merge_plan_action) THEN '2000000'::bigint ELSE '3000000'::bigint END)::integer) ROWS UNBOUNDED PRECEDING)
                          ->  CTE Scan on moves_with_batch mb  (cost=0.00..0.22 rows=11 width=16) (actual time=2.062..2.062 rows=0.00 loops=1)
                                Storage: Memory  Maximum Storage: 17kB
Planning:
  Buffers: shared hit=40
Planning Time: 0.285 ms
Execution Time: 12.082 ms
