-- Large Scale Production Benchmark: 1.1M legal units + 800K establishments
-- Tests temporal_merge at production scale with FK constraints
--
-- This benchmark models the Norway statistics bureau workload:
-- - 1.1M legal units (parent entities)
-- - 800K establishments (child entities with temporal FK to legal_unit)
--
-- Verifies:
-- 1. Parent table loads at O(1) per batch (~5000 rows/sec)
-- 2. Child table with temporal FK loads at O(1) per batch
-- 3. FK constraint checking doesn't cause slowdown as tables grow
--
-- Expected time: ~8-12 minutes total
--
\i sql/include/test_setup.sql
--
-- test_setup.sql
--
-- Common setup for regression tests that need to be self-contained.
-- This script creates the extension, a user role, and grants permissions.
--
SET datestyle = 'ISO, YMD';
CREATE EXTENSION IF NOT EXISTS btree_gist;
CREATE EXTENSION IF NOT EXISTS sql_saga CASCADE;
DO $$
BEGIN
    CREATE ROLE sql_saga_unprivileged_user;
EXCEPTION WHEN duplicate_object THEN
END
$$;
GRANT USAGE ON SCHEMA sql_saga TO sql_saga_unprivileged_user;
GRANT SELECT ON ALL TABLES IN SCHEMA sql_saga TO sql_saga_unprivileged_user;
GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA sql_saga TO sql_saga_unprivileged_user;
/*
 * Allow the unprivileged user to create tables in the public schema.
 * This is required for tests that create their own tables.
 * PG 15+ restricts this by default.
 */
GRANT CREATE ON SCHEMA public TO PUBLIC;
\i sql/include/benchmark_setup.sql
\set ECHO none
--------------------------------------------------------------------------------
-- SETUP
--------------------------------------------------------------------------------
DROP SCHEMA IF EXISTS large_bench CASCADE;
NOTICE:  schema "large_bench" does not exist, skipping
CREATE SCHEMA large_bench;
GRANT ALL ON SCHEMA large_bench TO sql_saga_unprivileged_user;
SET ROLE TO sql_saga_unprivileged_user;
--------------------------------------------------------------------------------
\echo ''

\echo '--- Creating temporal tables with FK relationship ---'
--- Creating temporal tables with FK relationship ---
--------------------------------------------------------------------------------
-- Parent: legal_unit
CREATE TABLE large_bench.legal_unit (
    id serial, 
    name text, 
    valid_range daterange, 
    valid_from date, 
    valid_until date
);
SELECT sql_saga.add_era('large_bench.legal_unit', 'valid_range',
    valid_from_column_name => 'valid_from',
    valid_until_column_name => 'valid_until');
NOTICE:  sql_saga: Created trigger "legal_unit_synchronize_temporal_columns_trigger" on table large_bench.legal_unit to synchronize columns: valid_from, valid_until
NOTICE:  sql_saga: Created GIST index "legal_unit_valid_range_gist_idx" on column large_bench.legal_unit.valid_range for temporal_merge performance
 add_era 
---------
 t
(1 row)

SELECT sql_saga.add_unique_key(
    table_oid => 'large_bench.legal_unit'::regclass, 
    column_names => ARRAY['id'], 
    key_type => 'primary',
    unique_key_name => 'large_bench_legal_unit_pk');
NOTICE:  sql_saga: Added constraints to table large_bench.legal_unit: ALTER COLUMN id SET NOT NULL; ADD PRIMARY KEY (id, valid_range WITHOUT OVERLAPS)
      add_unique_key       
---------------------------
 large_bench_legal_unit_pk
(1 row)

-- Child: establishment (with temporal FK to legal_unit)
CREATE TABLE large_bench.establishment (
    id serial,
    legal_unit_id int NOT NULL,
    name text,
    valid_range daterange, 
    valid_from date, 
    valid_until date
);
SELECT sql_saga.add_era('large_bench.establishment', 'valid_range',
    valid_from_column_name => 'valid_from',
    valid_until_column_name => 'valid_until');
NOTICE:  sql_saga: Created trigger "establishment_synchronize_temporal_columns_trigger" on table large_bench.establishment to synchronize columns: valid_from, valid_until
NOTICE:  sql_saga: Created GIST index "establishment_valid_range_gist_idx" on column large_bench.establishment.valid_range for temporal_merge performance
 add_era 
---------
 t
(1 row)

SELECT sql_saga.add_unique_key(
    table_oid => 'large_bench.establishment'::regclass, 
    column_names => ARRAY['id'], 
    key_type => 'primary',
    unique_key_name => 'large_bench_establishment_pk');
NOTICE:  sql_saga: Added constraints to table large_bench.establishment: ALTER COLUMN id SET NOT NULL; ADD PRIMARY KEY (id, valid_range WITHOUT OVERLAPS)
        add_unique_key        
------------------------------
 large_bench_establishment_pk
(1 row)

-- Add temporal FK: establishment.legal_unit_id -> legal_unit.id
SELECT sql_saga.add_foreign_key(
    fk_table_oid => 'large_bench.establishment'::regclass,
    fk_column_names => ARRAY['legal_unit_id'],
    pk_table_oid => 'large_bench.legal_unit'::regclass,
    pk_column_names => ARRAY['id']);
NOTICE:  sql_saga: No compatible index found for foreign key on table large_bench.establishment. Creating new index: CREATE INDEX establishment_legal_unit_id_valid_gist_idx ON large_bench.establishment USING GIST (legal_unit_id, valid_range)
          add_foreign_key          
-----------------------------------
 establishment_legal_unit_id_valid
(1 row)

--------------------------------------------------------------------------------
\echo '--- Creating staging tables ---'
--- Creating staging tables ---
--------------------------------------------------------------------------------
-- Staging for legal_unit
CREATE TABLE large_bench.lu_staging (
    row_id serial primary key,
    batch int not null,
    identity_correlation int not null,
    legal_unit_id int,
    lu_name text,
    valid_from date not null,
    valid_until date not null,
    valid_range daterange GENERATED ALWAYS AS (daterange(valid_from, valid_until)) STORED
);
CREATE INDEX ON large_bench.lu_staging (batch);
CREATE INDEX ON large_bench.lu_staging USING GIST (valid_range);
-- Staging for establishment
CREATE TABLE large_bench.es_staging (
    row_id serial primary key,
    batch int not null,
    identity_correlation int not null,
    establishment_id int,
    legal_unit_id int not null,
    es_name text,
    valid_from date not null,
    valid_until date not null,
    valid_range daterange GENERATED ALWAYS AS (daterange(valid_from, valid_until)) STORED
);
CREATE INDEX ON large_bench.es_staging (batch);
CREATE INDEX ON large_bench.es_staging USING GIST (valid_range);
-- Results table for timing data
CREATE TABLE large_bench.benchmark_results (
    phase text NOT NULL,
    checkpoint text NOT NULL,
    entities int,
    batches int,
    elapsed_sec numeric,
    rows_per_sec int,
    ms_per_batch int
);
--------------------------------------------------------------------------------
\echo ''

\echo '================================================================================'
================================================================================
\echo 'LARGE SCALE PRODUCTION BENCHMARK'
LARGE SCALE PRODUCTION BENCHMARK
\echo '1.1M legal units + 800K establishments (with temporal FK)'
1.1M legal units + 800K establishments (with temporal FK)
\echo '================================================================================'
================================================================================
\echo ''

--------------------------------------------------------------------------------
CREATE OR REPLACE PROCEDURE large_bench.run_benchmark()
LANGUAGE plpgsql AS $proc$
DECLARE
    v_lu_count int := 1100000;
    v_es_count int := 800000;
    v_batch_size int := 2000;
    v_lu_batches int;
    v_es_batches int;
    v_batch int;
    v_start timestamptz;
    v_phase_start timestamptz;
    v_checkpoint timestamptz;
    v_elapsed_sec numeric;
    v_last_report int := 0;
    v_checkpoint_entities int;
    v_rows_per_sec int;
    v_ms_per_batch int;
BEGIN
    v_lu_batches := CEIL(v_lu_count::numeric / v_batch_size);
    v_es_batches := CEIL(v_es_count::numeric / v_batch_size);
    
    ----------------------------------------------------------------------------
    -- PHASE 1: Generate all staging data
    ----------------------------------------------------------------------------
    v_start := clock_timestamp();
    
    INSERT INTO large_bench.lu_staging (row_id, batch, identity_correlation, lu_name, valid_from, valid_until)
    SELECT n, CEIL(n::numeric / v_batch_size), n, 
           format('Company-%s', n),
           '2024-01-01'::date, 'infinity'
    FROM generate_series(1, v_lu_count) n;
    
    INSERT INTO large_bench.es_staging (row_id, batch, identity_correlation, legal_unit_id, es_name, valid_from, valid_until)
    SELECT n, CEIL(n::numeric / v_batch_size), n, 
           n,
           format('Branch-%s', n),
           '2024-01-01'::date, 'infinity'
    FROM generate_series(1, v_es_count) n;
    
    ANALYZE large_bench.lu_staging;
    ANALYZE large_bench.es_staging;
    
    v_elapsed_sec := EXTRACT(EPOCH FROM clock_timestamp() - v_start);
    INSERT INTO large_bench.benchmark_results VALUES 
        ('Phase 1', 'Data generation', v_lu_count + v_es_count, NULL, 
         ROUND(v_elapsed_sec, 1), ROUND((v_lu_count + v_es_count) / v_elapsed_sec), NULL);
    COMMIT;
    
    ----------------------------------------------------------------------------
    -- PHASE 2: Load parent table (legal_unit)
    ----------------------------------------------------------------------------
    v_phase_start := clock_timestamp();
    v_checkpoint := v_phase_start;
    v_last_report := 0;
    
    FOR v_batch IN 1..v_lu_batches LOOP
        EXECUTE format($sql$
            CREATE OR REPLACE TEMP VIEW sv_lu AS
            SELECT row_id, identity_correlation as founding_id, legal_unit_id AS id, 
                   lu_name AS name,
                   valid_from, valid_until, valid_range
            FROM large_bench.lu_staging WHERE batch = %L
        $sql$, v_batch);
        
        CALL sql_saga.temporal_merge(
            target_table => 'large_bench.legal_unit',
            source_table => 'sv_lu',
            primary_identity_columns => ARRAY['id'],
            founding_id_column => 'founding_id',
            update_source_with_identity => true
        );
        
        COMMIT;
        
        -- Record progress every 100 batches
        IF v_batch - v_last_report >= 100 THEN
            v_checkpoint_entities := v_batch * v_batch_size;
            v_elapsed_sec := EXTRACT(EPOCH FROM clock_timestamp() - v_phase_start);
            v_rows_per_sec := ROUND(v_checkpoint_entities / v_elapsed_sec);
            v_ms_per_batch := ROUND(EXTRACT(EPOCH FROM clock_timestamp() - v_checkpoint) * 1000 / 100);
            
            INSERT INTO large_bench.benchmark_results VALUES 
                ('Phase 2 (LU)', format('%sK', v_checkpoint_entities / 1000), 
                 v_checkpoint_entities, v_batch, ROUND(v_elapsed_sec, 1), v_rows_per_sec, v_ms_per_batch);
            
            v_checkpoint := clock_timestamp();
            v_last_report := v_batch;
            COMMIT;
        END IF;
    END LOOP;
    
    v_elapsed_sec := EXTRACT(EPOCH FROM clock_timestamp() - v_phase_start);
    INSERT INTO large_bench.benchmark_results VALUES 
        ('Phase 2 (LU)', 'COMPLETE', v_lu_count, v_lu_batches, 
         ROUND(v_elapsed_sec, 1), ROUND(v_lu_count / v_elapsed_sec), 
         ROUND(v_elapsed_sec * 1000 / v_lu_batches));
    
    -- Back-propagate LU IDs to ES staging (required before Phase 3)
    v_start := clock_timestamp();
    
    UPDATE large_bench.es_staging es
    SET legal_unit_id = lu.legal_unit_id
    FROM large_bench.lu_staging lu
    WHERE es.legal_unit_id = lu.identity_correlation
      AND lu.legal_unit_id IS NOT NULL;
    
    v_elapsed_sec := EXTRACT(EPOCH FROM clock_timestamp() - v_start);
    INSERT INTO large_bench.benchmark_results VALUES 
        ('ID Propagation', 'LU->ES staging', v_es_count, NULL, ROUND(v_elapsed_sec, 1), 
         ROUND(v_es_count / v_elapsed_sec), NULL);
    
    ANALYZE large_bench.legal_unit;
    ANALYZE large_bench.es_staging;
    COMMIT;
    
    ----------------------------------------------------------------------------
    -- PHASE 3: Load child table (establishment) with FK constraint
    ----------------------------------------------------------------------------
    v_phase_start := clock_timestamp();
    v_checkpoint := v_phase_start;
    v_last_report := 0;
    
    FOR v_batch IN 1..v_es_batches LOOP
        EXECUTE format($sql$
            CREATE OR REPLACE TEMP VIEW sv_es AS
            SELECT row_id, identity_correlation as founding_id, establishment_id AS id, 
                   legal_unit_id,
                   es_name AS name,
                   valid_from, valid_until, valid_range
            FROM large_bench.es_staging WHERE batch = %L
        $sql$, v_batch);
        
        CALL sql_saga.temporal_merge(
            target_table => 'large_bench.establishment',
            source_table => 'sv_es',
            primary_identity_columns => ARRAY['id'],
            founding_id_column => 'founding_id',
            update_source_with_identity => true
        );
        
        COMMIT;
        
        -- Record progress every 100 batches
        IF v_batch - v_last_report >= 100 THEN
            v_checkpoint_entities := v_batch * v_batch_size;
            v_elapsed_sec := EXTRACT(EPOCH FROM clock_timestamp() - v_phase_start);
            v_rows_per_sec := ROUND(v_checkpoint_entities / v_elapsed_sec);
            v_ms_per_batch := ROUND(EXTRACT(EPOCH FROM clock_timestamp() - v_checkpoint) * 1000 / 100);
            
            INSERT INTO large_bench.benchmark_results VALUES 
                ('Phase 3 (ES)', format('%sK', v_checkpoint_entities / 1000), 
                 v_checkpoint_entities, v_batch, ROUND(v_elapsed_sec, 1), v_rows_per_sec, v_ms_per_batch);
            
            v_checkpoint := clock_timestamp();
            v_last_report := v_batch;
            COMMIT;
        END IF;
    END LOOP;
    
    v_elapsed_sec := EXTRACT(EPOCH FROM clock_timestamp() - v_phase_start);
    INSERT INTO large_bench.benchmark_results VALUES 
        ('Phase 3 (ES)', 'COMPLETE', v_es_count, v_es_batches, 
         ROUND(v_elapsed_sec, 1), ROUND(v_es_count / v_elapsed_sec), 
         ROUND(v_elapsed_sec * 1000 / v_es_batches));
    
    ANALYZE large_bench.establishment;
    COMMIT;
END;
$proc$;
CALL large_bench.run_benchmark();
--------------------------------------------------------------------------------
\echo ''

\echo '--- Final Counts ---'
--- Final Counts ---
--------------------------------------------------------------------------------
SELECT 'legal_unit' as table_name, COUNT(*) as row_count FROM large_bench.legal_unit
UNION ALL
SELECT 'establishment', COUNT(*) FROM large_bench.establishment
ORDER BY table_name;
  table_name   | row_count 
---------------+-----------
 establishment |    800000
 legal_unit    |   1100000
(2 rows)

\echo ''

\echo 'See expected/performance/113_benchmark_1m_scale.log for timing details.'
See expected/performance/113_benchmark_1m_scale.log for timing details.
\echo ''

--------------------------------------------------------------------------------
-- Write timing details to performance file (variable between runs)
--------------------------------------------------------------------------------
\set perf_file expected/performance/113_benchmark_1m_scale.log
\pset tuples_only on
\pset footer off
\o :perf_file
SELECT '# Performance Baseline: 1.1M legal units + 800K establishments';
SELECT '# These numbers are reference baselines, not test assertions.';
SELECT '# They are updated after significant performance changes.';
SELECT '#';
SELECT '';
\pset tuples_only off
SELECT '# Benchmark Results:' as "header";
SELECT 
    phase,
    checkpoint,
    entities,
    batches,
    elapsed_sec,
    rows_per_sec,
    ms_per_batch
FROM large_bench.benchmark_results
ORDER BY 
    CASE phase
        WHEN 'Phase 1' THEN 1
        WHEN 'Phase 2 (LU)' THEN 2
        WHEN 'ID Propagation' THEN 3
        WHEN 'Phase 3 (ES)' THEN 4
    END,
    CASE checkpoint
        WHEN 'Data generation' THEN 0
        WHEN 'LU->ES staging' THEN 0
        WHEN 'COMPLETE' THEN 1000
        ELSE COALESCE(NULLIF(regexp_replace(checkpoint, '[^0-9]', '', 'g'), '')::int, 500)
    END;
\pset tuples_only on
SELECT '';
SELECT '# Scaling verification (ms_per_batch should stay constant):';
\pset tuples_only off
SELECT 
    phase,
    MIN(ms_per_batch) as min_ms,
    MAX(ms_per_batch) as max_ms,
    MAX(ms_per_batch) - MIN(ms_per_batch) as variance,
    CASE 
        WHEN MAX(ms_per_batch) <= MIN(ms_per_batch) * 1.5 THEN 'O(1) - GOOD'
        WHEN MAX(ms_per_batch) <= MIN(ms_per_batch) * 2.0 THEN 'O(1) - OK'
        ELSE 'REGRESSION!'
    END as scaling
FROM large_bench.benchmark_results
WHERE checkpoint NOT IN ('Data generation', 'LU->ES staging', 'COMPLETE')
GROUP BY phase
ORDER BY phase;
\o
\pset footer on
\pset tuples_only off
--------------------------------------------------------------------------------
-- CLEANUP
--------------------------------------------------------------------------------
RESET ROLE;
DROP SCHEMA large_bench CASCADE;
NOTICE:  drop cascades to 8 other objects
DETAIL:  drop cascades to table large_bench.legal_unit
drop cascades to table large_bench.establishment
drop cascades to table large_bench.lu_staging
drop cascades to view sv_lu
drop cascades to table large_bench.es_staging
drop cascades to view sv_es
drop cascades to table large_bench.benchmark_results
drop cascades to function large_bench.run_benchmark()
\i sql/include/benchmark_teardown.sql
\set ECHO none
\i sql/include/test_teardown.sql
--
-- test_teardown.sql
--
-- Common teardown for regression tests. This script drops the unprivileged
-- user role created by test_setup.sql.
--
-- It is important to reset the role first, in case a test fails and
-- leaves the session role set to the user that is about to be dropped.
RESET ROLE;
-- Drop the extensions to ensure a clean state for the next test.
-- Use CASCADE to remove any dependent objects created by sql_saga.
DROP EXTENSION IF EXISTS sql_saga CASCADE;
DROP EXTENSION IF EXISTS btree_gist CASCADE;
-- Revoke any privileges held by the test user and drop any objects they own.
-- This is necessary before the role can be dropped.
DROP OWNED BY sql_saga_unprivileged_user;
DROP ROLE IF EXISTS sql_saga_unprivileged_user;
